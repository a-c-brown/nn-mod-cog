{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Understand the principles behind the creation of the recurrent neural network\n",
    "2. MATH\n",
    "3. CODE/MODELS\n",
    "4. MORE STUFF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Historical and theoretical background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The poet Derlmore Schartz once wrote: **\"...time is the fire in which we burn\"**. We can't scape time. Time is embedded in every human thought and action. Yet, so far, we have been oblivious to the role of time in neural network modeling. Indeed, in all models we have examined so far we have implicitly assumed that **data is \"perceived\" all at once**, although there are countless examples where time is a critical consideration: movement, speech production, planning, decision-making, etc. We also have implicitly assumed that **past-states have no influence in future-states**. This is, the input pattern at time-step $t-1$ has no influence in the output of time-step $t-0$, or $t+1$, or any subsequent outcome for that matter. In probabilistic jargon, this equals to assume that each sample is drawn independently from each other. We know in many scenarios this is simply not true: when giving a talk, my next utterance will depend upon my past utterances; when running, my last stride will condition my next stride, and so on. You can imagine endless examples.\n",
    "\n",
    "Multilayer perceptrons and Convolutional Networks, in principle, can be used to approach problems where time and sequences are a consideration. Nevertheless, such architectures where not designed with time in mind, and better architectures were envisioned. In particular, **Recurrent Neural Networks (RNNs)** are the modern standard to deal with **time-dependent** and/or **sequence-dependent** problems. This type of networks are \"recurrent\" in the sense that they can **revisit or reuse past states** as inputs to predict the next or future states. To put it plainly, they have **memory**. Indeed, memory is what allow us to incorporate our past thoughts and behaviors into our future thoughts and behaviors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hopfield Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of earliest examples of networks incorporating \"recurrences\" was the so-called Hopfield Network, introduced in 1982 by John Hofield, a physicist at Caltech. Hopfield networks were important as they help to reignite the interset in neural networks in the early '80s. \n",
    "\n",
    "Hopfield wanted to address the fundamental question of **emergence**: Can relatively stable cognitive phenomena, like memories and learned categories, emerge from the collective action of large numbers of simple neurons? After all, such behavior was observed in other physicial systems like vortex patterns in fluid flow, Why not in cognition? \n",
    "\n",
    "In resemblence to the McCulloch-Pitts neuron, Hopfield units are binary threshold units.  \n",
    "Hopfield Networks are characterized for having neurons with very elementary propierties, non-linearities, and little \"structure\". \n",
    "\n",
    "TO CONTINUE..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elman Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first successful example of what today is known as a recurrent network was introduced by [Jeffrey Elman](https://en.wikipedia.org/wiki/Jeffrey_Elman) in 1990, the so-called \"Elman Network\". \n",
    "\n",
    "\n",
    "TO CONTINUE..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Long Short-Term Memory Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes Goodfellow et all:\n",
    "- for processsing sequential data\n",
    "- sequences of variable lenght\n",
    "- parameter sharing allows to apply the model to sequences of different lenght (form) and generalize\n",
    "- we want to recognize information regardless of the position in the sequence\n",
    "- mlp need fix-size inputs (problematic for language), and lear weights for each feature separately\n",
    "- each member of the output depends on (is a function) the previous members of the output\n",
    "\n",
    "Notes Elman 1990:\n",
    "- First proposed by Jordan (1986) \n",
    "- hidden unit patterns feedback to themselves\n",
    "- internal representations reflect prior internal states\n",
    "- context-dependent and generalization across classes of items\n",
    "- language expresses as temporal sequences\n",
    "- two approaches to represent time in neural nets: \n",
    "    - explicitly, as an additional dimension (a matrix instead of a vector)\n",
    "    - implicitly, by the effect it has on processing (next outcome)\n",
    "- Explicit approach: \n",
    "    - in a vector, the first temporal event is the x_1, the second x_2, and son on. A time-series. Hence space (i.e., position in a vector) is encoding time.\n",
    "    - Problem 1: it needs to be represented for the networ all at once\n",
    "    - Problem 2: input pattern has to be large enough to represent the longest sequence, and fix on that. Hence, all inputs has be of same lenght.\n",
    "    - Problem 3: it's hard to distinguish relative temporal position from absolute temporal position. The same pattern, ocurring at slightly different times, would be processed as different in vetor space, when they are the same. \n",
    "- Implicit approach:\n",
    "    - network requiers memory\n",
    "    - Many ways to do it: Jordan 1986 introduced a good one. The recurrent connections allow the hiden units to see its previous output and use as an input for the current computation. In other words, memory.\n",
    "    - Modification. (Temporal) Contex units: copy the state of the hidden units, and then serve as input to the hidden unit in the next time-step. It is like a short-term memory storage. They must learn representations that learn both classification and to encode temporal propierties of sequential input. \n",
    "- Temporal XOR:\n",
    "    - feeding input one datapoint at a time. (x_1, x_2) -> (y_3), (x_1, x_2) -> (y_6), and so on. \n",
    "    - since prediction is just possible every 2 time steps, the error is going to follow a cyclic behavior. \n",
    "    - the network will try to use XOR att all time steps after training, although is going to work only every 2 time steps. \n",
    "- Structure in letter sequences:\n",
    "    - architecture: there were 6 input units, 20 hidden units, 6 output units, and 20 context units\n",
    "    - learns statistical regularities: the consonants were ordered randomly (high error), but the vowels were not (low error)\n",
    "    - After each consonant can predict next vowel. At the end of the vowel sequence can't predict next consonant. \n",
    "    - Net learns which vowels follow which consinants\n",
    "    - Net learns how many vowels follow each consonant\n",
    "    - Net knows a consonant follows, but not which one\n",
    "- Discovering the notion \"word\":\n",
    "    - many linguist at the time assumed that basic language constructs must be innate, otherwise, their language theories do not work. Assumes clear-cut and uncontrovertial definitions of constructs like phoneme, morphene, word, etc. This is not true in practice, May counterexamples.\n",
    "    - Fundamental concepts in lingistics are fliud. Learning is crucial.\n",
    "    - Neural nets learn graded representations.\n",
    "    - Can words emerge from learning the sequential structure of letter (or sound, gesture, etc)sequences?\n",
    "    - Elman's model learns to parse words, but such criteria relative. This is similar to what happens in child languague acquisition, that sometime treat groups of words as single units. \n",
    "    - This is not a full-blown model of language acquisition. Intsead, it shows how a neural net can learn to parse words via extracting patterns from data.\n",
    "- Discovering lexical classes from word oder:\n",
    "    - Lexical classes (syntactic structure, etc) can be learned from data. Such classes are implicit in the data.\n",
    "    - He was demonstrated that a network was able to learn the temporal structure of letter sequences\n",
    "    - In simulation, input patterns do not contain any information about lexical classes, yet, the somehow learn such structure from the co-ocurrence statistics in the data. \n",
    "    - Evaluate similarity structure of learned representations with hierarchical clustering. \n",
    "    - The network learn representations that resemble the natural organization of classes, like nouns and verbs, or clusters of animals and objects. \n",
    "    - Network learns although it has less info than human in real world context. \n",
    "    - Replacing man with zog example \n",
    "- Remarks about distributed representations:\n",
    "    - No limit to the number of concepts to be represented by a finite set of units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematical formalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limitations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
