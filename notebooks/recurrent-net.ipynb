{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Understand the principles behind the creation of the recurrent neural network\n",
    "2. Obtain intuition about difficulties training RNNs, namely: vanishing/exploding gradients and long-term dependencies\n",
    "3. Obtain intuition about mechanics of backpropagation through time BPTT\n",
    "4. Develop a Long Short-Term memory implementation in Keras \n",
    "5. Learn about uses and limitations of RNNs from a cognitive science perspective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Historical and theoretical background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The poet Derlmore Schartz once wrote: **\"...time is the fire in which we burn\"**. We can't scape time. Time is embedded in every human thought and action. Yet, so far, we have been oblivious to the role of time in neural network modeling. Indeed, in all models we have examined so far we have implicitly assumed that **data is \"perceived\" all at once**, although there are countless examples where time is a critical consideration: movement, speech production, planning, decision-making, etc. We also have implicitly assumed that **past-states have no influence in future-states**. This is, the input pattern at time-step $t-1$ has no influence in the output of time-step $t-0$, or $t+1$, or any subsequent outcome for that matter. In probabilistic jargon, this equals to assume that each sample is drawn independently from each other. We know in many scenarios this is simply not true: when giving a talk, my next utterance will depend upon my past utterances; when running, my last stride will condition my next stride, and so on. You can imagine endless examples.\n",
    "\n",
    "Multilayer Perceptrons and Convolutional Networks, in principle, can be used to approach problems where time and sequences are a consideration (for instance [Cui et al, 2016](https://arxiv.org/pdf/1603.06995.pdf)). Nevertheless, introducing time considerations in such architectures is cumbersome, and better architectures have been envisioned. In particular, **Recurrent Neural Networks (RNNs)** are the modern standard to deal with **time-dependent** and/or **sequence-dependent** problems. This type of networks are \"recurrent\" in the sense that they can **revisit or reuse past states as inputs to predict the next or future states**. To put it plainly, they have **memory**. Indeed, memory is what allow us to incorporate our past thoughts and behaviors into our future thoughts and behaviors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hopfield Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of earliest examples of networks incorporating \"recurrences\" was the so-called **Hopfield Network**, introduced in 1982 by [John Hopfield](https://en.wikipedia.org/wiki/John_Hopfield), at the time, a physicist at Caltech. Hopfield networks were important as they helped to reignite the interest in neural networks in the early '80s (along with backpropagation). In his 1982 paper, Hopfield wanted to address the fundamental question of **emergence** in cognitive systems: Can relatively stable cognitive phenomena, like memories, emerge from the collective action of large numbers of simple neurons? After all, such behavior was observed in other physicial systems like vortex patterns in fluid flow. Brains seemed like another promising candidate.\n",
    "\n",
    "Hopfield networks are known as a type of **energy-based** (instead of error-based) network because their properties derive from a global energy-function (Raj, 2020). In resemblence to the McCulloch-Pitts neuron, Hopfield neurons are binary threshold units but with recurrent instead of feed-forward connections, where each unit is **bi-directionally connected** to each other, as shown in **Figure 1**. This means that each unit *receives* inputs and *sends* inputs to every other connected unit. A consequence of this architecture is that **weights values are symmetric**, such that weights *coming into* a unit are the same as the ones *coming out* of a unit. The value of each unit is determined by a linear function wrapped into a threshold function $T$, as $y_i = T(\\sum w_{ji}y_j + b_i)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> Figure 1: Hopfield Network </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/rec-net/hopfield-net.svg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic idea of Hopfield networks is that each configuration of binary-values $C$ in the network is associated with a **global energy value $-E$**. Here is a simplified picture of the training process: imagine you have a network with five neurons with a configuration of $C_1=(0, 1, 0, 1, 0)$. Now, imagine $C_1$ yields a global energy-value $E_1= 2$ (following the energy function formula). Your goal is to *minimize* $E$ by changing one element of the network $c_i$ at a time. By using the weight updating rule $\\Delta w$, you can subsequently get a new configuration like $C_2=(1, 1, 0, 1, 0)$, as new weights will cause a change in the activation values $(0,1)$. If $C_2$ yields a *lower value of $E$*, let's say, $1.5$, you are moving in the right direction. If you keep iterating with new configurations the network will eventually \"settle\" into a **global energy minimun** (conditioned to the initial state of the network).\n",
    "\n",
    "A fascinanting aspect of Hopfield networks, besides the introduction of recurrence, is that is closely based in neuroscience research about learning and memory, particularly Hebbian learning (Hebb, 1949). In fact, Hopfield (1982) proposed this model as a way to capture **memory formation and retrieval**. Basically, the idea is that the energy-minima of the network could represent the **formation of a memory**, which further give rise to a property known as **[content-addressable memory (CAM)](https://en.wikipedia.org/wiki/Content-addressable_memory)**. Here is the idea with a computer analogy: when you access information storaged in the random access memory of your computer (RAM), you give the \"address\" where the \"memory\" is located to retrieve it. CAM works the other way around: you give information about the **content** you are searching for, and the computer should retrieve the \"memory\". This is great because this works even when you have **partial or corrupted** information about the content, which is a much more **realistic depiction of how human memory works**. It is similar to doing a google search. Just think in how many times you have searched for lyrics with partial information, like \"song with the beeeee bop ba bodda bope!\".\n",
    "\n",
    "Is important to highlight that the sequential adjustment of Hopfield networks is **not driven by error correction**. Actually, there isn't a \"target\" as in supervised-based neural networks. Hopfield networks are systems that \"evolve\" until they find an stable low-energy state. If you \"perturb\" such a system, the system will \"re-evolve\" towards its previous stable-state, similar to how those inflatable \"Bop Bags\" toys get back to their initial position no matter how hard you punch them. It is almost like the system \"remembers\" its previous stable-state (in't?). This ability to \"return\" to a previous stable-state after perturbation is why they serve as models of memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elman Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although Hopfield networks where innovative and fascinating models, the first successful example of a recurrent network trained with backpropagation was introduced by [Jeffrey Elman](https://en.wikipedia.org/wiki/Jeffrey_Elman), the so-called **Elman Network** (Elman, 1990). Elman was a cognitive scientist at UC San Diego at the time, part of the group of researchers that published the famous PDP book.\n",
    "\n",
    "In 1990, Elman published \"Finding Structure in Time\", an highly influential work for both cognitive science and machine learning (particularly natural language processing). Elman was concerned with the problem of representing \"time\" or \"sequences\" in neural networks. In his view, you could take either a \"explicit\" approach or an \"implicit\" approach. The **explicit** approach represents time **spacially**. Consider a vector $x = [x_1,x_2 \\cdots, x_n]$, where element $x_1$ represents the first value of a sequence, $x_2$ the second element, and $x_n$ the last element. Hence, the spacial location in $\\bf{x}$ is indicating the temporal location of a value. You can think about elements of $\\bf{x}$ as sequences of words or actions, one after the other, for instance: $x^1=[Sound, of, the, funky, drummer]$ is a sequence of length five. Elman saw **several drawbacks** to this approach. First, although $\\bf{x}$ is a sequence, the network still needs to represent the sequence all at once as an input, this is, a network would need five input neurons to process $x^1$. Second, imposes a rigid limit on the durations of pattern, in other words, the newtwork needs fixed number of elements for every input vector $\\bf{x}$: a network with five input units, can't accomodate a sequence of lenght six. True, you could start with a six input network, but then shorter sequences would be misrepresented, since mistmatched units would receive zero input. This is a problem for most domains where sequences have variable duation. Finally, it can't easily distinguish **relative** temporal position from **absolute** temporal position. Consider the sequence $s = [1, 1]$ and a vector input lenght of four bits. Such a sequence can be presented in at least three variations:\n",
    "\n",
    "$$\n",
    "x_1 = [0, 1, 1, 0]\\\\\n",
    "x_2 = [0, 0, 1, 1]\\\\\n",
    "x_3 = [1, 1, 0, 0]\n",
    "$$\n",
    "\n",
    "Here, $\\bf{x_1}$, $\\bf{x_2}$, and $\\bf{x_3}$ are instances of $\\bf{s}$ but spacially displaced in the input vector. Geometrically, those three vectors are very different from each other (you can compute similarity measures to put a number on that), although represent the same instance. Even though you can train a neural net to learn those three patterns are associated with the same target, their inherent disimilarity will hinder the network ability to generalize the learned association. It is like training network to learn that blue, yellow, and red are associated with the same target \"1\": the network will learn the association, but it's unlikely to make sense outside the training set.\n",
    "\n",
    "The **implicit** approach represents time by **its effect in intermediate computations**. To do this, Elman added a **contex unit** to save past computations and incorporate those in future computations. In short, **memory**. Elman based his approach in the work of [Michael I. Jordan](https://people.eecs.berkeley.edu/~jordan/) on serial processing (1986). Jordan's network implement recurrent connections from the network output $\\hat{y}$ to its hidden units $h$, via a \"memory unit\" $\\mu$ (equivalent to Elman's \"context unit\") as depicted in **Figure 2**. In short, the the memory unit keeps a running average of **all past outputs**: this is how the entire past history is implicitly accounted for on each new computation. There is no learning in the memory unit, which means the weights are fixed to $1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> Figure 2: Jordan Network </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/rec-net/jordan-net.svg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: Jordan's network diagrams exemplifies the two ways in which recurrent nets are usually represented. On the left, the **compact format** depicts the network structure as a circuit. On the right, the **unfolded representation** incorporates the notion of time-steps calculations. The unfolded representation also illustrate how a recurrent network can be constructed in a pure feed-forward fashion, with as many layers as time-steps in your sequence. One key consideration is that the weights will be identical on each time-step (or layer). Keep this unfolded representation in mind as will become important later.\n",
    "\n",
    "Elman's innovation was twofold: **recurrent conections between hidden units and memory** (contex) units, and **trainable parameters from the memory units to the hidden units**. Memory units now have to \"remember\" the past state of hidden units, which means that instead of keeping a running average, they \"clone\" the value at the previous time-step $t-1$. Memory units also have to learn useful representations (weights) for encoding temporal properties of the sequential input.  **Figure 3** summarizes Elman's network in compact and unfolded fashion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> Figure 3: Elman Network </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/rec-net/elman-net.svg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: there is something curious about Elman's architecture. What it is the point of \"cloning\" $h$ into $c$ at each time-step? You could bypass $c$ altogether by sending the value of $h_t$ straight into $h_{t+1}$, wich yield mathematically identical results. The most likely explanation for this was that Elman starting point was Jordan's network, which had a separated memory unit. Regardless, keep in mind we don't need $c$ units to design a functionally identical network. \n",
    "\n",
    "Elman performed multiple experiments with this architecture demostrating it was capable to solve multiple problems with a sequential structure: a temporal version of the XOR problem; learning the structure (i.e., vowels and consonants sequential order) in sequences of letters; discovering the notion of \"word\"; and even learning complex lexical classes like word order in short sentences. Let's briefly explore the temporal XOR solution as an exemplar. **Table 1** shows the XOR problem:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table 1**: Truth Table For XOR Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| $x_1$ | $x_2$ | $y$ |\n",
    "|---|---|--------|\n",
    "| 0 | 0 | 0      |\n",
    "| 0 | 1 | 1      |\n",
    "| 1 | 0 | 1      |\n",
    "| 1 | 1 | 0      |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a way to transform the XOR problem into a sequence. Consider the following vector: \n",
    "\n",
    "$$\n",
    "s= [1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,...]\n",
    "$$\n",
    "\n",
    "In $\\bf{s}$, the first and second elements, $s_1$ and $s_2$, represent $x_1$ and $x_2$ inputs of **Table 1**, whereas the third element, $s_3$, represents the corresponding output $y$. This pattern repeats until the end of the sequence $s$ as shown in **Figure 4**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> Figure 4: Temporal XOR </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/rec-net/temporal-xor.svg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elman trained his network with a 3,000 elements sequence for 600 iterations over the entire dataset, on the task of predicting the next item $s_{t+1}$ of the sequence $s$, meaning that he fed inputs to the network **one by one**. He showed that **error pattern** followed a predictable trend: the mean squared error was **lower every 3 outputs**, and higher in between, meaning the network learned to predict the third element in the sequence, as shown in **Chart 1** (the numbers are made up, but the pattern is the same found by Elman (1990))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import altair as alt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-a7d933977454486589f2531649e57ec8\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  (function(spec, embedOpt){\n",
       "    const outputDiv = document.getElementById(\"altair-viz-a7d933977454486589f2531649e57ec8\");\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.0.2?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function loadScript(lib) {\n",
       "      return new Promise(function(resolve, reject) {\n",
       "        var s = document.createElement('script');\n",
       "        s.src = paths[lib];\n",
       "        s.async = true;\n",
       "        s.onload = () => resolve(paths[lib]);\n",
       "        s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "        document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "      });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else if (typeof vegaEmbed === \"function\") {\n",
       "      displayChart(vegaEmbed);\n",
       "    } else {\n",
       "      loadScript(\"vega\")\n",
       "        .then(() => loadScript(\"vega-lite\"))\n",
       "        .then(() => loadScript(\"vega-embed\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-2fb8460af59365532c40b2d7e4fe648a\"}, \"mark\": \"line\", \"encoding\": {\"x\": {\"type\": \"quantitative\", \"field\": \"cycle\"}, \"y\": {\"type\": \"quantitative\", \"field\": \"MSE\"}}, \"title\": \"Chart 1\", \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.0.2.json\", \"datasets\": {\"data-2fb8460af59365532c40b2d7e4fe648a\": [{\"MSE\": 0.35, \"cycle\": 1}, {\"MSE\": 0.15, \"cycle\": 2}, {\"MSE\": 0.3, \"cycle\": 3}, {\"MSE\": 0.27, \"cycle\": 4}, {\"MSE\": 0.14, \"cycle\": 5}, {\"MSE\": 0.4, \"cycle\": 6}, {\"MSE\": 0.35, \"cycle\": 7}, {\"MSE\": 0.12, \"cycle\": 8}, {\"MSE\": 0.36, \"cycle\": 9}, {\"MSE\": 0.31, \"cycle\": 10}, {\"MSE\": 0.15, \"cycle\": 11}, {\"MSE\": 0.32, \"cycle\": 12}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.Chart(...)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = pd.DataFrame({\"MSE\": [0.35, 0.15, 0.30, 0.27, 0.14, 0.40, 0.35, 0.12, 0.36, 0.31, 0.15, 0.32],\n",
    "                  \"cycle\": np.arange(1, 13)})\n",
    "alt.Chart(s).mark_line().encode(x=\"cycle\", y=\"MSE\").properties(title='Chart 1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An inmediate advantage of this approach is the network can take **inputs of any lenght**, withouth having to alter the networ architecture at all.\n",
    "\n",
    "In the same paper, Elman showed that the **internal (hidden) representations** learned by the network grouped into meaningful categories, this is, **semantically similar words group together** when analyzed with [hierarchical clustering](https://en.wikipedia.org/wiki/Hierarchical_clustering). This was remarkable as demostrated the utility of RNNs as model of cognition in sequence-based problems. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interlude: vanishing and exploding gradients in RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turns out, training recurrent neural networks is hard. Considerably harder than multilayer-perceptrons. When faced with the task of training **very deep networks**, like RNNs, the gradients have the impolite tendency of either (1) **vanishing**, or (2) **exploding** (Bengio et all, 1994; Pascanu et all, 2012). Recall that RNNs can be unfolded so that recurrent connections follow pure feed-forward computations. This unrolled RNN will have as many layers as elements in the sequence. Thus, a sequence of 50 words will be unrolled as a RNN of 50 layers (taking word as a unit). \n",
    "\n",
    "Concretely, the **vanishing gradient problem** will make really hard to learn **long-term dependencies** in sequences. Let's say you have a collection of poems, where the last sentence makes reference to the first one. Such a dependency will be hard to learn for a deep RNN where gradients vanish as we move backward in the network. The **exploding gradient problem** will completely derail the learning process. In very deep networks this is often a problem because more layers amplify the effect of large gradients, compounding into very large updates to the network weights, to the point values completely blow up.        \n",
    "\n",
    "Here is the intuition for the **mechanics of gradient vanishing**: when gradients *begin small*, as you move backwards through the network computing gradients, they will get even smaller as you get closer to the input layer. Consequently, when doing the weight update based on such gradients, the weights closer to the output layer will obtain larger updates than weights closer to the input layer. This means that the weights closer to the input layer will hardly change at all, whereas the weights closer to the ouput layer will change a lot. This is a serious problem when **earlier layers matter for prediction**: they will keep propagating more or less the same signal forward because no learning (i.e., weight updates) will happen, which may significantly hinder the network performance.  \n",
    "\n",
    "Here is the intuition for the **mechanics of gradient explotion**: when gradients *begin large*, as you move backwards through the network computing gradients, they will get even larger as you get closer to the input layer. Consequently, when doing the weight update based on such gradients, the weights closer to the input layer will obtain larger updates than weights closer to the output layer. Learning can go wrong really fast. Recall that the signal propagated by each layer is the outcome of taking the product between the previous hidden-state and the current hidden-state. If the weights in earlier layers get really large, they will forward-propagate larger and larger signals on each iteration, and the predicted output values will spiral-up out of control, making the error $y-\\hat{y}$ so large that the network will be unable to learn at all. In fact, your computer will \"overflow\" quickly as it would unable to represent numbers that big. Very dramatic.  \n",
    "\n",
    "The mathematics of gradient vanishing and explotion gets complicated quickly. If you want to delve into the mathematics see [Bengio et all (1994)](http://ai.dinfo.unifi.it/paolo/ps/tnn-94-gradient.pdf), [Pascanu et all (2012)](https://arxiv.org/abs/1211.5063), and [Philipp et all (2017)](https://arxiv.org/abs/1712.05577). \n",
    "\n",
    "For our purposes, I'll give you a simplified numerical example for intuition. Consider the task of predicting a vector $y = \\begin{bmatrix} 1 & 1 \\end{bmatrix}$, from inputs $x = \\begin{bmatrix} 1 & 1 \\end{bmatrix}$, with a multilayer-perceptron with 5 hidden layers and tanh activation functions. We have two cases:\n",
    "\n",
    "- the weight matrix $W_l$ is initialized to large values $w_{ij} = 2$\n",
    "- the weight matrix $W_s$ is initialized to small values $w_{ij} = 0.02$\n",
    "\n",
    "Now, let's compute a single forward-propagation pass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output for large initial weights: \n",
      " [[3.99730269]\n",
      " [3.99730269]]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[1],[1]])\n",
    "W_l = np.array([[2, 2],\n",
    "                [2, 2]])\n",
    "\n",
    "h1 = np.tanh(W_l @ x)\n",
    "h2 = np.tanh(W_l @ h1)\n",
    "h3 = np.tanh(W_l @ h2)\n",
    "h4 = np.tanh(W_l @ h3)\n",
    "h5 = np.tanh(W_l @ h4)\n",
    "y_hat = (W_l @ h5)\n",
    "print(f'output for large initial weights: \\n {y_hat}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output for small initial weights: \n",
      " [[4.09381337e-09]\n",
      " [4.09381337e-09]]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[1],[1]])\n",
    "W_s = np.array([[0.02, 0.02],\n",
    "                [0.02, 0.02]])\n",
    "\n",
    "h1 = np.tanh(W_s @ x)\n",
    "h2 = np.tanh(W_s @ h1)\n",
    "h3 = np.tanh(W_s @ h2)\n",
    "h4 = np.tanh(W_s @ h3)\n",
    "h5 = np.tanh(W_s @ h4)\n",
    "y_hat = (W_s @ h5)\n",
    "print(f'output for small initial weights: \\n {y_hat}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that for $W_l$ the output $\\hat{y}\\approx4$, whereas for $W_s$ the output $\\hat{y} \\approx 0$. Why does this matter? We haven't done the gradient computation but you can probably anticipate what it's goin to happen: for the $W_l$ case, the gradient update is going to be very large, and for the $W_s$ very small. If you keep cycling through forward and backward passes these problems will become worse, leading to gradient explotion and vanishing respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Long Short-Term Memory Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Several challenges difficulted progress in RNN in the early '90s (Hochreiter & Schmidhuber, 1997; Pascanu et all, 2012). In addition to vanishing and exploding gradients, we have the fact that the **forward computation is slow**, as RNNs can't compute in parallel: to preserve the time-dependencies through the layers, each layer has to be computed sequentially, which naturally takes more time. Elman networks proved to be effective at solving relatively simple problems, but as the sequences scaled in size and complexity, this type of network struggle. \n",
    "\n",
    "Several approaches where proposed in the '90s to address the aforementioned issues like time-delay neural networks (Lang et al, 1990), simulated annealing (Bengio et al., 1994), and others. The architecture that really moved the field forward was the so-called **Long Short-Term Memory (LSTM) Network**, introduced by [Sepp Hochreiter](https://en.wikipedia.org/wiki/Sepp_Hochreiter) and [Jurgen Schmidhuber](https://en.wikipedia.org/wiki/J%C3%BCrgen_Schmidhuber) in 1997. As the name sugguest, the defining characteristic of LSTMs is the addition of units combining both short-memory and long-memory capabilities. \n",
    "\n",
    "In LSTMs, instead of having a simple memory unit \"cloning\" values from the hidden unit as in Elman networks, we have a (1) **cell unit** (a.k.a., memory unit) which effectively acts as a long-term memory storage, and (2) a **hidden-state** which acts as a memory controller. These two elements are integrated as a circuit of logic gates controlling the flow of information at each time-step. Understanding the notation is crucial here, which is depicted in **Figure 5**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> Figure 5: LSTM architecture </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/rec-net/lstm-unit.svg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In LSTMs $x_t$, $h_t$, and $c_t$ represent vectors of values. Lightish-pink circles represent element-wise operations, and darkish-pink boxes are fully-connected layers with trainable weights. The top part of the diagram acts as a **memory storage**, whereas the bottom part has a double role: (1) passing the hidden-state information from the previos time-step $t-1$ to the next time step $t$, and (2) to regulate the **influx** of information from $x_t$ and $h_{t-1}$ **into** the memory storage, and the **outflux** of information **from** the memory storage into the next hidden state $h-t$. The second role is the core idea behind LSTM. You can think about it as making **three decisions** at each time-step:\n",
    "\n",
    "1. **Is the *old information* $c_{t-1}$ worth to keep in my memory storage $c_t$?** If so, let the information pass, otherwise, \"forget\" such information. This is controlled by the *forget gate*.    \n",
    "2. **Is this *new information* (inputs) worth to be saved into my memory storage $c_t$?** If so, let information flow into $c_t$. This is controlled by the *input gate* and the *candidate memory cell*. \n",
    "3. **What elements of the information saved in my memory storage $c_t$ are relevant for the computation of the next hidden-state $h_t$?** Select them from $c_t$, combine them new hidden-state output, and let the pass into the next hidden-state $h_t$. This is controlled by the *output gate* and the *tanh* function. \n",
    "\n",
    "Decisions 1 and 2 will determine the information that keeps flowing through the memory storage at the top. Decision 3 will determine the information that flows to the next hidde state at the bottom. The conjunction of these decisions sometimes is called \"memory block\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> Figure 6: LSTM as a sequence of decisions </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/rec-net/lstm-choices.svg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To put LSTMs in context, imagine the following simplified scenerio: we are trying to **predict the next word in a sequence**. Let's say, squences are about sports. From past sequences, we saved in the memory block the type of sport: \"soccer\". For the current sequence, we receive a phrase like \"A basketball player...\". In such a case, we first want to \"forget\" the previous type of sport \"soccer\" (*decision 1*) by multplying $c_{t-1} \\odot f_t$. Next, we want to \"update\" memory with the new type of sport, \"basketball\" (*decision 2*), by adding $c_t = (c_{t-1} \\odot f_t) + (i_t \\odot \\tilde{c_t})$. Finally, we want to output (*decision 3*) a verb relevant for \"A basketball player...\", like \"shoot\" or \"dunk\" by $\\hat{y_t} = softmax(W_{hz}h_t + b_z)$.\n",
    "\n",
    "LSTMs long-term memory capabilities make them good at capturing long-term dependencies. The memory cell effectivelly counteract the vanishing gradient problem at preserving information as long the forget gate does not \"erase\" past information (Graves, 2012). All the above make LSTMs succesful in practical applications in sequence-modeling (see a list [here](https://en.wikipedia.org/wiki/Long_short-term_memory#Applications)). For instance, when you use [Google's Voice Transcription](https://ai.googleblog.com/2015/08/the-neural-networks-behind-google-voice.html) services there is a RNN doing the hard work of recognizing your voice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNNs and cognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with Convolutional Neural Networks, researchers utilizing RNN for approaching sequential problems like natural languague processing (NLP) or time-series prediction, do not *necessarily* care about (althouh some might) how good of a model of cognition and brain-activity are RNNs. What they really care is about solving problems like translation, speech recognition, and stock market prediction, and many advances in the field come from pursuing such goals. Still, RNN have many **desirable traits as model of neuro-cognitive activity**, and have been prolifically used to **model several aspects of human cognition and behavior**: child behavior in a object permanence tasks (Munakata et all, 1997); knowledge-intensive text-comprehension (St. John, 1992); processing in quasi-regular domains, like English word reading (Plaut et al., 1996); human performance in processing recursive language structures (Christiansen & Chater, 1999); human sequential action (Botvinick & Plaut, 2004); movement patterns in typical and atypical developing children (Muñoz-Organero et al., 2019). And many others. Neuroscientist have used RNNs to model a wide variety of aspects as well (for reviews see Barak, 2017, Güçlü & van Gerven, 2017, Jarne & Laje, 2019). Overall, RNN have demostrated to be a productive tool for modeling cognitive and brain function, in distributed representations paradigm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematical formalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two mathematically complex issues with RNNs: (1) computing hidden-states, and (2) backpropagation. The rest are common operations found in multilayer-perceptrons. LSTMs and its many variants are the facto standard when modeling any kind of sequential problem. Elman networks can be seen as a simplified version of a LSTM, so I'll focus my attention on LSTMs for the most part. My exposition is based in a combination of sources that you may want to review for extended explanations (Bengio et al., 1994; Hochreiter & Schmidhuber, 1997; Graves, 2012; Chen, 2016; Zhang et al., 2020).\n",
    "\n",
    "The LSTM architecture can be desribed by: \n",
    "\n",
    "**Forward pass**:\n",
    "- non-linear forget function\n",
    "- non-linear input function \n",
    "- non-linear candidate-memory function\n",
    "- non-linear output function\n",
    "- memory cell function\n",
    "- non-linear hidden-state function \n",
    "- softmax function (output)\n",
    "\n",
    "**Backward pass**:\n",
    "- Cost-function\n",
    "- Learning procedure (backpropagation)\n",
    "\n",
    "Following the indices for each function requires some definitions. I'll assume we have $h$ hidden units, training sequences of size $n$, and $d$ input units. \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{input-units} &= x_i \\in \\mathbb{R}^d \\\\ \n",
    "\\text{training-sequence} &= s_i \\in \\mathbb{R}^n \\\\\n",
    "\\text{output-class} &= y_i \\in \\mathbb{R}^k \\\\\n",
    "\\text{Input-layer} &= X_t \\in \\mathbb{R}^{n\\times d} \\\\\n",
    "\\text{hidden-layer} &= H_t \\in \\mathbb{R}^{n\\times h}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forget function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **forget function** is a sigmoidal mapping combining three elements: input vector $x_t$, past hidden-state $h_{t-1}$, and a bias term $b_f$. We didn't mentioned the bias before, but it is the same bias that all neural networks incorporate, one for each unit in $f$. More formally:\n",
    "\n",
    "$$\n",
    "f_t = \\sigma(W_{xf}x_t + W_{hf}h_{t-1} + b_f)\n",
    "$$ \n",
    "\n",
    "Each matrix $W$ has dimensionality equal to (number of incoming units, number for connected units). For example, $W_{xf}$ referes to $W_{input-units, forget-units}$. Keep this in mind to read the indices of the $W$ matrices for subsequent definitions.\n",
    "\n",
    "Here is an important insight: What would it happen if $f_t = 0$? If you look at the diagram in **Figure 6**, $f_t$ performs an elementwise multiplication of each element in $c_{t-1}$, meaning that every value would be reduced to $0$. In short, the network would completely \"forget\" past states. Naturally, if $f_t = 1$, the network would keep its memory intact. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input function and Candidate memory function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **input function** is a sigmoidal mapping combining three elements: input vector $x_t$, past hidden-state $h_{t-1}$, and a bias term $b_f$. It's defined as:\n",
    "\n",
    "$$\n",
    "i_t = \\sigma(W_{xi}x_t + W_{hi}h_{t-1} + b_i)\n",
    "$$ \n",
    "\n",
    "The **candidate memory function** is an hyperbolic tanget function combining the same elements that $i_t$. It's defined as:\n",
    "\n",
    "$$\n",
    "\\tilde{c}_t = tanh(W_{xc}x_t + W_{hc}h_{t-1} + b_c)\n",
    "$$ \n",
    "\n",
    "Both functions are combined to update the memory cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **output function** is a sigmoidal mapping combining three elements: input vector $x_t$, past hidden-state $h_{t-1}$, and a bias term $b_f$. Is defined as:\n",
    "\n",
    "$$\n",
    "o_t = \\sigma(W_{xo}x_t + W_{ho}h_{t-1} + b_o)\n",
    "$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory cell function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **memory cell function** (what I've been calling \"memory storage\" for conceptual clarity), combines the effect of the forget function, input function, and candidate memory function. It's defined as:\n",
    "\n",
    "$$\n",
    "c_t = (c_{t-1} \\odot f_t) + (i_t \\odot \\tilde{c_t})\n",
    "$$\n",
    "\n",
    "Where $\\odot$ implies an elementwise multiplication (instead of the usual dot product). This expands to:\n",
    "\n",
    "$$\n",
    "c_t = (c_{t-1} \\odot \\sigma(W_{xf}x_t + W_{hf}h_{t-1} + b_f)) + (\\sigma(W_{xi}x_t + W_{hi}h_{t-1} + b_i) \\odot tanh(W_{xc}x_t + W_{hc}h_{t-1} + b_c))\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hidden state function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next **hidden state function** combines the effect of the output function and the contents of the memory cell scaled by a tanh function. It is defined as:\n",
    "\n",
    "$$\n",
    "h_t = O_t \\odot tanh(c_t) \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output function will depend upon the problem to be approached. For our our purposes, we will assume a multi-class problem, for which the **softmax function** is appropiated. For this, we first pass the hidden-state by a linear function, and then the softmax as:\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "z_t &= (W_{hz}h_t + b_z)\\\\\n",
    "\\hat{y}_t &= softmax(z_t) = \\frac{e^{z_t}}{\\sum_{j=1}^K e^{z_j}} \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The softmax computes the exponent for each $z_t$ and then normalize by dividing by the sum of every output value exponentiated. In this manner, the output of the softmax can be interpreted as the likelihood value $p$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost function "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with the output function, the cost function will depend upon the problem. For regression problems, the Mean-Squared Error can be used. For our purposes (classification), the cross-entropy function is appropiated. It's defined as:\n",
    "\n",
    "$$\n",
    "E_i = - \\sum_t y_ilog(p_i)\n",
    "$$\n",
    "\n",
    "Where $y_i$ is the true label for the $ith$ output unit, and $log(p_i)$ is the log of the softmax value for the $ith$ output unit. The summation indicates we need to aggregate the cost at each time-step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning procedure: Backpropagation Through Time (BPTT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Originally, Hochreiter and Schmidhuber (1997) trained LSTMs with a combination of approximate gradient descent computed with a combination of real-time recurrent learning and backpropagation through time (BPTT). Nevertheless, LSTM can be trained with pure backpropagation. Following Graves (2012), I'll only describe BTT because is more accurate,  easier to debug, and to describe.\n",
    "\n",
    "**Note**: we call it **backpropagation through time** because of the sequential time-dependent structure of RNNs. Recall that each layer represents a time-step, and forward propagation happens in sequence, one layer computed after the other. Hence, when we backpropagate, we do the same but backwards (i.e., \"through time\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> Figure 7: Three-layer simplified RNN </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/rec-net/simple-rnn.svg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I reviewed backpropagation for a simple multilayer perceptron [here](https://com-cog-book.github.io/com-cog-book/features/multilayer-perceptron.html#Backpropagation-algorithm). Nevertheless, I'll sketch BPTT for the simplest case as shown in **Figure 7**, this is, with a genereic non-linear hidden-layer similar to Elman network withouth \"context units\" (some like to call it \"vanilla\" RNN, which I avoid because I beleive is derogatory against vanilla). This exercise will allow us to review backpropagation and to understand how it differs from BPTT. We begin by defining a simplified RNN as: \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "z_t &= W_{hz}h_t + b_z\\\\\n",
    "h_t &= \\sigma(W_{hh}h_{t-1} + W_{xh}x_t+b_h)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Where $h_t$ and $z_t$ indicates a hidden-state (or layer) and  the output respectively. Therefore, **we have to compute gradients w.r.t. five sets of weights**: $\\{W_{hz}, W_{hh}, W_{xh}, b_z, b_h\\}$.\n",
    "\n",
    "\n",
    "\n",
    "First, consider the error derivatives w.r.t. $W_{hz}$ at time $t$, the weight matrix for the linear function at the output layer. Recall that $W_{hz}$ is shared across all time-steps, hence, we can compute the gradients at each time step and then take the sum as:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial{E}}{\\partial{W_{hz}}} = \\sum_t\\frac{\\partial{E}}{\\partial{z_t}} \\frac{\\partial{z_t}}{\\partial{W_{hz}}}\n",
    "$$\n",
    "\n",
    "Same for the bias term:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial{E}}{\\partial{b_z}} = \\sum_t\\frac{\\partial{E}}{\\partial{z_t}} \\frac{\\partial{z_t}}{\\partial{b_z}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That part is straightforward. The issue arises when we try to compute the gradients w.r.t. the wights $W_{hh}$ in the hidden layer. Consider a three layer RNN (i.e., unfolded over three time-steps). In such a case, we have: \n",
    "\n",
    "- $E_3$ depens on $z_3$\n",
    "- $z_3$ depends on $h_3$\n",
    "- $h_3$ depens on $h_2$\n",
    "- $h_2$ depens on $h_1$\n",
    "- $h_1$ depens on $h_0$, where $h_0$ is a random starting state.\n",
    "\n",
    "Now, we have that $E_3$ w.r.t to $h_3$ becomes:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial{E_3}}{\\partial{W_{hh}}} = \n",
    "\\frac{\\partial{E_3}}{\\partial{z_3}}\n",
    "\\frac{\\partial{z_3}}{\\partial{h_3}}\n",
    "\\frac{\\partial{h_3}}{\\partial{W_{hh}}}\n",
    "$$\n",
    "\n",
    "The issue here is that $h_3$ depends on $h_2$, since according to our definition, the $W_{hh}$ is multiplied by $h_{t-1}$, meaning **we can't compute $\\frac{\\partial{h_3}}{\\partial{W_{hh}}}$ directly**. Othewise, we would be treating $h_2$ as a constant, which is incorrect: is a function. What we need to do is to **compute the gradients separately**: the direct contribution of ${W_{hh}}$ on $E$ and the indirect contribution via $h_2$. Following the rules of calculus in multiple variables, we compute them independently and add them up together as:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial{E_3}}{\\partial{W_{hh}}} = \n",
    "\\frac{\\partial{E_3}}{\\partial{z_3}}\n",
    "\\frac{\\partial{z_3}}{\\partial{h_3}}\n",
    "\\frac{\\partial{h_3}}{\\partial{W_{hh}}}+\n",
    "\\frac{\\partial{E_3}}{\\partial{z_3}}\n",
    "\\frac{\\partial{z_3}}{\\partial{h_3}}\n",
    "\\frac{\\partial{h_3}}{\\partial{h_2}}\n",
    "\\frac{\\partial{h_2}}{\\partial{W_{hh}}}\n",
    "$$\n",
    "\n",
    "Again, we have that we can't compute $\\frac{\\partial{h_2}}{\\partial{W_{hh}}}$ directly. Following the same procedure, we have that our full expression becomes:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial{E_3}}{\\partial{W_{hh}}} = \n",
    "\\frac{\\partial{E_3}}{\\partial{z_3}}\n",
    "\\frac{\\partial{z_3}}{\\partial{h_3}}\n",
    "\\frac{\\partial{h_3}}{\\partial{W_{hh}}}+\n",
    "\\frac{\\partial{E_3}}{\\partial{z_3}}\n",
    "\\frac{\\partial{z_3}}{\\partial{h_3}}\n",
    "\\frac{\\partial{h_3}}{\\partial{h_2}}\n",
    "\\frac{\\partial{h_2}}{\\partial{W_{hh}}}+\n",
    "\\frac{\\partial{E_3}}{\\partial{z_3}}\n",
    "\\frac{\\partial{z_3}}{\\partial{h_3}}\n",
    "\\frac{\\partial{h_3}}{\\partial{h_2}}\n",
    "\\frac{\\partial{h_2}}{\\partial{h_1}}\n",
    "\\frac{\\partial{h_1}}{\\partial{W_{hh}}}\n",
    "$$\n",
    "\n",
    "Essentially, this means that we compute and add the contribution of $W_{hh}$ to $E$ at each time-step. The expression for $b_h$ is the same:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial{E_3}}{\\partial{b_h}} = \n",
    "\\frac{\\partial{E_3}}{\\partial{z_3}}\n",
    "\\frac{\\partial{z_3}}{\\partial{h_3}}\n",
    "\\frac{\\partial{h_3}}{\\partial{b_h}}+\n",
    "\\frac{\\partial{E_3}}{\\partial{z_3}}\n",
    "\\frac{\\partial{z_3}}{\\partial{h_3}}\n",
    "\\frac{\\partial{h_3}}{\\partial{h_2}}\n",
    "\\frac{\\partial{h_2}}{\\partial{b_h}}+\n",
    "\\frac{\\partial{E_3}}{\\partial{z_3}}\n",
    "\\frac{\\partial{z_3}}{\\partial{h_3}}\n",
    "\\frac{\\partial{h_3}}{\\partial{h_2}}\n",
    "\\frac{\\partial{h_2}}{\\partial{h_1}}\n",
    "\\frac{\\partial{h_1}}{\\partial{b_h}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we need to compute the gradients w.r.t. $W_{xh}$. Here, again, we have to add the contributions of $W_{xh}$ via $h_3$, $h_2$, and $h_1$: \n",
    "\n",
    "$$\n",
    "\\frac{\\partial{E_3}}{\\partial{W_{xh}}} = \n",
    "\\frac{\\partial{E_3}}{\\partial{z_3}}\n",
    "\\frac{\\partial{z_3}}{\\partial{h_3}}\n",
    "\\frac{\\partial{h_3}}{\\partial{W_{xh}}}+\n",
    "\\frac{\\partial{E_3}}{\\partial{z_3}}\n",
    "\\frac{\\partial{z_3}}{\\partial{h_3}}\n",
    "\\frac{\\partial{h_3}}{\\partial{h_2}}\n",
    "\\frac{\\partial{h_2}}{\\partial{W_{xh}}}+\n",
    "\\frac{\\partial{E_3}}{\\partial{z_3}}\n",
    "\\frac{\\partial{z_3}}{\\partial{h_3}}\n",
    "\\frac{\\partial{h_3}}{\\partial{h_2}}\n",
    "\\frac{\\partial{h_2}}{\\partial{h_1}}\n",
    "\\frac{\\partial{h_1}}{\\partial{W_{xh}}}\n",
    "$$\n",
    "\n",
    "That's for BPTT for a simple RNN. The math reviewed here generalize with minimal changes to more complex architectures as LSTMs. Actually, the only difference regarding LSTMs, is that we have more weights to differentiate for. Instead of a single generic $W_{hh}$, we have $W$ for all the gates: forget, input, output, and candidate cell. The rest remains the same. For a detailed derivation of BPTT fot the LSTM see [Graves (2012)](https://www.cs.toronto.edu/~graves/preprint.pdf) and [Chen (2016)](https://arxiv.org/abs/1610.02583)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interlude: Sequence-data representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working with sequence-data, like text or time-series, requieres to pre-process it in a manner that is \"digestible\" for RNNs. As with any neural network, RNN can't take raw text as an input, we need to **parse** text sequences and then **\"map\"** them into vectors of numbers. Here I'll briefly review these issues to provide enough context for our example applications. For an extended revision please refer to [Jurafsky and Martin (2019)](https://web.stanford.edu/~jurafsky/slp3/), [Goldberg (2015)](http://u.cs.biu.ac.il/~yogo/nnlp.pdf), [Chollet (2017)](https://www.manning.com/books/deep-learning-with-python), and [Zhang et al (2020)](https://d2l.ai/chapter_recurrent-neural-networks/index.html).\n",
    "\n",
    "Parsing can be done in multiple manners, the most common being: \n",
    "\n",
    "- Using **word** as unit, which each word represented as a vector\n",
    "- Using **character** as a unit, with each character represented as a vector\n",
    "- Using **n-grams** of words or characters as unit, with each n-gram represented as a vector. N-grams are sets of words or characters of size \"N\" or less.\n",
    "\n",
    "The process of parsing text into smaller units is called \"tokenization\", and each resulting unit is called a \"token\", the top pane in **Figure 8** display a sketch of the tokenization process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> Figure 8: Tokenization </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/rec-net/text-pro.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once a corpus of text has been parsed into tokens, we have to map such tokens into numerical vectors. Two common ways to do this are **one-hot encoding** approach and the **word embeddings** approach, as depicted in the bottom pane of **Figure 8**. We used one-hot encodings to transform the MNIST class-labels into vectors of numbers for classification in the [CovNets chapter](https://com-cog-book.github.io/com-cog-book/features/cov-net.html). In a one-hot encoding vector, each token is mapped into a *unique* vector of zeros and ones. The vector size is determined by the vocabullary size. For instance, for the set $x= \\{\"cat\", \"dog\", \"ferret\"\\}$, we could use a 3-dimensional one-hot encoding as:\n",
    "\n",
    "$$\n",
    "\\text{cat}=\n",
    "\\begin{bmatrix}\n",
    "1 \\\\\n",
    "0 \\\\\n",
    "0\n",
    "\\end{bmatrix},\n",
    "\\text{dog}=\n",
    "\\begin{bmatrix}\n",
    "0 \\\\\n",
    "1 \\\\\n",
    "0\n",
    "\\end{bmatrix},\n",
    "\\text{ferret}=\n",
    "\\begin{bmatrix}\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "One-hot encodings have the advantages of being straighforward to implement, and to provide an unique identifier for each token. Its main disadvantage is that tends to create really **sparse** and **high-dimensional** representations for large corpus of texts. For instance, if you tried a one-hot encoding for 50,000 tokens, you'd end up with a 50,000 by 50,000 dimensional matrix, which it may be unpractical for most tasks. \n",
    "\n",
    "**Word embeddings** represent text by mapping tokens into vectors of real-valued numbers instead of only zeros and ones. This significantly increments the representational capacity of vectors, reducing the required dimensionality for a given corpus of text compared to one-hot encodings. For instance, 50,000 tokens could be represented by as little as 2 or 3 vectors (although the representation may not be very good). Taking the same set $x$ as before, we could have a 2-dimensional word embedding like:\n",
    "\n",
    "$$\n",
    "\\text{cat}=\n",
    "\\begin{bmatrix}\n",
    "0.1 \\\\\n",
    "0.8\n",
    "\\end{bmatrix},\n",
    "\\text{dog}=\n",
    "\\begin{bmatrix}\n",
    "0.2 \\\\\n",
    "1 \n",
    "\\end{bmatrix},\n",
    "\\text{ferret}=\n",
    "\\begin{bmatrix}\n",
    "0.6 \\\\\n",
    "0.2\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "You may be wondering why to bother with one-hot encodings when word embeddings are much more space-efficient. The main issue with word-embedding is that **there isn't an obvious way to map tokens into vectors** as with one-hot encodings. For instance, you could assign tokens to vectors at random (assuming every token is assigned to a *unique* vector). The problem with such approach is that the semantic structure in the corpus is broken. Ideally, you want words of similar meaning mapped into similar vectors. We can preserve the semantic structure of a text corpus in the same manner than everything else in machine learning: by **learning from data**. There are two ways to do this:\n",
    "\n",
    "- Learning the word embeddings **at the same time** you train the RNN.\n",
    "- Utilizing **pretrained** word embeddings, this is, embeddings learned in a different task. This is a form of \"transfer learning\".\n",
    "\n",
    "Learning word embeddings for your task is advisable as semantic relationships among words tend to be **context dependent**. For instance, \"exploitation\" in the context of mining is related to resource \"extraction\", hence relative neutral. But, \"exploitation\" in the context of labor rights is related to the idea of \"abuse\", hence a negative connotation. This is more critical when we are dealing with different languages. Nevertheless, learning embeddings for every task sometimes is impractical, either because your corpus is too \"small\" (i.e., not enough data to extract semantic relantionships), or too \"large\" (i.e., you don't have enough time and/or resources to learn the embeddings). Examples of freely accessible pretrained word embeddings are Google's [Word2vec](https://code.google.com/archive/p/word2vec/) and the [Global Vectors for Word Representation](https://nlp.stanford.edu/projects/glove/) (GloVe)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in previous chapters, I'll use Keras to implement both (a modified version of) the Elman Network for the XOR problem and a LSTM for text prediction.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elman Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By now, it may be clear to you that Elman networks are a simple RNN with two neurons, one for each input pattern, in the hidden-state. Originally, Elman trained his architecture with a truncated version of BPTT, meaning that only considered two time-steps for computing the gradients, $t$ and $t-1$. We will implement a modified version of Elman's architecture bypassing the \"contex\" unit (which does not alter the result at all) and utilizing BPTT instead of its truncated version. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nowadays, we don't to generate the 3,000 bits sequence that Elman used in his original work. We can simply generate a single pair of training and testing for the XOR problem, and pass the training sequence (lenght two) as the inputs, and the expected outputs as the target. This is very much alike any classification task. An important caveat is that simpleRNN layes in Keras expect an input tensor of shape (number-samples, timesteps, number-input-features). In out case, this has to be: number-samples= 4,  timesteps=1, number-input-features=2. No separate encoding is necessary here because we are manually setting the input and output values to binary vector representations. Finally, we won't worry about training and testing sets for this example, which is way to simple for that (we will do that for the next example)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table 1**: Truth Table For XOR Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| $x_1$ | $x_2$ | $y$ |\n",
    "|---|---|--------|\n",
    "| 0 | 0 | 0      |\n",
    "| 0 | 1 | 1      |\n",
    "| 1 | 0 | 1      |\n",
    "| 1 | 1 | 0      |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, SimpleRNN\n",
    "import altair as alt\n",
    "from numpy.random import seed\n",
    "seed(12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data shape: (4, 1, 2)\n",
      "targets data shape: (4, 1)\n"
     ]
    }
   ],
   "source": [
    "# features\n",
    "X = np.array([[[0, 0, 1, 1]],\n",
    "              [[0, 1, 0, 1]]]).T\n",
    "\n",
    "# expected values\n",
    "y = np.array([[0, 1, 1, 0]]).T\n",
    "\n",
    "print(f'training data shape: {X.shape}')\n",
    "print(f'targets data shape: {y.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elman network architecture in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining a (modified) in Keras is extremely simple as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a network as a linear stack of layers\n",
    "model = Sequential()\n",
    "\n",
    "# Add a recurrent layer with 2 units\n",
    "model.add(SimpleRNN(2, input_shape=(1, 2)))\n",
    "\n",
    "# Add the output layer with a sigmoid activation\n",
    "model.add(Dense(1, activation='tanh'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model summary shows that our architecture yields 13 trainable parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "simple_rnn_1 (SimpleRNN)     (None, 2)                 10        \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 3         \n",
      "=================================================================\n",
      "Total params: 13\n",
      "Trainable params: 13\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elman network Application: XOR classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we compile and fit our model. I'll utilie [Adadelta](https://github.com/keras-team/keras/blob/master/keras/optimizers.py#L376) (to avoid manually adjusting the learning rate) as the optimizer, and the Mean-Squared Error (as in Elman original work). I'll train the model for 15,000 epochs over the 4 samples dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='Adadelta', \n",
    "              loss='mean_squared_error', \n",
    "              metrics=['acc'])\n",
    "history = model.fit(X, y,\n",
    "                    epochs=5000,\n",
    "                    verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Chart 2** shows the error curve (red, right axis), and the accuracy curve (blue, left axis) for each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-d3fdf6ce52ca43018d724a63fc30c3b1\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  (function(spec, embedOpt){\n",
       "    const outputDiv = document.getElementById(\"altair-viz-d3fdf6ce52ca43018d724a63fc30c3b1\");\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.0.2?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function loadScript(lib) {\n",
       "      return new Promise(function(resolve, reject) {\n",
       "        var s = document.createElement('script');\n",
       "        s.src = paths[lib];\n",
       "        s.async = true;\n",
       "        s.onload = () => resolve(paths[lib]);\n",
       "        s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "        document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "      });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else if (typeof vegaEmbed === \"function\") {\n",
       "      displayChart(vegaEmbed);\n",
       "    } else {\n",
       "      loadScript(\"vega\")\n",
       "        .then(() => loadScript(\"vega-lite\"))\n",
       "        .then(() => loadScript(\"vega-embed\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"layer\": [{\"mark\": {\"type\": \"line\", \"color\": \"blue\"}, \"encoding\": {\"x\": {\"type\": \"quantitative\", \"field\": \"time-step\"}, \"y\": {\"type\": \"quantitative\", \"field\": \"accuracy\"}}}, {\"mark\": {\"type\": \"line\", \"color\": \"red\"}, \"encoding\": {\"x\": {\"type\": \"quantitative\", \"field\": \"time-step\"}, \"y\": {\"type\": \"quantitative\", \"field\": \"loss\"}}}], \"data\": {\"name\": \"data-0549baac71c0ac0d808fb26352b6c2e4\"}, \"resolve\": {\"scale\": {\"y\": \"independent\"}}, \"title\": \"Chart 2\", \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.0.2.json\", \"datasets\": {\"data-0549baac71c0ac0d808fb26352b6c2e4\": [{\"accuracy\": 0.5, \"loss\": 0.7890881896018982, \"time-step\": 0}, {\"accuracy\": 0.5, \"loss\": 0.7832838296890259, \"time-step\": 1}, {\"accuracy\": 0.5, \"loss\": 0.7774494290351868, \"time-step\": 2}, {\"accuracy\": 0.5, \"loss\": 0.7716150879859924, \"time-step\": 3}, {\"accuracy\": 0.5, \"loss\": 0.7657961249351501, \"time-step\": 4}, {\"accuracy\": 0.5, \"loss\": 0.760002076625824, \"time-step\": 5}, {\"accuracy\": 0.5, \"loss\": 0.7542392015457153, \"time-step\": 6}, {\"accuracy\": 0.5, \"loss\": 0.7485121488571167, \"time-step\": 7}, {\"accuracy\": 0.5, \"loss\": 0.742824137210846, \"time-step\": 8}, {\"accuracy\": 0.5, \"loss\": 0.7371782064437866, \"time-step\": 9}, {\"accuracy\": 0.5, \"loss\": 0.7315762639045715, \"time-step\": 10}, {\"accuracy\": 0.5, \"loss\": 0.726020097732544, \"time-step\": 11}, {\"accuracy\": 0.5, \"loss\": 0.7205111980438232, \"time-step\": 12}, {\"accuracy\": 0.5, \"loss\": 0.7150506377220154, \"time-step\": 13}, {\"accuracy\": 0.5, \"loss\": 0.7096396088600159, \"time-step\": 14}, {\"accuracy\": 0.5, \"loss\": 0.7042787075042725, \"time-step\": 15}, {\"accuracy\": 0.5, \"loss\": 0.698968768119812, \"time-step\": 16}, {\"accuracy\": 0.5, \"loss\": 0.6937103271484375, \"time-step\": 17}, {\"accuracy\": 0.5, \"loss\": 0.6885038018226624, \"time-step\": 18}, {\"accuracy\": 0.5, \"loss\": 0.683349609375, \"time-step\": 19}, {\"accuracy\": 0.5, \"loss\": 0.6782480478286743, \"time-step\": 20}, {\"accuracy\": 0.5, \"loss\": 0.6731993556022644, \"time-step\": 21}, {\"accuracy\": 0.5, \"loss\": 0.6682037115097046, \"time-step\": 22}, {\"accuracy\": 0.5, \"loss\": 0.6632611155509949, \"time-step\": 23}, {\"accuracy\": 0.5, \"loss\": 0.6583716869354248, \"time-step\": 24}, {\"accuracy\": 0.5, \"loss\": 0.6535354852676392, \"time-step\": 25}, {\"accuracy\": 0.5, \"loss\": 0.6487525105476379, \"time-step\": 26}, {\"accuracy\": 0.5, \"loss\": 0.6440225839614868, \"time-step\": 27}, {\"accuracy\": 0.5, \"loss\": 0.6393455862998962, \"time-step\": 28}, {\"accuracy\": 0.5, \"loss\": 0.634721577167511, \"time-step\": 29}, {\"accuracy\": 0.5, \"loss\": 0.6301501393318176, \"time-step\": 30}, {\"accuracy\": 0.5, \"loss\": 0.6256312131881714, \"time-step\": 31}, {\"accuracy\": 0.5, \"loss\": 0.6211645603179932, \"time-step\": 32}, {\"accuracy\": 0.5, \"loss\": 0.6167500019073486, \"time-step\": 33}, {\"accuracy\": 0.5, \"loss\": 0.6123871803283691, \"time-step\": 34}, {\"accuracy\": 0.5, \"loss\": 0.6080758571624756, \"time-step\": 35}, {\"accuracy\": 0.5, \"loss\": 0.6038156151771545, \"time-step\": 36}, {\"accuracy\": 0.5, \"loss\": 0.5996062159538269, \"time-step\": 37}, {\"accuracy\": 0.5, \"loss\": 0.5954474806785583, \"time-step\": 38}, {\"accuracy\": 0.5, \"loss\": 0.5913386940956116, \"time-step\": 39}, {\"accuracy\": 0.5, \"loss\": 0.5872798562049866, \"time-step\": 40}, {\"accuracy\": 0.5, \"loss\": 0.5832703113555908, \"time-step\": 41}, {\"accuracy\": 0.5, \"loss\": 0.57930988073349, \"time-step\": 42}, {\"accuracy\": 0.5, \"loss\": 0.5753979682922363, \"time-step\": 43}, {\"accuracy\": 0.5, \"loss\": 0.571534276008606, \"time-step\": 44}, {\"accuracy\": 0.5, \"loss\": 0.5677182674407959, \"time-step\": 45}, {\"accuracy\": 0.5, \"loss\": 0.5639497637748718, \"time-step\": 46}, {\"accuracy\": 0.5, \"loss\": 0.5602279901504517, \"time-step\": 47}, {\"accuracy\": 0.5, \"loss\": 0.5565528869628906, \"time-step\": 48}, {\"accuracy\": 0.5, \"loss\": 0.5529236793518066, \"time-step\": 49}, {\"accuracy\": 0.5, \"loss\": 0.5493400692939758, \"time-step\": 50}, {\"accuracy\": 0.5, \"loss\": 0.5458015203475952, \"time-step\": 51}, {\"accuracy\": 0.5, \"loss\": 0.5423075556755066, \"time-step\": 52}, {\"accuracy\": 0.5, \"loss\": 0.5388579368591309, \"time-step\": 53}, {\"accuracy\": 0.5, \"loss\": 0.5354518294334412, \"time-step\": 54}, {\"accuracy\": 0.5, \"loss\": 0.5320889949798584, \"time-step\": 55}, {\"accuracy\": 0.5, \"loss\": 0.5287688970565796, \"time-step\": 56}, {\"accuracy\": 0.5, \"loss\": 0.5254911184310913, \"time-step\": 57}, {\"accuracy\": 0.5, \"loss\": 0.5222550630569458, \"time-step\": 58}, {\"accuracy\": 0.5, \"loss\": 0.5190603137016296, \"time-step\": 59}, {\"accuracy\": 0.5, \"loss\": 0.5159063935279846, \"time-step\": 60}, {\"accuracy\": 0.5, \"loss\": 0.5127928853034973, \"time-step\": 61}, {\"accuracy\": 0.5, \"loss\": 0.5097192525863647, \"time-step\": 62}, {\"accuracy\": 0.5, \"loss\": 0.5066849589347839, \"time-step\": 63}, {\"accuracy\": 0.5, \"loss\": 0.5036896467208862, \"time-step\": 64}, {\"accuracy\": 0.5, \"loss\": 0.5007327198982239, \"time-step\": 65}, {\"accuracy\": 0.5, \"loss\": 0.4978138208389282, \"time-step\": 66}, {\"accuracy\": 0.5, \"loss\": 0.4949324429035187, \"time-step\": 67}, {\"accuracy\": 0.5, \"loss\": 0.4920880198478699, \"time-step\": 68}, {\"accuracy\": 0.5, \"loss\": 0.48928022384643555, \"time-step\": 69}, {\"accuracy\": 0.5, \"loss\": 0.48650848865509033, \"time-step\": 70}, {\"accuracy\": 0.5, \"loss\": 0.4837724268436432, \"time-step\": 71}, {\"accuracy\": 0.5, \"loss\": 0.48107147216796875, \"time-step\": 72}, {\"accuracy\": 0.5, \"loss\": 0.47840529680252075, \"time-step\": 73}, {\"accuracy\": 0.5, \"loss\": 0.4757733941078186, \"time-step\": 74}, {\"accuracy\": 0.5, \"loss\": 0.4731753170490265, \"time-step\": 75}, {\"accuracy\": 0.5, \"loss\": 0.4706106185913086, \"time-step\": 76}, {\"accuracy\": 0.5, \"loss\": 0.4680787920951843, \"time-step\": 77}, {\"accuracy\": 0.5, \"loss\": 0.4655795097351074, \"time-step\": 78}, {\"accuracy\": 0.5, \"loss\": 0.4631122648715973, \"time-step\": 79}, {\"accuracy\": 0.5, \"loss\": 0.4606766700744629, \"time-step\": 80}, {\"accuracy\": 0.5, \"loss\": 0.45827221870422363, \"time-step\": 81}, {\"accuracy\": 0.5, \"loss\": 0.45589861273765564, \"time-step\": 82}, {\"accuracy\": 0.5, \"loss\": 0.45355528593063354, \"time-step\": 83}, {\"accuracy\": 0.5, \"loss\": 0.45124194025993347, \"time-step\": 84}, {\"accuracy\": 0.5, \"loss\": 0.44895821809768677, \"time-step\": 85}, {\"accuracy\": 0.5, \"loss\": 0.4467034637928009, \"time-step\": 86}, {\"accuracy\": 0.5, \"loss\": 0.44447749853134155, \"time-step\": 87}, {\"accuracy\": 0.5, \"loss\": 0.4422798454761505, \"time-step\": 88}, {\"accuracy\": 0.5, \"loss\": 0.4401102364063263, \"time-step\": 89}, {\"accuracy\": 0.5, \"loss\": 0.43796807527542114, \"time-step\": 90}, {\"accuracy\": 0.5, \"loss\": 0.43585315346717834, \"time-step\": 91}, {\"accuracy\": 0.5, \"loss\": 0.4337649345397949, \"time-step\": 92}, {\"accuracy\": 0.5, \"loss\": 0.4317031800746918, \"time-step\": 93}, {\"accuracy\": 0.5, \"loss\": 0.4296674132347107, \"time-step\": 94}, {\"accuracy\": 0.5, \"loss\": 0.4276573657989502, \"time-step\": 95}, {\"accuracy\": 0.5, \"loss\": 0.42567262053489685, \"time-step\": 96}, {\"accuracy\": 0.5, \"loss\": 0.42371290922164917, \"time-step\": 97}, {\"accuracy\": 0.5, \"loss\": 0.42177772521972656, \"time-step\": 98}, {\"accuracy\": 0.5, \"loss\": 0.41986680030822754, \"time-step\": 99}, {\"accuracy\": 0.5, \"loss\": 0.41797977685928345, \"time-step\": 100}, {\"accuracy\": 0.5, \"loss\": 0.4161164164543152, \"time-step\": 101}, {\"accuracy\": 0.5, \"loss\": 0.41427624225616455, \"time-step\": 102}, {\"accuracy\": 0.5, \"loss\": 0.41245898604393005, \"time-step\": 103}, {\"accuracy\": 0.5, \"loss\": 0.4106643795967102, \"time-step\": 104}, {\"accuracy\": 0.5, \"loss\": 0.40889203548431396, \"time-step\": 105}, {\"accuracy\": 0.5, \"loss\": 0.40714165568351746, \"time-step\": 106}, {\"accuracy\": 0.5, \"loss\": 0.40541282296180725, \"time-step\": 107}, {\"accuracy\": 0.5, \"loss\": 0.4037053883075714, \"time-step\": 108}, {\"accuracy\": 0.5, \"loss\": 0.4020189642906189, \"time-step\": 109}, {\"accuracy\": 0.5, \"loss\": 0.400353342294693, \"time-step\": 110}, {\"accuracy\": 0.5, \"loss\": 0.3987080454826355, \"time-step\": 111}, {\"accuracy\": 0.5, \"loss\": 0.39708301424980164, \"time-step\": 112}, {\"accuracy\": 0.5, \"loss\": 0.3954777717590332, \"time-step\": 113}, {\"accuracy\": 0.5, \"loss\": 0.3938921391963959, \"time-step\": 114}, {\"accuracy\": 0.5, \"loss\": 0.39232584834098816, \"time-step\": 115}, {\"accuracy\": 0.5, \"loss\": 0.390778511762619, \"time-step\": 116}, {\"accuracy\": 0.5, \"loss\": 0.3892499506473541, \"time-step\": 117}, {\"accuracy\": 0.5, \"loss\": 0.3877398669719696, \"time-step\": 118}, {\"accuracy\": 0.5, \"loss\": 0.38624805212020874, \"time-step\": 119}, {\"accuracy\": 0.5, \"loss\": 0.38477420806884766, \"time-step\": 120}, {\"accuracy\": 0.5, \"loss\": 0.38331809639930725, \"time-step\": 121}, {\"accuracy\": 0.5, \"loss\": 0.38187941908836365, \"time-step\": 122}, {\"accuracy\": 0.5, \"loss\": 0.38045796751976013, \"time-step\": 123}, {\"accuracy\": 0.5, \"loss\": 0.3790534436702728, \"time-step\": 124}, {\"accuracy\": 0.5, \"loss\": 0.3776656985282898, \"time-step\": 125}, {\"accuracy\": 0.5, \"loss\": 0.37629440426826477, \"time-step\": 126}, {\"accuracy\": 0.5, \"loss\": 0.3749393820762634, \"time-step\": 127}, {\"accuracy\": 0.5, \"loss\": 0.37360039353370667, \"time-step\": 128}, {\"accuracy\": 0.5, \"loss\": 0.37227725982666016, \"time-step\": 129}, {\"accuracy\": 0.5, \"loss\": 0.37096962332725525, \"time-step\": 130}, {\"accuracy\": 0.5, \"loss\": 0.36967742443084717, \"time-step\": 131}, {\"accuracy\": 0.5, \"loss\": 0.36840033531188965, \"time-step\": 132}, {\"accuracy\": 0.5, \"loss\": 0.36713823676109314, \"time-step\": 133}, {\"accuracy\": 0.5, \"loss\": 0.36589083075523376, \"time-step\": 134}, {\"accuracy\": 0.5, \"loss\": 0.3646579086780548, \"time-step\": 135}, {\"accuracy\": 0.5, \"loss\": 0.36343929171562195, \"time-step\": 136}, {\"accuracy\": 0.5, \"loss\": 0.36223486065864563, \"time-step\": 137}, {\"accuracy\": 0.5, \"loss\": 0.36104434728622437, \"time-step\": 138}, {\"accuracy\": 0.5, \"loss\": 0.35986757278442383, \"time-step\": 139}, {\"accuracy\": 0.5, \"loss\": 0.3587043285369873, \"time-step\": 140}, {\"accuracy\": 0.5, \"loss\": 0.35755443572998047, \"time-step\": 141}, {\"accuracy\": 0.5, \"loss\": 0.356417715549469, \"time-step\": 142}, {\"accuracy\": 0.5, \"loss\": 0.3552941083908081, \"time-step\": 143}, {\"accuracy\": 0.5, \"loss\": 0.354183167219162, \"time-step\": 144}, {\"accuracy\": 0.5, \"loss\": 0.3530849814414978, \"time-step\": 145}, {\"accuracy\": 0.5, \"loss\": 0.3519992232322693, \"time-step\": 146}, {\"accuracy\": 0.5, \"loss\": 0.3509258031845093, \"time-step\": 147}, {\"accuracy\": 0.5, \"loss\": 0.34986457228660583, \"time-step\": 148}, {\"accuracy\": 0.5, \"loss\": 0.3488152325153351, \"time-step\": 149}, {\"accuracy\": 0.5, \"loss\": 0.347777783870697, \"time-step\": 150}, {\"accuracy\": 0.5, \"loss\": 0.3467520475387573, \"time-step\": 151}, {\"accuracy\": 0.5, \"loss\": 0.3457378149032593, \"time-step\": 152}, {\"accuracy\": 0.5, \"loss\": 0.3447349965572357, \"time-step\": 153}, {\"accuracy\": 0.5, \"loss\": 0.34374338388442993, \"time-step\": 154}, {\"accuracy\": 0.5, \"loss\": 0.34276288747787476, \"time-step\": 155}, {\"accuracy\": 0.5, \"loss\": 0.34179335832595825, \"time-step\": 156}, {\"accuracy\": 0.5, \"loss\": 0.3408345580101013, \"time-step\": 157}, {\"accuracy\": 0.5, \"loss\": 0.33988651633262634, \"time-step\": 158}, {\"accuracy\": 0.5, \"loss\": 0.3389489948749542, \"time-step\": 159}, {\"accuracy\": 0.5, \"loss\": 0.3380219340324402, \"time-step\": 160}, {\"accuracy\": 0.5, \"loss\": 0.3371050953865051, \"time-step\": 161}, {\"accuracy\": 0.5, \"loss\": 0.33619847893714905, \"time-step\": 162}, {\"accuracy\": 0.5, \"loss\": 0.33530187606811523, \"time-step\": 163}, {\"accuracy\": 0.5, \"loss\": 0.33441516757011414, \"time-step\": 164}, {\"accuracy\": 0.5, \"loss\": 0.3335382640361786, \"time-step\": 165}, {\"accuracy\": 0.5, \"loss\": 0.3326711058616638, \"time-step\": 166}, {\"accuracy\": 0.5, \"loss\": 0.3318134844303131, \"time-step\": 167}, {\"accuracy\": 0.5, \"loss\": 0.3309652805328369, \"time-step\": 168}, {\"accuracy\": 0.5, \"loss\": 0.33012643456459045, \"time-step\": 169}, {\"accuracy\": 0.5, \"loss\": 0.32929685711860657, \"time-step\": 170}, {\"accuracy\": 0.5, \"loss\": 0.3284763693809509, \"time-step\": 171}, {\"accuracy\": 0.5, \"loss\": 0.32766494154930115, \"time-step\": 172}, {\"accuracy\": 0.5, \"loss\": 0.3268623948097229, \"time-step\": 173}, {\"accuracy\": 0.5, \"loss\": 0.3260686695575714, \"time-step\": 174}, {\"accuracy\": 0.5, \"loss\": 0.3252837061882019, \"time-step\": 175}, {\"accuracy\": 0.5, \"loss\": 0.3245072662830353, \"time-step\": 176}, {\"accuracy\": 0.5, \"loss\": 0.3237394094467163, \"time-step\": 177}, {\"accuracy\": 0.5, \"loss\": 0.32297995686531067, \"time-step\": 178}, {\"accuracy\": 0.5, \"loss\": 0.3222287893295288, \"time-step\": 179}, {\"accuracy\": 0.5, \"loss\": 0.3214859366416931, \"time-step\": 180}, {\"accuracy\": 0.5, \"loss\": 0.3207511305809021, \"time-step\": 181}, {\"accuracy\": 0.5, \"loss\": 0.32002437114715576, \"time-step\": 182}, {\"accuracy\": 0.5, \"loss\": 0.31930556893348694, \"time-step\": 183}, {\"accuracy\": 0.5, \"loss\": 0.31859469413757324, \"time-step\": 184}, {\"accuracy\": 0.5, \"loss\": 0.3178914785385132, \"time-step\": 185}, {\"accuracy\": 0.5, \"loss\": 0.3171960413455963, \"time-step\": 186}, {\"accuracy\": 0.5, \"loss\": 0.3165081739425659, \"time-step\": 187}, {\"accuracy\": 0.5, \"loss\": 0.3158278167247772, \"time-step\": 188}, {\"accuracy\": 0.5, \"loss\": 0.31515488028526306, \"time-step\": 189}, {\"accuracy\": 0.5, \"loss\": 0.31448933482170105, \"time-step\": 190}, {\"accuracy\": 0.5, \"loss\": 0.31383103132247925, \"time-step\": 191}, {\"accuracy\": 0.5, \"loss\": 0.3131798803806305, \"time-step\": 192}, {\"accuracy\": 0.5, \"loss\": 0.3125358819961548, \"time-step\": 193}, {\"accuracy\": 0.5, \"loss\": 0.3118988871574402, \"time-step\": 194}, {\"accuracy\": 0.5, \"loss\": 0.31126880645751953, \"time-step\": 195}, {\"accuracy\": 0.5, \"loss\": 0.31064561009407043, \"time-step\": 196}, {\"accuracy\": 0.5, \"loss\": 0.31002920866012573, \"time-step\": 197}, {\"accuracy\": 0.5, \"loss\": 0.30941957235336304, \"time-step\": 198}, {\"accuracy\": 0.5, \"loss\": 0.30881649255752563, \"time-step\": 199}, {\"accuracy\": 0.5, \"loss\": 0.3082199692726135, \"time-step\": 200}, {\"accuracy\": 0.5, \"loss\": 0.3076299726963043, \"time-step\": 201}, {\"accuracy\": 0.5, \"loss\": 0.3070463538169861, \"time-step\": 202}, {\"accuracy\": 0.5, \"loss\": 0.3064691424369812, \"time-step\": 203}, {\"accuracy\": 0.5, \"loss\": 0.30589812994003296, \"time-step\": 204}, {\"accuracy\": 0.5, \"loss\": 0.30533334612846375, \"time-step\": 205}, {\"accuracy\": 0.5, \"loss\": 0.3047747015953064, \"time-step\": 206}, {\"accuracy\": 0.5, \"loss\": 0.304222047328949, \"time-step\": 207}, {\"accuracy\": 0.5, \"loss\": 0.3036753833293915, \"time-step\": 208}, {\"accuracy\": 0.5, \"loss\": 0.3031346797943115, \"time-step\": 209}, {\"accuracy\": 0.5, \"loss\": 0.3025997579097748, \"time-step\": 210}, {\"accuracy\": 0.5, \"loss\": 0.302070677280426, \"time-step\": 211}, {\"accuracy\": 0.5, \"loss\": 0.3015472888946533, \"time-step\": 212}, {\"accuracy\": 0.5, \"loss\": 0.3010294437408447, \"time-step\": 213}, {\"accuracy\": 0.5, \"loss\": 0.3005172610282898, \"time-step\": 214}, {\"accuracy\": 0.5, \"loss\": 0.3000105619430542, \"time-step\": 215}, {\"accuracy\": 0.5, \"loss\": 0.2995092272758484, \"time-step\": 216}, {\"accuracy\": 0.5, \"loss\": 0.29901331663131714, \"time-step\": 217}, {\"accuracy\": 0.5, \"loss\": 0.2985227108001709, \"time-step\": 218}, {\"accuracy\": 0.5, \"loss\": 0.2980372905731201, \"time-step\": 219}, {\"accuracy\": 0.5, \"loss\": 0.2975570261478424, \"time-step\": 220}, {\"accuracy\": 0.5, \"loss\": 0.29708194732666016, \"time-step\": 221}, {\"accuracy\": 0.5, \"loss\": 0.29661187529563904, \"time-step\": 222}, {\"accuracy\": 0.5, \"loss\": 0.2961466908454895, \"time-step\": 223}, {\"accuracy\": 0.5, \"loss\": 0.2956864833831787, \"time-step\": 224}, {\"accuracy\": 0.5, \"loss\": 0.2952311038970947, \"time-step\": 225}, {\"accuracy\": 0.5, \"loss\": 0.2947804927825928, \"time-step\": 226}, {\"accuracy\": 0.5, \"loss\": 0.29433462023735046, \"time-step\": 227}, {\"accuracy\": 0.5, \"loss\": 0.29389333724975586, \"time-step\": 228}, {\"accuracy\": 0.75, \"loss\": 0.29345670342445374, \"time-step\": 229}, {\"accuracy\": 0.75, \"loss\": 0.29302453994750977, \"time-step\": 230}, {\"accuracy\": 0.75, \"loss\": 0.29259687662124634, \"time-step\": 231}, {\"accuracy\": 0.75, \"loss\": 0.2921735644340515, \"time-step\": 232}, {\"accuracy\": 0.75, \"loss\": 0.2917546033859253, \"time-step\": 233}, {\"accuracy\": 0.75, \"loss\": 0.2913399338722229, \"time-step\": 234}, {\"accuracy\": 0.75, \"loss\": 0.2909294068813324, \"time-step\": 235}, {\"accuracy\": 0.75, \"loss\": 0.29052308201789856, \"time-step\": 236}, {\"accuracy\": 0.75, \"loss\": 0.29012084007263184, \"time-step\": 237}, {\"accuracy\": 0.75, \"loss\": 0.28972259163856506, \"time-step\": 238}, {\"accuracy\": 0.75, \"loss\": 0.28932830691337585, \"time-step\": 239}, {\"accuracy\": 0.75, \"loss\": 0.2889380156993866, \"time-step\": 240}, {\"accuracy\": 0.75, \"loss\": 0.2885514497756958, \"time-step\": 241}, {\"accuracy\": 0.75, \"loss\": 0.2881687581539154, \"time-step\": 242}, {\"accuracy\": 0.75, \"loss\": 0.2877897620201111, \"time-step\": 243}, {\"accuracy\": 0.75, \"loss\": 0.28741443157196045, \"time-step\": 244}, {\"accuracy\": 0.75, \"loss\": 0.2870427370071411, \"time-step\": 245}, {\"accuracy\": 0.75, \"loss\": 0.2866746187210083, \"time-step\": 246}, {\"accuracy\": 0.75, \"loss\": 0.2863100469112396, \"time-step\": 247}, {\"accuracy\": 0.75, \"loss\": 0.2859489321708679, \"time-step\": 248}, {\"accuracy\": 0.75, \"loss\": 0.2855912148952484, \"time-step\": 249}, {\"accuracy\": 0.75, \"loss\": 0.28523680567741394, \"time-step\": 250}, {\"accuracy\": 0.75, \"loss\": 0.2848857641220093, \"time-step\": 251}, {\"accuracy\": 0.75, \"loss\": 0.28453803062438965, \"time-step\": 252}, {\"accuracy\": 0.75, \"loss\": 0.2841934859752655, \"time-step\": 253}, {\"accuracy\": 0.75, \"loss\": 0.28385210037231445, \"time-step\": 254}, {\"accuracy\": 0.75, \"loss\": 0.2835139036178589, \"time-step\": 255}, {\"accuracy\": 0.75, \"loss\": 0.28317874670028687, \"time-step\": 256}, {\"accuracy\": 0.75, \"loss\": 0.28284671902656555, \"time-step\": 257}, {\"accuracy\": 0.75, \"loss\": 0.28251761198043823, \"time-step\": 258}, {\"accuracy\": 0.75, \"loss\": 0.28219157457351685, \"time-step\": 259}, {\"accuracy\": 0.75, \"loss\": 0.28186842799186707, \"time-step\": 260}, {\"accuracy\": 0.75, \"loss\": 0.2815482020378113, \"time-step\": 261}, {\"accuracy\": 0.75, \"loss\": 0.2812308669090271, \"time-step\": 262}, {\"accuracy\": 0.75, \"loss\": 0.28091633319854736, \"time-step\": 263}, {\"accuracy\": 0.75, \"loss\": 0.28060463070869446, \"time-step\": 264}, {\"accuracy\": 0.75, \"loss\": 0.2802956700325012, \"time-step\": 265}, {\"accuracy\": 0.75, \"loss\": 0.27998948097229004, \"time-step\": 266}, {\"accuracy\": 0.75, \"loss\": 0.27968594431877136, \"time-step\": 267}, {\"accuracy\": 0.75, \"loss\": 0.27938514947891235, \"time-step\": 268}, {\"accuracy\": 0.75, \"loss\": 0.2790869474411011, \"time-step\": 269}, {\"accuracy\": 0.75, \"loss\": 0.2787914574146271, \"time-step\": 270}, {\"accuracy\": 0.75, \"loss\": 0.27849847078323364, \"time-step\": 271}, {\"accuracy\": 0.75, \"loss\": 0.27820807695388794, \"time-step\": 272}, {\"accuracy\": 0.75, \"loss\": 0.2779202461242676, \"time-step\": 273}, {\"accuracy\": 0.75, \"loss\": 0.2776349186897278, \"time-step\": 274}, {\"accuracy\": 0.75, \"loss\": 0.27735209465026855, \"time-step\": 275}, {\"accuracy\": 0.75, \"loss\": 0.2770717144012451, \"time-step\": 276}, {\"accuracy\": 0.75, \"loss\": 0.2767937481403351, \"time-step\": 277}, {\"accuracy\": 0.75, \"loss\": 0.2765182852745056, \"time-step\": 278}, {\"accuracy\": 0.75, \"loss\": 0.2762451767921448, \"time-step\": 279}, {\"accuracy\": 0.75, \"loss\": 0.27597442269325256, \"time-step\": 280}, {\"accuracy\": 0.75, \"loss\": 0.27570608258247375, \"time-step\": 281}, {\"accuracy\": 0.75, \"loss\": 0.2754400074481964, \"time-step\": 282}, {\"accuracy\": 0.75, \"loss\": 0.2751763164997101, \"time-step\": 283}, {\"accuracy\": 0.75, \"loss\": 0.27491483092308044, \"time-step\": 284}, {\"accuracy\": 0.75, \"loss\": 0.27465564012527466, \"time-step\": 285}, {\"accuracy\": 0.75, \"loss\": 0.2743987739086151, \"time-step\": 286}, {\"accuracy\": 0.75, \"loss\": 0.2741440534591675, \"time-step\": 287}, {\"accuracy\": 0.75, \"loss\": 0.2738915979862213, \"time-step\": 288}, {\"accuracy\": 0.75, \"loss\": 0.27364128828048706, \"time-step\": 289}, {\"accuracy\": 0.75, \"loss\": 0.2733931839466095, \"time-step\": 290}, {\"accuracy\": 0.75, \"loss\": 0.27314725518226624, \"time-step\": 291}, {\"accuracy\": 0.75, \"loss\": 0.2729034423828125, \"time-step\": 292}, {\"accuracy\": 0.75, \"loss\": 0.2726617753505707, \"time-step\": 293}, {\"accuracy\": 0.75, \"loss\": 0.2724221646785736, \"time-step\": 294}, {\"accuracy\": 0.75, \"loss\": 0.27218469977378845, \"time-step\": 295}, {\"accuracy\": 0.75, \"loss\": 0.27194926142692566, \"time-step\": 296}, {\"accuracy\": 0.75, \"loss\": 0.2717158794403076, \"time-step\": 297}, {\"accuracy\": 0.75, \"loss\": 0.2714846134185791, \"time-step\": 298}, {\"accuracy\": 0.75, \"loss\": 0.2712552845478058, \"time-step\": 299}, {\"accuracy\": 0.75, \"loss\": 0.27102798223495483, \"time-step\": 300}, {\"accuracy\": 0.75, \"loss\": 0.27080270648002625, \"time-step\": 301}, {\"accuracy\": 0.75, \"loss\": 0.27057939767837524, \"time-step\": 302}, {\"accuracy\": 0.75, \"loss\": 0.27035802602767944, \"time-step\": 303}, {\"accuracy\": 0.75, \"loss\": 0.27013862133026123, \"time-step\": 304}, {\"accuracy\": 0.75, \"loss\": 0.2699211537837982, \"time-step\": 305}, {\"accuracy\": 0.75, \"loss\": 0.26970553398132324, \"time-step\": 306}, {\"accuracy\": 0.75, \"loss\": 0.26949194073677063, \"time-step\": 307}, {\"accuracy\": 0.75, \"loss\": 0.26928016543388367, \"time-step\": 308}, {\"accuracy\": 0.75, \"loss\": 0.26907026767730713, \"time-step\": 309}, {\"accuracy\": 0.75, \"loss\": 0.2688622772693634, \"time-step\": 310}, {\"accuracy\": 0.75, \"loss\": 0.26865607500076294, \"time-step\": 311}, {\"accuracy\": 0.75, \"loss\": 0.2684517502784729, \"time-step\": 312}, {\"accuracy\": 0.75, \"loss\": 0.2682492136955261, \"time-step\": 313}, {\"accuracy\": 0.75, \"loss\": 0.268048495054245, \"time-step\": 314}, {\"accuracy\": 0.75, \"loss\": 0.2678495943546295, \"time-step\": 315}, {\"accuracy\": 0.75, \"loss\": 0.2676524519920349, \"time-step\": 316}, {\"accuracy\": 0.75, \"loss\": 0.26745709776878357, \"time-step\": 317}, {\"accuracy\": 0.75, \"loss\": 0.2672634720802307, \"time-step\": 318}, {\"accuracy\": 0.75, \"loss\": 0.26707160472869873, \"time-step\": 319}, {\"accuracy\": 0.75, \"loss\": 0.26688146591186523, \"time-step\": 320}, {\"accuracy\": 0.75, \"loss\": 0.26669302582740784, \"time-step\": 321}, {\"accuracy\": 0.75, \"loss\": 0.2665063738822937, \"time-step\": 322}, {\"accuracy\": 0.75, \"loss\": 0.2663213014602661, \"time-step\": 323}, {\"accuracy\": 0.75, \"loss\": 0.266137957572937, \"time-step\": 324}, {\"accuracy\": 0.75, \"loss\": 0.265956312417984, \"time-step\": 325}, {\"accuracy\": 0.75, \"loss\": 0.26577630639076233, \"time-step\": 326}, {\"accuracy\": 0.75, \"loss\": 0.265597939491272, \"time-step\": 327}, {\"accuracy\": 0.75, \"loss\": 0.26542118191719055, \"time-step\": 328}, {\"accuracy\": 0.75, \"loss\": 0.26524606347084045, \"time-step\": 329}, {\"accuracy\": 0.75, \"loss\": 0.2650725543498993, \"time-step\": 330}, {\"accuracy\": 0.75, \"loss\": 0.26490065455436707, \"time-step\": 331}, {\"accuracy\": 0.75, \"loss\": 0.26473039388656616, \"time-step\": 332}, {\"accuracy\": 0.75, \"loss\": 0.26456165313720703, \"time-step\": 333}, {\"accuracy\": 0.75, \"loss\": 0.26439449191093445, \"time-step\": 334}, {\"accuracy\": 0.75, \"loss\": 0.26422885060310364, \"time-step\": 335}, {\"accuracy\": 0.75, \"loss\": 0.2640647888183594, \"time-step\": 336}, {\"accuracy\": 0.75, \"loss\": 0.2639022469520569, \"time-step\": 337}, {\"accuracy\": 0.75, \"loss\": 0.26374122500419617, \"time-step\": 338}, {\"accuracy\": 0.75, \"loss\": 0.26358169317245483, \"time-step\": 339}, {\"accuracy\": 0.75, \"loss\": 0.26342371106147766, \"time-step\": 340}, {\"accuracy\": 0.75, \"loss\": 0.2632671594619751, \"time-step\": 341}, {\"accuracy\": 0.75, \"loss\": 0.2631121575832367, \"time-step\": 342}, {\"accuracy\": 0.75, \"loss\": 0.2629585862159729, \"time-step\": 343}, {\"accuracy\": 0.75, \"loss\": 0.2628064453601837, \"time-step\": 344}, {\"accuracy\": 0.75, \"loss\": 0.2626558244228363, \"time-step\": 345}, {\"accuracy\": 0.75, \"loss\": 0.2625065743923187, \"time-step\": 346}, {\"accuracy\": 0.75, \"loss\": 0.26235878467559814, \"time-step\": 347}, {\"accuracy\": 0.75, \"loss\": 0.2622123956680298, \"time-step\": 348}, {\"accuracy\": 0.75, \"loss\": 0.26206740736961365, \"time-step\": 349}, {\"accuracy\": 0.75, \"loss\": 0.26192381978034973, \"time-step\": 350}, {\"accuracy\": 0.75, \"loss\": 0.26178163290023804, \"time-step\": 351}, {\"accuracy\": 0.75, \"loss\": 0.2616408169269562, \"time-step\": 352}, {\"accuracy\": 0.75, \"loss\": 0.26150137186050415, \"time-step\": 353}, {\"accuracy\": 0.75, \"loss\": 0.26136326789855957, \"time-step\": 354}, {\"accuracy\": 0.75, \"loss\": 0.2612265348434448, \"time-step\": 355}, {\"accuracy\": 0.75, \"loss\": 0.26109111309051514, \"time-step\": 356}, {\"accuracy\": 0.75, \"loss\": 0.2609570622444153, \"time-step\": 357}, {\"accuracy\": 0.75, \"loss\": 0.2608243227005005, \"time-step\": 358}, {\"accuracy\": 0.75, \"loss\": 0.26069289445877075, \"time-step\": 359}, {\"accuracy\": 0.75, \"loss\": 0.2605627179145813, \"time-step\": 360}, {\"accuracy\": 0.75, \"loss\": 0.2604338526725769, \"time-step\": 361}, {\"accuracy\": 0.75, \"loss\": 0.2603062689304352, \"time-step\": 362}, {\"accuracy\": 0.75, \"loss\": 0.26017993688583374, \"time-step\": 363}, {\"accuracy\": 0.75, \"loss\": 0.26005494594573975, \"time-step\": 364}, {\"accuracy\": 0.75, \"loss\": 0.25993114709854126, \"time-step\": 365}, {\"accuracy\": 0.75, \"loss\": 0.25980859994888306, \"time-step\": 366}, {\"accuracy\": 0.75, \"loss\": 0.25968724489212036, \"time-step\": 367}, {\"accuracy\": 0.75, \"loss\": 0.2595672011375427, \"time-step\": 368}, {\"accuracy\": 0.75, \"loss\": 0.2594483196735382, \"time-step\": 369}, {\"accuracy\": 0.75, \"loss\": 0.2593306303024292, \"time-step\": 370}, {\"accuracy\": 0.75, \"loss\": 0.2592141926288605, \"time-step\": 371}, {\"accuracy\": 0.75, \"loss\": 0.2590988874435425, \"time-step\": 372}, {\"accuracy\": 0.75, \"loss\": 0.2589848041534424, \"time-step\": 373}, {\"accuracy\": 0.75, \"loss\": 0.258871853351593, \"time-step\": 374}, {\"accuracy\": 0.75, \"loss\": 0.25876009464263916, \"time-step\": 375}, {\"accuracy\": 0.75, \"loss\": 0.2586494982242584, \"time-step\": 376}, {\"accuracy\": 0.75, \"loss\": 0.2585400342941284, \"time-step\": 377}, {\"accuracy\": 0.75, \"loss\": 0.25843173265457153, \"time-step\": 378}, {\"accuracy\": 0.75, \"loss\": 0.2583245038986206, \"time-step\": 379}, {\"accuracy\": 0.75, \"loss\": 0.25821834802627563, \"time-step\": 380}, {\"accuracy\": 0.75, \"loss\": 0.25811341404914856, \"time-step\": 381}, {\"accuracy\": 0.75, \"loss\": 0.25800955295562744, \"time-step\": 382}, {\"accuracy\": 0.75, \"loss\": 0.2579067051410675, \"time-step\": 383}, {\"accuracy\": 0.75, \"loss\": 0.2578050196170807, \"time-step\": 384}, {\"accuracy\": 0.75, \"loss\": 0.25770437717437744, \"time-step\": 385}, {\"accuracy\": 0.75, \"loss\": 0.25760480761528015, \"time-step\": 386}, {\"accuracy\": 0.75, \"loss\": 0.25750628113746643, \"time-step\": 387}, {\"accuracy\": 0.75, \"loss\": 0.25740885734558105, \"time-step\": 388}, {\"accuracy\": 0.75, \"loss\": 0.2573124170303345, \"time-step\": 389}, {\"accuracy\": 0.75, \"loss\": 0.25721704959869385, \"time-step\": 390}, {\"accuracy\": 0.75, \"loss\": 0.257122665643692, \"time-step\": 391}, {\"accuracy\": 0.75, \"loss\": 0.25702929496765137, \"time-step\": 392}, {\"accuracy\": 0.75, \"loss\": 0.2569369673728943, \"time-step\": 393}, {\"accuracy\": 0.75, \"loss\": 0.256845623254776, \"time-step\": 394}, {\"accuracy\": 0.75, \"loss\": 0.2567552328109741, \"time-step\": 395}, {\"accuracy\": 0.75, \"loss\": 0.2566658854484558, \"time-step\": 396}, {\"accuracy\": 0.75, \"loss\": 0.2565775215625763, \"time-step\": 397}, {\"accuracy\": 0.75, \"loss\": 0.2564900517463684, \"time-step\": 398}, {\"accuracy\": 0.75, \"loss\": 0.2564035654067993, \"time-step\": 399}, {\"accuracy\": 0.75, \"loss\": 0.256318062543869, \"time-step\": 400}, {\"accuracy\": 0.75, \"loss\": 0.25623348355293274, \"time-step\": 401}, {\"accuracy\": 0.75, \"loss\": 0.25614985823631287, \"time-step\": 402}, {\"accuracy\": 0.75, \"loss\": 0.2560671269893646, \"time-step\": 403}, {\"accuracy\": 0.75, \"loss\": 0.2559853196144104, \"time-step\": 404}, {\"accuracy\": 0.75, \"loss\": 0.2559044361114502, \"time-step\": 405}, {\"accuracy\": 0.75, \"loss\": 0.255824476480484, \"time-step\": 406}, {\"accuracy\": 0.75, \"loss\": 0.2557453513145447, \"time-step\": 407}, {\"accuracy\": 0.75, \"loss\": 0.25566715002059937, \"time-step\": 408}, {\"accuracy\": 0.75, \"loss\": 0.2555897831916809, \"time-step\": 409}, {\"accuracy\": 0.5, \"loss\": 0.25551334023475647, \"time-step\": 410}, {\"accuracy\": 0.5, \"loss\": 0.2554377615451813, \"time-step\": 411}, {\"accuracy\": 0.5, \"loss\": 0.2553630471229553, \"time-step\": 412}, {\"accuracy\": 0.5, \"loss\": 0.2552891671657562, \"time-step\": 413}, {\"accuracy\": 0.5, \"loss\": 0.255216121673584, \"time-step\": 414}, {\"accuracy\": 0.5, \"loss\": 0.2551438808441162, \"time-step\": 415}, {\"accuracy\": 0.5, \"loss\": 0.2550725042819977, \"time-step\": 416}, {\"accuracy\": 0.5, \"loss\": 0.2550019919872284, \"time-step\": 417}, {\"accuracy\": 0.5, \"loss\": 0.2549322247505188, \"time-step\": 418}, {\"accuracy\": 0.5, \"loss\": 0.25486329197883606, \"time-step\": 419}, {\"accuracy\": 0.5, \"loss\": 0.254795104265213, \"time-step\": 420}, {\"accuracy\": 0.5, \"loss\": 0.2547277808189392, \"time-step\": 421}, {\"accuracy\": 0.5, \"loss\": 0.2546612024307251, \"time-step\": 422}, {\"accuracy\": 0.5, \"loss\": 0.2545953691005707, \"time-step\": 423}, {\"accuracy\": 0.5, \"loss\": 0.2545303702354431, \"time-step\": 424}, {\"accuracy\": 0.5, \"loss\": 0.25446611642837524, \"time-step\": 425}, {\"accuracy\": 0.5, \"loss\": 0.25440260767936707, \"time-step\": 426}, {\"accuracy\": 0.5, \"loss\": 0.2543398141860962, \"time-step\": 427}, {\"accuracy\": 0.5, \"loss\": 0.2542777955532074, \"time-step\": 428}, {\"accuracy\": 0.5, \"loss\": 0.2542165219783783, \"time-step\": 429}, {\"accuracy\": 0.5, \"loss\": 0.2541559636592865, \"time-step\": 430}, {\"accuracy\": 0.5, \"loss\": 0.2540960907936096, \"time-step\": 431}, {\"accuracy\": 0.5, \"loss\": 0.25403696298599243, \"time-step\": 432}, {\"accuracy\": 0.5, \"loss\": 0.25397855043411255, \"time-step\": 433}, {\"accuracy\": 0.5, \"loss\": 0.25392085313796997, \"time-step\": 434}, {\"accuracy\": 0.5, \"loss\": 0.2538638412952423, \"time-step\": 435}, {\"accuracy\": 0.5, \"loss\": 0.2538074851036072, \"time-step\": 436}, {\"accuracy\": 0.5, \"loss\": 0.25375181436538696, \"time-step\": 437}, {\"accuracy\": 0.5, \"loss\": 0.25369682908058167, \"time-step\": 438}, {\"accuracy\": 0.5, \"loss\": 0.2536425292491913, \"time-step\": 439}, {\"accuracy\": 0.5, \"loss\": 0.25358888506889343, \"time-step\": 440}, {\"accuracy\": 0.5, \"loss\": 0.2535358667373657, \"time-step\": 441}, {\"accuracy\": 0.5, \"loss\": 0.25348353385925293, \"time-step\": 442}, {\"accuracy\": 0.5, \"loss\": 0.2534317970275879, \"time-step\": 443}, {\"accuracy\": 0.5, \"loss\": 0.25338074564933777, \"time-step\": 444}, {\"accuracy\": 0.5, \"loss\": 0.2533302903175354, \"time-step\": 445}, {\"accuracy\": 0.5, \"loss\": 0.2532804608345032, \"time-step\": 446}, {\"accuracy\": 0.5, \"loss\": 0.2532312273979187, \"time-step\": 447}, {\"accuracy\": 0.5, \"loss\": 0.25318264961242676, \"time-step\": 448}, {\"accuracy\": 0.5, \"loss\": 0.2531346380710602, \"time-step\": 449}, {\"accuracy\": 0.5, \"loss\": 0.25308728218078613, \"time-step\": 450}, {\"accuracy\": 0.5, \"loss\": 0.25304049253463745, \"time-step\": 451}, {\"accuracy\": 0.5, \"loss\": 0.25299423933029175, \"time-step\": 452}, {\"accuracy\": 0.5, \"loss\": 0.2529486119747162, \"time-step\": 453}, {\"accuracy\": 0.5, \"loss\": 0.2529035806655884, \"time-step\": 454}, {\"accuracy\": 0.5, \"loss\": 0.2528590261936188, \"time-step\": 455}, {\"accuracy\": 0.5, \"loss\": 0.2528151273727417, \"time-step\": 456}, {\"accuracy\": 0.5, \"loss\": 0.25277179479599, \"time-step\": 457}, {\"accuracy\": 0.5, \"loss\": 0.2527289390563965, \"time-step\": 458}, {\"accuracy\": 0.5, \"loss\": 0.25268664956092834, \"time-step\": 459}, {\"accuracy\": 0.5, \"loss\": 0.25264495611190796, \"time-step\": 460}, {\"accuracy\": 0.5, \"loss\": 0.2526037096977234, \"time-step\": 461}, {\"accuracy\": 0.5, \"loss\": 0.2525630593299866, \"time-step\": 462}, {\"accuracy\": 0.5, \"loss\": 0.25252288579940796, \"time-step\": 463}, {\"accuracy\": 0.5, \"loss\": 0.2524832785129547, \"time-step\": 464}, {\"accuracy\": 0.5, \"loss\": 0.25244420766830444, \"time-step\": 465}, {\"accuracy\": 0.5, \"loss\": 0.2524055242538452, \"time-step\": 466}, {\"accuracy\": 0.5, \"loss\": 0.25236746668815613, \"time-step\": 467}, {\"accuracy\": 0.5, \"loss\": 0.25232985615730286, \"time-step\": 468}, {\"accuracy\": 0.5, \"loss\": 0.2522927224636078, \"time-step\": 469}, {\"accuracy\": 0.5, \"loss\": 0.2522560656070709, \"time-step\": 470}, {\"accuracy\": 0.5, \"loss\": 0.25221991539001465, \"time-step\": 471}, {\"accuracy\": 0.5, \"loss\": 0.25218427181243896, \"time-step\": 472}, {\"accuracy\": 0.5, \"loss\": 0.2521490156650543, \"time-step\": 473}, {\"accuracy\": 0.5, \"loss\": 0.25211429595947266, \"time-step\": 474}, {\"accuracy\": 0.5, \"loss\": 0.2520800232887268, \"time-step\": 475}, {\"accuracy\": 0.5, \"loss\": 0.252046138048172, \"time-step\": 476}, {\"accuracy\": 0.5, \"loss\": 0.2520127296447754, \"time-step\": 477}, {\"accuracy\": 0.5, \"loss\": 0.2519798278808594, \"time-step\": 478}, {\"accuracy\": 0.5, \"loss\": 0.251947283744812, \"time-step\": 479}, {\"accuracy\": 0.5, \"loss\": 0.25191521644592285, \"time-step\": 480}, {\"accuracy\": 0.5, \"loss\": 0.2518835961818695, \"time-step\": 481}, {\"accuracy\": 0.5, \"loss\": 0.2518523931503296, \"time-step\": 482}, {\"accuracy\": 0.5, \"loss\": 0.2518216371536255, \"time-step\": 483}, {\"accuracy\": 0.5, \"loss\": 0.25179123878479004, \"time-step\": 484}, {\"accuracy\": 0.5, \"loss\": 0.25176119804382324, \"time-step\": 485}, {\"accuracy\": 0.5, \"loss\": 0.2517316937446594, \"time-step\": 486}, {\"accuracy\": 0.5, \"loss\": 0.2517024874687195, \"time-step\": 487}, {\"accuracy\": 0.5, \"loss\": 0.25167369842529297, \"time-step\": 488}, {\"accuracy\": 0.5, \"loss\": 0.2516453266143799, \"time-step\": 489}, {\"accuracy\": 0.5, \"loss\": 0.25161731243133545, \"time-step\": 490}, {\"accuracy\": 0.5, \"loss\": 0.25158971548080444, \"time-step\": 491}, {\"accuracy\": 0.5, \"loss\": 0.2515624761581421, \"time-step\": 492}, {\"accuracy\": 0.5, \"loss\": 0.251535564661026, \"time-step\": 493}, {\"accuracy\": 0.5, \"loss\": 0.25150907039642334, \"time-step\": 494}, {\"accuracy\": 0.5, \"loss\": 0.25148293375968933, \"time-step\": 495}, {\"accuracy\": 0.5, \"loss\": 0.251457154750824, \"time-step\": 496}, {\"accuracy\": 0.5, \"loss\": 0.2514317035675049, \"time-step\": 497}, {\"accuracy\": 0.5, \"loss\": 0.25140663981437683, \"time-step\": 498}, {\"accuracy\": 0.5, \"loss\": 0.25138187408447266, \"time-step\": 499}, {\"accuracy\": 0.5, \"loss\": 0.25135746598243713, \"time-step\": 500}, {\"accuracy\": 0.5, \"loss\": 0.25133341550827026, \"time-step\": 501}, {\"accuracy\": 0.5, \"loss\": 0.25130966305732727, \"time-step\": 502}, {\"accuracy\": 0.5, \"loss\": 0.25128626823425293, \"time-step\": 503}, {\"accuracy\": 0.5, \"loss\": 0.2512631416320801, \"time-step\": 504}, {\"accuracy\": 0.5, \"loss\": 0.2512403726577759, \"time-step\": 505}, {\"accuracy\": 0.5, \"loss\": 0.25121793150901794, \"time-step\": 506}, {\"accuracy\": 0.5, \"loss\": 0.2511957287788391, \"time-step\": 507}, {\"accuracy\": 0.5, \"loss\": 0.2511739134788513, \"time-step\": 508}, {\"accuracy\": 0.5, \"loss\": 0.251152366399765, \"time-step\": 509}, {\"accuracy\": 0.5, \"loss\": 0.2511311173439026, \"time-step\": 510}, {\"accuracy\": 0.5, \"loss\": 0.25111010670661926, \"time-step\": 511}, {\"accuracy\": 0.5, \"loss\": 0.2510894238948822, \"time-step\": 512}, {\"accuracy\": 0.5, \"loss\": 0.251069039106369, \"time-step\": 513}, {\"accuracy\": 0.5, \"loss\": 0.25104889273643494, \"time-step\": 514}, {\"accuracy\": 0.5, \"loss\": 0.25102901458740234, \"time-step\": 515}, {\"accuracy\": 0.5, \"loss\": 0.25100943446159363, \"time-step\": 516}, {\"accuracy\": 0.5, \"loss\": 0.250990092754364, \"time-step\": 517}, {\"accuracy\": 0.5, \"loss\": 0.2509710192680359, \"time-step\": 518}, {\"accuracy\": 0.5, \"loss\": 0.25095218420028687, \"time-step\": 519}, {\"accuracy\": 0.5, \"loss\": 0.2509336471557617, \"time-step\": 520}, {\"accuracy\": 0.5, \"loss\": 0.2509153187274933, \"time-step\": 521}, {\"accuracy\": 0.5, \"loss\": 0.2508971691131592, \"time-step\": 522}, {\"accuracy\": 0.5, \"loss\": 0.25087934732437134, \"time-step\": 523}, {\"accuracy\": 0.5, \"loss\": 0.2508617341518402, \"time-step\": 524}, {\"accuracy\": 0.5, \"loss\": 0.2508443593978882, \"time-step\": 525}, {\"accuracy\": 0.5, \"loss\": 0.2508271634578705, \"time-step\": 526}, {\"accuracy\": 0.5, \"loss\": 0.2508101463317871, \"time-step\": 527}, {\"accuracy\": 0.5, \"loss\": 0.2507933974266052, \"time-step\": 528}, {\"accuracy\": 0.5, \"loss\": 0.25077682733535767, \"time-step\": 529}, {\"accuracy\": 0.5, \"loss\": 0.2507604956626892, \"time-step\": 530}, {\"accuracy\": 0.5, \"loss\": 0.2507442831993103, \"time-step\": 531}, {\"accuracy\": 0.5, \"loss\": 0.2507283091545105, \"time-step\": 532}, {\"accuracy\": 0.5, \"loss\": 0.250712513923645, \"time-step\": 533}, {\"accuracy\": 0.5, \"loss\": 0.25069689750671387, \"time-step\": 534}, {\"accuracy\": 0.5, \"loss\": 0.25068148970603943, \"time-step\": 535}, {\"accuracy\": 0.5, \"loss\": 0.25066617131233215, \"time-step\": 536}, {\"accuracy\": 0.5, \"loss\": 0.250651091337204, \"time-step\": 537}, {\"accuracy\": 0.5, \"loss\": 0.25063613057136536, \"time-step\": 538}, {\"accuracy\": 0.5, \"loss\": 0.25062131881713867, \"time-step\": 539}, {\"accuracy\": 0.5, \"loss\": 0.2506066858768463, \"time-step\": 540}, {\"accuracy\": 0.5, \"loss\": 0.2505921721458435, \"time-step\": 541}, {\"accuracy\": 0.5, \"loss\": 0.25057777762413025, \"time-step\": 542}, {\"accuracy\": 0.5, \"loss\": 0.25056350231170654, \"time-step\": 543}, {\"accuracy\": 0.5, \"loss\": 0.2505493760108948, \"time-step\": 544}, {\"accuracy\": 0.5, \"loss\": 0.25053533911705017, \"time-step\": 545}, {\"accuracy\": 0.5, \"loss\": 0.2505214512348175, \"time-step\": 546}, {\"accuracy\": 0.5, \"loss\": 0.2505075931549072, \"time-step\": 547}, {\"accuracy\": 0.5, \"loss\": 0.2504939138889313, \"time-step\": 548}, {\"accuracy\": 0.5, \"loss\": 0.2504802346229553, \"time-step\": 549}, {\"accuracy\": 0.5, \"loss\": 0.2504667043685913, \"time-step\": 550}, {\"accuracy\": 0.5, \"loss\": 0.2504532039165497, \"time-step\": 551}, {\"accuracy\": 0.5, \"loss\": 0.25043976306915283, \"time-step\": 552}, {\"accuracy\": 0.5, \"loss\": 0.25042641162872314, \"time-step\": 553}, {\"accuracy\": 0.5, \"loss\": 0.25041308999061584, \"time-step\": 554}, {\"accuracy\": 0.5, \"loss\": 0.25039979815483093, \"time-step\": 555}, {\"accuracy\": 0.5, \"loss\": 0.250386506319046, \"time-step\": 556}, {\"accuracy\": 0.5, \"loss\": 0.25037330389022827, \"time-step\": 557}, {\"accuracy\": 0.5, \"loss\": 0.25036004185676575, \"time-step\": 558}, {\"accuracy\": 0.5, \"loss\": 0.250346839427948, \"time-step\": 559}, {\"accuracy\": 0.5, \"loss\": 0.2503335177898407, \"time-step\": 560}, {\"accuracy\": 0.5, \"loss\": 0.2503202557563782, \"time-step\": 561}, {\"accuracy\": 0.5, \"loss\": 0.2503069341182709, \"time-step\": 562}, {\"accuracy\": 0.5, \"loss\": 0.2502935528755188, \"time-step\": 563}, {\"accuracy\": 0.5, \"loss\": 0.25028008222579956, \"time-step\": 564}, {\"accuracy\": 0.5, \"loss\": 0.25026655197143555, \"time-step\": 565}, {\"accuracy\": 0.5, \"loss\": 0.25025293231010437, \"time-step\": 566}, {\"accuracy\": 0.5, \"loss\": 0.25023919343948364, \"time-step\": 567}, {\"accuracy\": 0.5, \"loss\": 0.25022536516189575, \"time-step\": 568}, {\"accuracy\": 0.5, \"loss\": 0.25021132826805115, \"time-step\": 569}, {\"accuracy\": 0.5, \"loss\": 0.250197172164917, \"time-step\": 570}, {\"accuracy\": 0.5, \"loss\": 0.25018274784088135, \"time-step\": 571}, {\"accuracy\": 0.5, \"loss\": 0.25016817450523376, \"time-step\": 572}, {\"accuracy\": 0.5, \"loss\": 0.2501533329486847, \"time-step\": 573}, {\"accuracy\": 0.5, \"loss\": 0.2501382827758789, \"time-step\": 574}, {\"accuracy\": 0.5, \"loss\": 0.250122994184494, \"time-step\": 575}, {\"accuracy\": 0.5, \"loss\": 0.2501072883605957, \"time-step\": 576}, {\"accuracy\": 0.5, \"loss\": 0.2500913143157959, \"time-step\": 577}, {\"accuracy\": 0.5, \"loss\": 0.25007495284080505, \"time-step\": 578}, {\"accuracy\": 0.5, \"loss\": 0.2500581443309784, \"time-step\": 579}, {\"accuracy\": 0.5, \"loss\": 0.2500408887863159, \"time-step\": 580}, {\"accuracy\": 0.5, \"loss\": 0.2500232458114624, \"time-step\": 581}, {\"accuracy\": 0.5, \"loss\": 0.2500050365924835, \"time-step\": 582}, {\"accuracy\": 0.5, \"loss\": 0.24998624622821808, \"time-step\": 583}, {\"accuracy\": 0.5, \"loss\": 0.24996687471866608, \"time-step\": 584}, {\"accuracy\": 0.5, \"loss\": 0.24994686245918274, \"time-step\": 585}, {\"accuracy\": 0.5, \"loss\": 0.24992609024047852, \"time-step\": 586}, {\"accuracy\": 0.5, \"loss\": 0.2499045729637146, \"time-step\": 587}, {\"accuracy\": 0.5, \"loss\": 0.24988223612308502, \"time-step\": 588}, {\"accuracy\": 0.5, \"loss\": 0.24985896050930023, \"time-step\": 589}, {\"accuracy\": 0.5, \"loss\": 0.24983471632003784, \"time-step\": 590}, {\"accuracy\": 0.5, \"loss\": 0.24980944395065308, \"time-step\": 591}, {\"accuracy\": 0.5, \"loss\": 0.24978305399417877, \"time-step\": 592}, {\"accuracy\": 0.5, \"loss\": 0.2497553825378418, \"time-step\": 593}, {\"accuracy\": 0.5, \"loss\": 0.24972638487815857, \"time-step\": 594}, {\"accuracy\": 0.5, \"loss\": 0.2496960163116455, \"time-step\": 595}, {\"accuracy\": 0.5, \"loss\": 0.2496640980243683, \"time-step\": 596}, {\"accuracy\": 0.5, \"loss\": 0.24963051080703735, \"time-step\": 597}, {\"accuracy\": 0.5, \"loss\": 0.24959513545036316, \"time-step\": 598}, {\"accuracy\": 0.5, \"loss\": 0.249557763338089, \"time-step\": 599}, {\"accuracy\": 0.5, \"loss\": 0.24951834976673126, \"time-step\": 600}, {\"accuracy\": 0.5, \"loss\": 0.24947670102119446, \"time-step\": 601}, {\"accuracy\": 0.5, \"loss\": 0.24943259358406067, \"time-step\": 602}, {\"accuracy\": 0.5, \"loss\": 0.249385803937912, \"time-step\": 603}, {\"accuracy\": 0.5, \"loss\": 0.24933621287345886, \"time-step\": 604}, {\"accuracy\": 0.5, \"loss\": 0.24928352236747742, \"time-step\": 605}, {\"accuracy\": 0.5, \"loss\": 0.24922755360603333, \"time-step\": 606}, {\"accuracy\": 0.5, \"loss\": 0.24916797876358032, \"time-step\": 607}, {\"accuracy\": 0.5, \"loss\": 0.2491045594215393, \"time-step\": 608}, {\"accuracy\": 0.5, \"loss\": 0.24903690814971924, \"time-step\": 609}, {\"accuracy\": 0.5, \"loss\": 0.24896478652954102, \"time-step\": 610}, {\"accuracy\": 0.5, \"loss\": 0.2488877922296524, \"time-step\": 611}, {\"accuracy\": 0.5, \"loss\": 0.24880552291870117, \"time-step\": 612}, {\"accuracy\": 0.5, \"loss\": 0.24871759116649628, \"time-step\": 613}, {\"accuracy\": 0.5, \"loss\": 0.2486235499382019, \"time-step\": 614}, {\"accuracy\": 0.5, \"loss\": 0.24852284789085388, \"time-step\": 615}, {\"accuracy\": 0.5, \"loss\": 0.24841508269309998, \"time-step\": 616}, {\"accuracy\": 0.5, \"loss\": 0.24829959869384766, \"time-step\": 617}, {\"accuracy\": 0.5, \"loss\": 0.24817588925361633, \"time-step\": 618}, {\"accuracy\": 0.5, \"loss\": 0.24804332852363586, \"time-step\": 619}, {\"accuracy\": 0.5, \"loss\": 0.24790118634700775, \"time-step\": 620}, {\"accuracy\": 0.75, \"loss\": 0.24774880707263947, \"time-step\": 621}, {\"accuracy\": 0.75, \"loss\": 0.24758541584014893, \"time-step\": 622}, {\"accuracy\": 0.75, \"loss\": 0.24741022288799286, \"time-step\": 623}, {\"accuracy\": 0.75, \"loss\": 0.2472223937511444, \"time-step\": 624}, {\"accuracy\": 0.75, \"loss\": 0.2470211237668991, \"time-step\": 625}, {\"accuracy\": 0.75, \"loss\": 0.24680548906326294, \"time-step\": 626}, {\"accuracy\": 0.75, \"loss\": 0.2465745210647583, \"time-step\": 627}, {\"accuracy\": 0.75, \"loss\": 0.2463272511959076, \"time-step\": 628}, {\"accuracy\": 0.75, \"loss\": 0.2460627555847168, \"time-step\": 629}, {\"accuracy\": 0.75, \"loss\": 0.2457798570394516, \"time-step\": 630}, {\"accuracy\": 0.75, \"loss\": 0.24547773599624634, \"time-step\": 631}, {\"accuracy\": 0.75, \"loss\": 0.2451551854610443, \"time-step\": 632}, {\"accuracy\": 0.75, \"loss\": 0.24481123685836792, \"time-step\": 633}, {\"accuracy\": 0.75, \"loss\": 0.2444448620080948, \"time-step\": 634}, {\"accuracy\": 0.75, \"loss\": 0.24405497312545776, \"time-step\": 635}, {\"accuracy\": 0.75, \"loss\": 0.24364058673381805, \"time-step\": 636}, {\"accuracy\": 0.75, \"loss\": 0.24320083856582642, \"time-step\": 637}, {\"accuracy\": 0.75, \"loss\": 0.24273470044136047, \"time-step\": 638}, {\"accuracy\": 0.75, \"loss\": 0.24224130809307098, \"time-step\": 639}, {\"accuracy\": 0.75, \"loss\": 0.24171988666057587, \"time-step\": 640}, {\"accuracy\": 0.75, \"loss\": 0.24116963148117065, \"time-step\": 641}, {\"accuracy\": 0.75, \"loss\": 0.24058999121189117, \"time-step\": 642}, {\"accuracy\": 0.75, \"loss\": 0.23998035490512848, \"time-step\": 643}, {\"accuracy\": 0.75, \"loss\": 0.239340141415596, \"time-step\": 644}, {\"accuracy\": 0.75, \"loss\": 0.23866905272006989, \"time-step\": 645}, {\"accuracy\": 0.75, \"loss\": 0.23796674609184265, \"time-step\": 646}, {\"accuracy\": 0.75, \"loss\": 0.23723292350769043, \"time-step\": 647}, {\"accuracy\": 0.75, \"loss\": 0.23646768927574158, \"time-step\": 648}, {\"accuracy\": 0.75, \"loss\": 0.23567083477973938, \"time-step\": 649}, {\"accuracy\": 0.75, \"loss\": 0.23484258353710175, \"time-step\": 650}, {\"accuracy\": 0.75, \"loss\": 0.23398303985595703, \"time-step\": 651}, {\"accuracy\": 0.75, \"loss\": 0.23309262096881866, \"time-step\": 652}, {\"accuracy\": 0.75, \"loss\": 0.23217162489891052, \"time-step\": 653}, {\"accuracy\": 0.75, \"loss\": 0.23122063279151917, \"time-step\": 654}, {\"accuracy\": 0.75, \"loss\": 0.23024022579193115, \"time-step\": 655}, {\"accuracy\": 0.75, \"loss\": 0.22923125326633453, \"time-step\": 656}, {\"accuracy\": 0.75, \"loss\": 0.22819450497627258, \"time-step\": 657}, {\"accuracy\": 0.75, \"loss\": 0.22713106870651245, \"time-step\": 658}, {\"accuracy\": 0.75, \"loss\": 0.22604212164878845, \"time-step\": 659}, {\"accuracy\": 0.75, \"loss\": 0.22492899000644684, \"time-step\": 660}, {\"accuracy\": 0.75, \"loss\": 0.223793163895607, \"time-step\": 661}, {\"accuracy\": 0.75, \"loss\": 0.22263631224632263, \"time-step\": 662}, {\"accuracy\": 0.75, \"loss\": 0.22146031260490417, \"time-step\": 663}, {\"accuracy\": 0.75, \"loss\": 0.2202671468257904, \"time-step\": 664}, {\"accuracy\": 0.75, \"loss\": 0.219059020280838, \"time-step\": 665}, {\"accuracy\": 0.75, \"loss\": 0.21783822774887085, \"time-step\": 666}, {\"accuracy\": 0.75, \"loss\": 0.2166072279214859, \"time-step\": 667}, {\"accuracy\": 0.75, \"loss\": 0.21536871790885925, \"time-step\": 668}, {\"accuracy\": 0.75, \"loss\": 0.21412524580955505, \"time-step\": 669}, {\"accuracy\": 0.75, \"loss\": 0.21287965774536133, \"time-step\": 670}, {\"accuracy\": 0.75, \"loss\": 0.2116345912218094, \"time-step\": 671}, {\"accuracy\": 0.75, \"loss\": 0.21039295196533203, \"time-step\": 672}, {\"accuracy\": 0.75, \"loss\": 0.20915740728378296, \"time-step\": 673}, {\"accuracy\": 0.75, \"loss\": 0.20793062448501587, \"time-step\": 674}, {\"accuracy\": 0.75, \"loss\": 0.20671510696411133, \"time-step\": 675}, {\"accuracy\": 0.75, \"loss\": 0.20551331341266632, \"time-step\": 676}, {\"accuracy\": 0.75, \"loss\": 0.20432738959789276, \"time-step\": 677}, {\"accuracy\": 0.75, \"loss\": 0.20315931737422943, \"time-step\": 678}, {\"accuracy\": 0.75, \"loss\": 0.20201097428798676, \"time-step\": 679}, {\"accuracy\": 0.75, \"loss\": 0.20088371634483337, \"time-step\": 680}, {\"accuracy\": 0.75, \"loss\": 0.19977888464927673, \"time-step\": 681}, {\"accuracy\": 0.75, \"loss\": 0.1986973136663437, \"time-step\": 682}, {\"accuracy\": 0.75, \"loss\": 0.19763977825641632, \"time-step\": 683}, {\"accuracy\": 0.75, \"loss\": 0.19660648703575134, \"time-step\": 684}, {\"accuracy\": 0.75, \"loss\": 0.19559767842292786, \"time-step\": 685}, {\"accuracy\": 0.75, \"loss\": 0.19461305439472198, \"time-step\": 686}, {\"accuracy\": 0.75, \"loss\": 0.1936521679162979, \"time-step\": 687}, {\"accuracy\": 0.75, \"loss\": 0.19271434843540192, \"time-step\": 688}, {\"accuracy\": 0.75, \"loss\": 0.19179868698120117, \"time-step\": 689}, {\"accuracy\": 0.75, \"loss\": 0.19090408086776733, \"time-step\": 690}, {\"accuracy\": 0.75, \"loss\": 0.19002927839756012, \"time-step\": 691}, {\"accuracy\": 0.75, \"loss\": 0.1891728788614273, \"time-step\": 692}, {\"accuracy\": 0.75, \"loss\": 0.1883334219455719, \"time-step\": 693}, {\"accuracy\": 0.75, \"loss\": 0.18750914931297302, \"time-step\": 694}, {\"accuracy\": 0.75, \"loss\": 0.18669846653938293, \"time-step\": 695}, {\"accuracy\": 0.75, \"loss\": 0.1858995407819748, \"time-step\": 696}, {\"accuracy\": 0.75, \"loss\": 0.18511050939559937, \"time-step\": 697}, {\"accuracy\": 0.75, \"loss\": 0.18432952463626862, \"time-step\": 698}, {\"accuracy\": 0.75, \"loss\": 0.18355467915534973, \"time-step\": 699}, {\"accuracy\": 0.75, \"loss\": 0.18278394639492035, \"time-step\": 700}, {\"accuracy\": 0.75, \"loss\": 0.1820155382156372, \"time-step\": 701}, {\"accuracy\": 0.75, \"loss\": 0.18124714493751526, \"time-step\": 702}, {\"accuracy\": 0.75, \"loss\": 0.18047694861888885, \"time-step\": 703}, {\"accuracy\": 0.75, \"loss\": 0.17970286309719086, \"time-step\": 704}, {\"accuracy\": 0.75, \"loss\": 0.1789228469133377, \"time-step\": 705}, {\"accuracy\": 0.75, \"loss\": 0.17813490331172943, \"time-step\": 706}, {\"accuracy\": 0.75, \"loss\": 0.17733703553676605, \"time-step\": 707}, {\"accuracy\": 0.75, \"loss\": 0.1765269935131073, \"time-step\": 708}, {\"accuracy\": 0.75, \"loss\": 0.17570294439792633, \"time-step\": 709}, {\"accuracy\": 0.75, \"loss\": 0.17486289143562317, \"time-step\": 710}, {\"accuracy\": 0.75, \"loss\": 0.17400464415550232, \"time-step\": 711}, {\"accuracy\": 0.75, \"loss\": 0.17312636971473694, \"time-step\": 712}, {\"accuracy\": 0.75, \"loss\": 0.17222610116004944, \"time-step\": 713}, {\"accuracy\": 0.75, \"loss\": 0.1713019460439682, \"time-step\": 714}, {\"accuracy\": 0.75, \"loss\": 0.17035186290740967, \"time-step\": 715}, {\"accuracy\": 0.75, \"loss\": 0.16937418282032013, \"time-step\": 716}, {\"accuracy\": 0.75, \"loss\": 0.16836705803871155, \"time-step\": 717}, {\"accuracy\": 0.75, \"loss\": 0.16732868552207947, \"time-step\": 718}, {\"accuracy\": 0.75, \"loss\": 0.16625744104385376, \"time-step\": 719}, {\"accuracy\": 0.75, \"loss\": 0.1651516556739807, \"time-step\": 720}, {\"accuracy\": 0.75, \"loss\": 0.16400983929634094, \"time-step\": 721}, {\"accuracy\": 0.75, \"loss\": 0.1628303825855255, \"time-step\": 722}, {\"accuracy\": 0.75, \"loss\": 0.16161206364631653, \"time-step\": 723}, {\"accuracy\": 0.75, \"loss\": 0.16035345196723938, \"time-step\": 724}, {\"accuracy\": 0.75, \"loss\": 0.15905334055423737, \"time-step\": 725}, {\"accuracy\": 0.75, \"loss\": 0.15771064162254333, \"time-step\": 726}, {\"accuracy\": 0.75, \"loss\": 0.15632440149784088, \"time-step\": 727}, {\"accuracy\": 0.75, \"loss\": 0.154893696308136, \"time-step\": 728}, {\"accuracy\": 0.75, \"loss\": 0.15341797471046448, \"time-step\": 729}, {\"accuracy\": 0.75, \"loss\": 0.15189675986766815, \"time-step\": 730}, {\"accuracy\": 0.75, \"loss\": 0.15032964944839478, \"time-step\": 731}, {\"accuracy\": 0.75, \"loss\": 0.14871665835380554, \"time-step\": 732}, {\"accuracy\": 0.75, \"loss\": 0.14705796539783478, \"time-step\": 733}, {\"accuracy\": 0.75, \"loss\": 0.1453537940979004, \"time-step\": 734}, {\"accuracy\": 0.75, \"loss\": 0.14360487461090088, \"time-step\": 735}, {\"accuracy\": 0.75, \"loss\": 0.14181196689605713, \"time-step\": 736}, {\"accuracy\": 0.75, \"loss\": 0.1399761289358139, \"time-step\": 737}, {\"accuracy\": 0.75, \"loss\": 0.13809853792190552, \"time-step\": 738}, {\"accuracy\": 0.75, \"loss\": 0.13618063926696777, \"time-step\": 739}, {\"accuracy\": 1.0, \"loss\": 0.13422417640686035, \"time-step\": 740}, {\"accuracy\": 1.0, \"loss\": 0.13223083317279816, \"time-step\": 741}, {\"accuracy\": 1.0, \"loss\": 0.1302027404308319, \"time-step\": 742}, {\"accuracy\": 1.0, \"loss\": 0.12814201414585114, \"time-step\": 743}, {\"accuracy\": 1.0, \"loss\": 0.1260509192943573, \"time-step\": 744}, {\"accuracy\": 1.0, \"loss\": 0.12393206357955933, \"time-step\": 745}, {\"accuracy\": 1.0, \"loss\": 0.12178795039653778, \"time-step\": 746}, {\"accuracy\": 1.0, \"loss\": 0.11962137371301651, \"time-step\": 747}, {\"accuracy\": 1.0, \"loss\": 0.11743517220020294, \"time-step\": 748}, {\"accuracy\": 1.0, \"loss\": 0.11523230373859406, \"time-step\": 749}, {\"accuracy\": 1.0, \"loss\": 0.11301571130752563, \"time-step\": 750}, {\"accuracy\": 1.0, \"loss\": 0.11078856885433197, \"time-step\": 751}, {\"accuracy\": 1.0, \"loss\": 0.10855396091938019, \"time-step\": 752}, {\"accuracy\": 1.0, \"loss\": 0.10631509870290756, \"time-step\": 753}, {\"accuracy\": 1.0, \"loss\": 0.1040751039981842, \"time-step\": 754}, {\"accuracy\": 1.0, \"loss\": 0.10183712840080261, \"time-step\": 755}, {\"accuracy\": 1.0, \"loss\": 0.09960432350635529, \"time-step\": 756}, {\"accuracy\": 1.0, \"loss\": 0.09737983345985413, \"time-step\": 757}, {\"accuracy\": 1.0, \"loss\": 0.09516660124063492, \"time-step\": 758}, {\"accuracy\": 1.0, \"loss\": 0.09296761453151703, \"time-step\": 759}, {\"accuracy\": 1.0, \"loss\": 0.09078569710254669, \"time-step\": 760}, {\"accuracy\": 1.0, \"loss\": 0.08862343430519104, \"time-step\": 761}, {\"accuracy\": 1.0, \"loss\": 0.08648364245891571, \"time-step\": 762}, {\"accuracy\": 1.0, \"loss\": 0.0843685120344162, \"time-step\": 763}, {\"accuracy\": 1.0, \"loss\": 0.08228044956922531, \"time-step\": 764}, {\"accuracy\": 1.0, \"loss\": 0.0802214965224266, \"time-step\": 765}, {\"accuracy\": 1.0, \"loss\": 0.07819359004497528, \"time-step\": 766}, {\"accuracy\": 1.0, \"loss\": 0.07619848847389221, \"time-step\": 767}, {\"accuracy\": 1.0, \"loss\": 0.07423776388168335, \"time-step\": 768}, {\"accuracy\": 1.0, \"loss\": 0.07231277227401733, \"time-step\": 769}, {\"accuracy\": 1.0, \"loss\": 0.07042470574378967, \"time-step\": 770}, {\"accuracy\": 1.0, \"loss\": 0.06857465207576752, \"time-step\": 771}, {\"accuracy\": 1.0, \"loss\": 0.06676338613033295, \"time-step\": 772}, {\"accuracy\": 1.0, \"loss\": 0.06499162316322327, \"time-step\": 773}, {\"accuracy\": 1.0, \"loss\": 0.06325998902320862, \"time-step\": 774}, {\"accuracy\": 1.0, \"loss\": 0.061568714678287506, \"time-step\": 775}, {\"accuracy\": 1.0, \"loss\": 0.05991813912987709, \"time-step\": 776}, {\"accuracy\": 1.0, \"loss\": 0.05830831825733185, \"time-step\": 777}, {\"accuracy\": 1.0, \"loss\": 0.056739285588264465, \"time-step\": 778}, {\"accuracy\": 1.0, \"loss\": 0.05521084740757942, \"time-step\": 779}, {\"accuracy\": 1.0, \"loss\": 0.05372282490134239, \"time-step\": 780}, {\"accuracy\": 1.0, \"loss\": 0.05227489024400711, \"time-step\": 781}, {\"accuracy\": 1.0, \"loss\": 0.05086667835712433, \"time-step\": 782}, {\"accuracy\": 1.0, \"loss\": 0.0494975745677948, \"time-step\": 783}, {\"accuracy\": 1.0, \"loss\": 0.048167187720537186, \"time-step\": 784}, {\"accuracy\": 1.0, \"loss\": 0.046874791383743286, \"time-step\": 785}, {\"accuracy\": 1.0, \"loss\": 0.04561986029148102, \"time-step\": 786}, {\"accuracy\": 1.0, \"loss\": 0.04440154880285263, \"time-step\": 787}, {\"accuracy\": 1.0, \"loss\": 0.043219223618507385, \"time-step\": 788}, {\"accuracy\": 1.0, \"loss\": 0.0420721136033535, \"time-step\": 789}, {\"accuracy\": 1.0, \"loss\": 0.04095940291881561, \"time-step\": 790}, {\"accuracy\": 1.0, \"loss\": 0.039880331605672836, \"time-step\": 791}, {\"accuracy\": 1.0, \"loss\": 0.038833994418382645, \"time-step\": 792}, {\"accuracy\": 1.0, \"loss\": 0.037819698452949524, \"time-step\": 793}, {\"accuracy\": 1.0, \"loss\": 0.03683649003505707, \"time-step\": 794}, {\"accuracy\": 1.0, \"loss\": 0.03588355705142021, \"time-step\": 795}, {\"accuracy\": 1.0, \"loss\": 0.03496013581752777, \"time-step\": 796}, {\"accuracy\": 1.0, \"loss\": 0.03406531736254692, \"time-step\": 797}, {\"accuracy\": 1.0, \"loss\": 0.0331982746720314, \"time-step\": 798}, {\"accuracy\": 1.0, \"loss\": 0.03235826641321182, \"time-step\": 799}, {\"accuracy\": 1.0, \"loss\": 0.03154441714286804, \"time-step\": 800}, {\"accuracy\": 1.0, \"loss\": 0.030755948275327682, \"time-step\": 801}, {\"accuracy\": 1.0, \"loss\": 0.029992088675498962, \"time-step\": 802}, {\"accuracy\": 1.0, \"loss\": 0.029252057895064354, \"time-step\": 803}, {\"accuracy\": 1.0, \"loss\": 0.028535133227705956, \"time-step\": 804}, {\"accuracy\": 1.0, \"loss\": 0.027840565890073776, \"time-step\": 805}, {\"accuracy\": 1.0, \"loss\": 0.02716762386262417, \"time-step\": 806}, {\"accuracy\": 1.0, \"loss\": 0.026515617966651917, \"time-step\": 807}, {\"accuracy\": 1.0, \"loss\": 0.02588381990790367, \"time-step\": 808}, {\"accuracy\": 1.0, \"loss\": 0.02527158334851265, \"time-step\": 809}, {\"accuracy\": 1.0, \"loss\": 0.024678325280547142, \"time-step\": 810}, {\"accuracy\": 1.0, \"loss\": 0.02410338819026947, \"time-step\": 811}, {\"accuracy\": 1.0, \"loss\": 0.023546088486909866, \"time-step\": 812}, {\"accuracy\": 1.0, \"loss\": 0.0230058953166008, \"time-step\": 813}, {\"accuracy\": 1.0, \"loss\": 0.022482208907604218, \"time-step\": 814}, {\"accuracy\": 1.0, \"loss\": 0.02197449654340744, \"time-step\": 815}, {\"accuracy\": 1.0, \"loss\": 0.0214821919798851, \"time-step\": 816}, {\"accuracy\": 1.0, \"loss\": 0.021004771813750267, \"time-step\": 817}, {\"accuracy\": 1.0, \"loss\": 0.020541736856102943, \"time-step\": 818}, {\"accuracy\": 1.0, \"loss\": 0.02009260281920433, \"time-step\": 819}, {\"accuracy\": 1.0, \"loss\": 0.019656870514154434, \"time-step\": 820}, {\"accuracy\": 1.0, \"loss\": 0.019234146922826767, \"time-step\": 821}, {\"accuracy\": 1.0, \"loss\": 0.018823901191353798, \"time-step\": 822}, {\"accuracy\": 1.0, \"loss\": 0.018425755202770233, \"time-step\": 823}, {\"accuracy\": 1.0, \"loss\": 0.018039310351014137, \"time-step\": 824}, {\"accuracy\": 1.0, \"loss\": 0.017664138227701187, \"time-step\": 825}, {\"accuracy\": 1.0, \"loss\": 0.017299834638834, \"time-step\": 826}, {\"accuracy\": 1.0, \"loss\": 0.01694609224796295, \"time-step\": 827}, {\"accuracy\": 1.0, \"loss\": 0.016602499410510063, \"time-step\": 828}, {\"accuracy\": 1.0, \"loss\": 0.016268735751509666, \"time-step\": 829}, {\"accuracy\": 1.0, \"loss\": 0.015944454818964005, \"time-step\": 830}, {\"accuracy\": 1.0, \"loss\": 0.015629353001713753, \"time-step\": 831}, {\"accuracy\": 1.0, \"loss\": 0.015323102474212646, \"time-step\": 832}, {\"accuracy\": 1.0, \"loss\": 0.015025423839688301, \"time-step\": 833}, {\"accuracy\": 1.0, \"loss\": 0.014736018143594265, \"time-step\": 834}, {\"accuracy\": 1.0, \"loss\": 0.01445460133254528, \"time-step\": 835}, {\"accuracy\": 1.0, \"loss\": 0.014180928468704224, \"time-step\": 836}, {\"accuracy\": 1.0, \"loss\": 0.01391473226249218, \"time-step\": 837}, {\"accuracy\": 1.0, \"loss\": 0.013655759394168854, \"time-step\": 838}, {\"accuracy\": 1.0, \"loss\": 0.013403761200606823, \"time-step\": 839}, {\"accuracy\": 1.0, \"loss\": 0.013158534653484821, \"time-step\": 840}, {\"accuracy\": 1.0, \"loss\": 0.012919818982481956, \"time-step\": 841}, {\"accuracy\": 1.0, \"loss\": 0.0126874428242445, \"time-step\": 842}, {\"accuracy\": 1.0, \"loss\": 0.01246117427945137, \"time-step\": 843}, {\"accuracy\": 1.0, \"loss\": 0.01224082987755537, \"time-step\": 844}, {\"accuracy\": 1.0, \"loss\": 0.012026224285364151, \"time-step\": 845}, {\"accuracy\": 1.0, \"loss\": 0.011817146092653275, \"time-step\": 846}, {\"accuracy\": 1.0, \"loss\": 0.011613411828875542, \"time-step\": 847}, {\"accuracy\": 1.0, \"loss\": 0.011414896696805954, \"time-step\": 848}, {\"accuracy\": 1.0, \"loss\": 0.011221405118703842, \"time-step\": 849}, {\"accuracy\": 1.0, \"loss\": 0.01103278249502182, \"time-step\": 850}, {\"accuracy\": 1.0, \"loss\": 0.010848874226212502, \"time-step\": 851}, {\"accuracy\": 1.0, \"loss\": 0.010669521056115627, \"time-step\": 852}, {\"accuracy\": 1.0, \"loss\": 0.010494616813957691, \"time-step\": 853}, {\"accuracy\": 1.0, \"loss\": 0.01032397523522377, \"time-step\": 854}, {\"accuracy\": 1.0, \"loss\": 0.010157482698559761, \"time-step\": 855}, {\"accuracy\": 1.0, \"loss\": 0.009995035827159882, \"time-step\": 856}, {\"accuracy\": 1.0, \"loss\": 0.009836480021476746, \"time-step\": 857}, {\"accuracy\": 1.0, \"loss\": 0.009681709110736847, \"time-step\": 858}, {\"accuracy\": 1.0, \"loss\": 0.009530614130198956, \"time-step\": 859}, {\"accuracy\": 1.0, \"loss\": 0.009383076801896095, \"time-step\": 860}, {\"accuracy\": 1.0, \"loss\": 0.00923898909240961, \"time-step\": 861}, {\"accuracy\": 1.0, \"loss\": 0.009098231792449951, \"time-step\": 862}, {\"accuracy\": 1.0, \"loss\": 0.00896073505282402, \"time-step\": 863}, {\"accuracy\": 1.0, \"loss\": 0.008826391771435738, \"time-step\": 864}, {\"accuracy\": 1.0, \"loss\": 0.008695114403963089, \"time-step\": 865}, {\"accuracy\": 1.0, \"loss\": 0.008566796779632568, \"time-step\": 866}, {\"accuracy\": 1.0, \"loss\": 0.008441369980573654, \"time-step\": 867}, {\"accuracy\": 1.0, \"loss\": 0.008318731561303139, \"time-step\": 868}, {\"accuracy\": 1.0, \"loss\": 0.008198793977499008, \"time-step\": 869}, {\"accuracy\": 1.0, \"loss\": 0.008081527426838875, \"time-step\": 870}, {\"accuracy\": 1.0, \"loss\": 0.00796681921929121, \"time-step\": 871}, {\"accuracy\": 1.0, \"loss\": 0.007854592986404896, \"time-step\": 872}, {\"accuracy\": 1.0, \"loss\": 0.007744796108454466, \"time-step\": 873}, {\"accuracy\": 1.0, \"loss\": 0.007637339178472757, \"time-step\": 874}, {\"accuracy\": 1.0, \"loss\": 0.0075321923941373825, \"time-step\": 875}, {\"accuracy\": 1.0, \"loss\": 0.007429255172610283, \"time-step\": 876}, {\"accuracy\": 1.0, \"loss\": 0.007328473497182131, \"time-step\": 877}, {\"accuracy\": 1.0, \"loss\": 0.007229809649288654, \"time-step\": 878}, {\"accuracy\": 1.0, \"loss\": 0.007133185397833586, \"time-step\": 879}, {\"accuracy\": 1.0, \"loss\": 0.007038543000817299, \"time-step\": 880}, {\"accuracy\": 1.0, \"loss\": 0.0069458382204174995, \"time-step\": 881}, {\"accuracy\": 1.0, \"loss\": 0.006855018436908722, \"time-step\": 882}, {\"accuracy\": 1.0, \"loss\": 0.006766024976968765, \"time-step\": 883}, {\"accuracy\": 1.0, \"loss\": 0.006678825244307518, \"time-step\": 884}, {\"accuracy\": 1.0, \"loss\": 0.006593348458409309, \"time-step\": 885}, {\"accuracy\": 1.0, \"loss\": 0.006509576458483934, \"time-step\": 886}, {\"accuracy\": 1.0, \"loss\": 0.006427440792322159, \"time-step\": 887}, {\"accuracy\": 1.0, \"loss\": 0.0063469065353274345, \"time-step\": 888}, {\"accuracy\": 1.0, \"loss\": 0.006267930846661329, \"time-step\": 889}, {\"accuracy\": 1.0, \"loss\": 0.006190485320985317, \"time-step\": 890}, {\"accuracy\": 1.0, \"loss\": 0.0061145140789449215, \"time-step\": 891}, {\"accuracy\": 1.0, \"loss\": 0.006040005944669247, \"time-step\": 892}, {\"accuracy\": 1.0, \"loss\": 0.005966885015368462, \"time-step\": 893}, {\"accuracy\": 1.0, \"loss\": 0.005895126145333052, \"time-step\": 894}, {\"accuracy\": 1.0, \"loss\": 0.005824727937579155, \"time-step\": 895}, {\"accuracy\": 1.0, \"loss\": 0.005755627993494272, \"time-step\": 896}, {\"accuracy\": 1.0, \"loss\": 0.005687783472239971, \"time-step\": 897}, {\"accuracy\": 1.0, \"loss\": 0.005621189251542091, \"time-step\": 898}, {\"accuracy\": 1.0, \"loss\": 0.005555788520723581, \"time-step\": 899}, {\"accuracy\": 1.0, \"loss\": 0.005491593852639198, \"time-step\": 900}, {\"accuracy\": 1.0, \"loss\": 0.005428534001111984, \"time-step\": 901}, {\"accuracy\": 1.0, \"loss\": 0.005366600584238768, \"time-step\": 902}, {\"accuracy\": 1.0, \"loss\": 0.00530576054006815, \"time-step\": 903}, {\"accuracy\": 1.0, \"loss\": 0.005245994310826063, \"time-step\": 904}, {\"accuracy\": 1.0, \"loss\": 0.005187267437577248, \"time-step\": 905}, {\"accuracy\": 1.0, \"loss\": 0.00512955617159605, \"time-step\": 906}, {\"accuracy\": 1.0, \"loss\": 0.005072837229818106, \"time-step\": 907}, {\"accuracy\": 1.0, \"loss\": 0.005017098970711231, \"time-step\": 908}, {\"accuracy\": 1.0, \"loss\": 0.004962298087775707, \"time-step\": 909}, {\"accuracy\": 1.0, \"loss\": 0.004908428993076086, \"time-step\": 910}, {\"accuracy\": 1.0, \"loss\": 0.0048554567620158195, \"time-step\": 911}, {\"accuracy\": 1.0, \"loss\": 0.004803389310836792, \"time-step\": 912}, {\"accuracy\": 1.0, \"loss\": 0.0047521693632006645, \"time-step\": 913}, {\"accuracy\": 1.0, \"loss\": 0.00470179645344615, \"time-step\": 914}, {\"accuracy\": 1.0, \"loss\": 0.004652256611734629, \"time-step\": 915}, {\"accuracy\": 1.0, \"loss\": 0.004603507462888956, \"time-step\": 916}, {\"accuracy\": 1.0, \"loss\": 0.004555561579763889, \"time-step\": 917}, {\"accuracy\": 1.0, \"loss\": 0.0045083737932145596, \"time-step\": 918}, {\"accuracy\": 1.0, \"loss\": 0.004461945500224829, \"time-step\": 919}, {\"accuracy\": 1.0, \"loss\": 0.004416260868310928, \"time-step\": 920}, {\"accuracy\": 1.0, \"loss\": 0.004371287301182747, \"time-step\": 921}, {\"accuracy\": 1.0, \"loss\": 0.0043270159512758255, \"time-step\": 922}, {\"accuracy\": 1.0, \"loss\": 0.004283443093299866, \"time-step\": 923}, {\"accuracy\": 1.0, \"loss\": 0.0042405445128679276, \"time-step\": 924}, {\"accuracy\": 1.0, \"loss\": 0.004198296461254358, \"time-step\": 925}, {\"accuracy\": 1.0, \"loss\": 0.004156694281846285, \"time-step\": 926}, {\"accuracy\": 1.0, \"loss\": 0.004115735646337271, \"time-step\": 927}, {\"accuracy\": 1.0, \"loss\": 0.004075396806001663, \"time-step\": 928}, {\"accuracy\": 1.0, \"loss\": 0.004035658668726683, \"time-step\": 929}, {\"accuracy\": 1.0, \"loss\": 0.0039965081959962845, \"time-step\": 930}, {\"accuracy\": 1.0, \"loss\": 0.0039579421281814575, \"time-step\": 931}, {\"accuracy\": 1.0, \"loss\": 0.003919948823750019, \"time-step\": 932}, {\"accuracy\": 1.0, \"loss\": 0.0038825012743473053, \"time-step\": 933}, {\"accuracy\": 1.0, \"loss\": 0.0038456108886748552, \"time-step\": 934}, {\"accuracy\": 1.0, \"loss\": 0.0038092504255473614, \"time-step\": 935}, {\"accuracy\": 1.0, \"loss\": 0.0037734098732471466, \"time-step\": 936}, {\"accuracy\": 1.0, \"loss\": 0.003738081082701683, \"time-step\": 937}, {\"accuracy\": 1.0, \"loss\": 0.003703261259943247, \"time-step\": 938}, {\"accuracy\": 1.0, \"loss\": 0.003668929683044553, \"time-step\": 939}, {\"accuracy\": 1.0, \"loss\": 0.003635078901425004, \"time-step\": 940}, {\"accuracy\": 1.0, \"loss\": 0.0036016996018588543, \"time-step\": 941}, {\"accuracy\": 1.0, \"loss\": 0.003568793646991253, \"time-step\": 942}, {\"accuracy\": 1.0, \"loss\": 0.0035363249480724335, \"time-step\": 943}, {\"accuracy\": 1.0, \"loss\": 0.0035043186508119106, \"time-step\": 944}, {\"accuracy\": 1.0, \"loss\": 0.0034727402962744236, \"time-step\": 945}, {\"accuracy\": 1.0, \"loss\": 0.0034415971022099257, \"time-step\": 946}, {\"accuracy\": 1.0, \"loss\": 0.0034108608961105347, \"time-step\": 947}, {\"accuracy\": 1.0, \"loss\": 0.0033805472776293755, \"time-step\": 948}, {\"accuracy\": 1.0, \"loss\": 0.0033506397157907486, \"time-step\": 949}, {\"accuracy\": 1.0, \"loss\": 0.0033211233094334602, \"time-step\": 950}, {\"accuracy\": 1.0, \"loss\": 0.003291993634775281, \"time-step\": 951}, {\"accuracy\": 1.0, \"loss\": 0.0032632474321871996, \"time-step\": 952}, {\"accuracy\": 1.0, \"loss\": 0.0032348716631531715, \"time-step\": 953}, {\"accuracy\": 1.0, \"loss\": 0.0032068761065602303, \"time-step\": 954}, {\"accuracy\": 1.0, \"loss\": 0.0031792314257472754, \"time-step\": 955}, {\"accuracy\": 1.0, \"loss\": 0.003151944372802973, \"time-step\": 956}, {\"accuracy\": 1.0, \"loss\": 0.003125000512227416, \"time-step\": 957}, {\"accuracy\": 1.0, \"loss\": 0.003098404500633478, \"time-step\": 958}, {\"accuracy\": 1.0, \"loss\": 0.0030721407383680344, \"time-step\": 959}, {\"accuracy\": 1.0, \"loss\": 0.0030461978167295456, \"time-step\": 960}, {\"accuracy\": 1.0, \"loss\": 0.0030205855146050453, \"time-step\": 961}, {\"accuracy\": 1.0, \"loss\": 0.0029952924232929945, \"time-step\": 962}, {\"accuracy\": 1.0, \"loss\": 0.0029703034088015556, \"time-step\": 963}, {\"accuracy\": 1.0, \"loss\": 0.002945628482848406, \"time-step\": 964}, {\"accuracy\": 1.0, \"loss\": 0.002921250881627202, \"time-step\": 965}, {\"accuracy\": 1.0, \"loss\": 0.002897158730775118, \"time-step\": 966}, {\"accuracy\": 1.0, \"loss\": 0.0028733774088323116, \"time-step\": 967}, {\"accuracy\": 1.0, \"loss\": 0.002849870128557086, \"time-step\": 968}, {\"accuracy\": 1.0, \"loss\": 0.0028266345616430044, \"time-step\": 969}, {\"accuracy\": 1.0, \"loss\": 0.002803691430017352, \"time-step\": 970}, {\"accuracy\": 1.0, \"loss\": 0.002781011164188385, \"time-step\": 971}, {\"accuracy\": 1.0, \"loss\": 0.002758597256615758, \"time-step\": 972}, {\"accuracy\": 1.0, \"loss\": 0.0027364431880414486, \"time-step\": 973}, {\"accuracy\": 1.0, \"loss\": 0.0027145498897880316, \"time-step\": 974}, {\"accuracy\": 1.0, \"loss\": 0.0026929101441055536, \"time-step\": 975}, {\"accuracy\": 1.0, \"loss\": 0.002671516966074705, \"time-step\": 976}, {\"accuracy\": 1.0, \"loss\": 0.0026503661647439003, \"time-step\": 977}, {\"accuracy\": 1.0, \"loss\": 0.0026294616982340813, \"time-step\": 978}, {\"accuracy\": 1.0, \"loss\": 0.0026087926235049963, \"time-step\": 979}, {\"accuracy\": 1.0, \"loss\": 0.0025883461348712444, \"time-step\": 980}, {\"accuracy\": 1.0, \"loss\": 0.002568142954260111, \"time-step\": 981}, {\"accuracy\": 1.0, \"loss\": 0.002548153745010495, \"time-step\": 982}, {\"accuracy\": 1.0, \"loss\": 0.002528392244130373, \"time-step\": 983}, {\"accuracy\": 1.0, \"loss\": 0.002508846577256918, \"time-step\": 984}, {\"accuracy\": 1.0, \"loss\": 0.0024895137175917625, \"time-step\": 985}, {\"accuracy\": 1.0, \"loss\": 0.0024703999515622854, \"time-step\": 986}, {\"accuracy\": 1.0, \"loss\": 0.0024514882825315, \"time-step\": 987}, {\"accuracy\": 1.0, \"loss\": 0.002432777313515544, \"time-step\": 988}, {\"accuracy\": 1.0, \"loss\": 0.0024142717011272907, \"time-step\": 989}, {\"accuracy\": 1.0, \"loss\": 0.002395965624600649, \"time-step\": 990}, {\"accuracy\": 1.0, \"loss\": 0.002377858152613044, \"time-step\": 991}, {\"accuracy\": 1.0, \"loss\": 0.0023599350824952126, \"time-step\": 992}, {\"accuracy\": 1.0, \"loss\": 0.0023422036319971085, \"time-step\": 993}, {\"accuracy\": 1.0, \"loss\": 0.002324659377336502, \"time-step\": 994}, {\"accuracy\": 1.0, \"loss\": 0.002307299990206957, \"time-step\": 995}, {\"accuracy\": 1.0, \"loss\": 0.0022901203483343124, \"time-step\": 996}, {\"accuracy\": 1.0, \"loss\": 0.0022731118369847536, \"time-step\": 997}, {\"accuracy\": 1.0, \"loss\": 0.002256286796182394, \"time-step\": 998}, {\"accuracy\": 1.0, \"loss\": 0.002239637542515993, \"time-step\": 999}, {\"accuracy\": 1.0, \"loss\": 0.002223154529929161, \"time-step\": 1000}, {\"accuracy\": 1.0, \"loss\": 0.002206831704825163, \"time-step\": 1001}, {\"accuracy\": 1.0, \"loss\": 0.0021906825713813305, \"time-step\": 1002}, {\"accuracy\": 1.0, \"loss\": 0.0021747045684605837, \"time-step\": 1003}, {\"accuracy\": 1.0, \"loss\": 0.0021588727831840515, \"time-step\": 1004}, {\"accuracy\": 1.0, \"loss\": 0.0021432116627693176, \"time-step\": 1005}, {\"accuracy\": 1.0, \"loss\": 0.0021276993211358786, \"time-step\": 1006}, {\"accuracy\": 1.0, \"loss\": 0.002112362766638398, \"time-step\": 1007}, {\"accuracy\": 1.0, \"loss\": 0.0020971642807126045, \"time-step\": 1008}, {\"accuracy\": 1.0, \"loss\": 0.002082145307213068, \"time-step\": 1009}, {\"accuracy\": 1.0, \"loss\": 0.0020672983955591917, \"time-step\": 1010}, {\"accuracy\": 1.0, \"loss\": 0.0020526417065411806, \"time-step\": 1011}, {\"accuracy\": 1.0, \"loss\": 0.0020382024813443422, \"time-step\": 1012}, {\"accuracy\": 1.0, \"loss\": 0.002024069894105196, \"time-step\": 1013}, {\"accuracy\": 1.0, \"loss\": 0.002010318450629711, \"time-step\": 1014}, {\"accuracy\": 1.0, \"loss\": 0.001997115323320031, \"time-step\": 1015}, {\"accuracy\": 1.0, \"loss\": 0.001984747126698494, \"time-step\": 1016}, {\"accuracy\": 1.0, \"loss\": 0.0019732597284018993, \"time-step\": 1017}, {\"accuracy\": 1.0, \"loss\": 0.0019630405586212873, \"time-step\": 1018}, {\"accuracy\": 1.0, \"loss\": 0.00195330660790205, \"time-step\": 1019}, {\"accuracy\": 1.0, \"loss\": 0.0019442354096099734, \"time-step\": 1020}, {\"accuracy\": 1.0, \"loss\": 0.001934744417667389, \"time-step\": 1021}, {\"accuracy\": 1.0, \"loss\": 0.0019252365455031395, \"time-step\": 1022}, {\"accuracy\": 1.0, \"loss\": 0.0019152434542775154, \"time-step\": 1023}, {\"accuracy\": 1.0, \"loss\": 0.0019051528070122004, \"time-step\": 1024}, {\"accuracy\": 1.0, \"loss\": 0.0018948325887322426, \"time-step\": 1025}, {\"accuracy\": 1.0, \"loss\": 0.0018845032900571823, \"time-step\": 1026}, {\"accuracy\": 1.0, \"loss\": 0.0018740929663181305, \"time-step\": 1027}, {\"accuracy\": 1.0, \"loss\": 0.0018637212924659252, \"time-step\": 1028}, {\"accuracy\": 1.0, \"loss\": 0.0018533527618274093, \"time-step\": 1029}, {\"accuracy\": 1.0, \"loss\": 0.0018430596683174372, \"time-step\": 1030}, {\"accuracy\": 1.0, \"loss\": 0.001832801615819335, \"time-step\": 1031}, {\"accuracy\": 1.0, \"loss\": 0.0018226495012640953, \"time-step\": 1032}, {\"accuracy\": 1.0, \"loss\": 0.001812538830563426, \"time-step\": 1033}, {\"accuracy\": 1.0, \"loss\": 0.0018025580793619156, \"time-step\": 1034}, {\"accuracy\": 1.0, \"loss\": 0.0017926074797287583, \"time-step\": 1035}, {\"accuracy\": 1.0, \"loss\": 0.0017828216077759862, \"time-step\": 1036}, {\"accuracy\": 1.0, \"loss\": 0.0017730498220771551, \"time-step\": 1037}, {\"accuracy\": 1.0, \"loss\": 0.0017634487012401223, \"time-step\": 1038}, {\"accuracy\": 1.0, \"loss\": 0.0017538582906126976, \"time-step\": 1039}, {\"accuracy\": 1.0, \"loss\": 0.00174445821903646, \"time-step\": 1040}, {\"accuracy\": 1.0, \"loss\": 0.0017350467387586832, \"time-step\": 1041}, {\"accuracy\": 1.0, \"loss\": 0.0017258471343666315, \"time-step\": 1042}, {\"accuracy\": 1.0, \"loss\": 0.0017166233155876398, \"time-step\": 1043}, {\"accuracy\": 1.0, \"loss\": 0.0017076210351660848, \"time-step\": 1044}, {\"accuracy\": 1.0, \"loss\": 0.0016985588008537889, \"time-step\": 1045}, {\"accuracy\": 1.0, \"loss\": 0.0016897552413865924, \"time-step\": 1046}, {\"accuracy\": 1.0, \"loss\": 0.001680879620835185, \"time-step\": 1047}, {\"accuracy\": 1.0, \"loss\": 0.0016722624422982335, \"time-step\": 1048}, {\"accuracy\": 1.0, \"loss\": 0.0016635563224554062, \"time-step\": 1049}, {\"accuracy\": 1.0, \"loss\": 0.0016551227308809757, \"time-step\": 1050}, {\"accuracy\": 1.0, \"loss\": 0.0016465813387185335, \"time-step\": 1051}, {\"accuracy\": 1.0, \"loss\": 0.0016383357578888535, \"time-step\": 1052}, {\"accuracy\": 1.0, \"loss\": 0.0016299586277455091, \"time-step\": 1053}, {\"accuracy\": 1.0, \"loss\": 0.0016218950040638447, \"time-step\": 1054}, {\"accuracy\": 1.0, \"loss\": 0.0016136782942339778, \"time-step\": 1055}, {\"accuracy\": 1.0, \"loss\": 0.0016057975590229034, \"time-step\": 1056}, {\"accuracy\": 1.0, \"loss\": 0.0015977284638211131, \"time-step\": 1057}, {\"accuracy\": 1.0, \"loss\": 0.0015900180442258716, \"time-step\": 1058}, {\"accuracy\": 1.0, \"loss\": 0.0015821055276319385, \"time-step\": 1059}, {\"accuracy\": 1.0, \"loss\": 0.0015745664713904262, \"time-step\": 1060}, {\"accuracy\": 1.0, \"loss\": 0.0015668001724407077, \"time-step\": 1061}, {\"accuracy\": 1.0, \"loss\": 0.0015594282886013389, \"time-step\": 1062}, {\"accuracy\": 1.0, \"loss\": 0.0015517930733039975, \"time-step\": 1063}, {\"accuracy\": 1.0, \"loss\": 0.0015445852186530828, \"time-step\": 1064}, {\"accuracy\": 1.0, \"loss\": 0.0015370920300483704, \"time-step\": 1065}, {\"accuracy\": 1.0, \"loss\": 0.001530044712126255, \"time-step\": 1066}, {\"accuracy\": 1.0, \"loss\": 0.0015226873802021146, \"time-step\": 1067}, {\"accuracy\": 1.0, \"loss\": 0.0015157966408878565, \"time-step\": 1068}, {\"accuracy\": 1.0, \"loss\": 0.0015085720224305987, \"time-step\": 1069}, {\"accuracy\": 1.0, \"loss\": 0.0015018387930467725, \"time-step\": 1070}, {\"accuracy\": 1.0, \"loss\": 0.0014947319868952036, \"time-step\": 1071}, {\"accuracy\": 1.0, \"loss\": 0.001488144276663661, \"time-step\": 1072}, {\"accuracy\": 1.0, \"loss\": 0.0014811563305556774, \"time-step\": 1073}, {\"accuracy\": 1.0, \"loss\": 0.0014747235691174865, \"time-step\": 1074}, {\"accuracy\": 1.0, \"loss\": 0.0014678554143756628, \"time-step\": 1075}, {\"accuracy\": 1.0, \"loss\": 0.0014615667751058936, \"time-step\": 1076}, {\"accuracy\": 1.0, \"loss\": 0.001454809564165771, \"time-step\": 1077}, {\"accuracy\": 1.0, \"loss\": 0.0014486633008345962, \"time-step\": 1078}, {\"accuracy\": 1.0, \"loss\": 0.0014420215738937259, \"time-step\": 1079}, {\"accuracy\": 1.0, \"loss\": 0.0014360082568600774, \"time-step\": 1080}, {\"accuracy\": 1.0, \"loss\": 0.0014294740976765752, \"time-step\": 1081}, {\"accuracy\": 1.0, \"loss\": 0.0014236017595976591, \"time-step\": 1082}, {\"accuracy\": 1.0, \"loss\": 0.0014171685324981809, \"time-step\": 1083}, {\"accuracy\": 1.0, \"loss\": 0.001411430537700653, \"time-step\": 1084}, {\"accuracy\": 1.0, \"loss\": 0.0014050942845642567, \"time-step\": 1085}, {\"accuracy\": 1.0, \"loss\": 0.0013994931941851974, \"time-step\": 1086}, {\"accuracy\": 1.0, \"loss\": 0.0013932542642578483, \"time-step\": 1087}, {\"accuracy\": 1.0, \"loss\": 0.0013877741293981671, \"time-step\": 1088}, {\"accuracy\": 1.0, \"loss\": 0.001381627400405705, \"time-step\": 1089}, {\"accuracy\": 1.0, \"loss\": 0.0013762838207185268, \"time-step\": 1090}, {\"accuracy\": 1.0, \"loss\": 0.001370229641906917, \"time-step\": 1091}, {\"accuracy\": 1.0, \"loss\": 0.001365014468319714, \"time-step\": 1092}, {\"accuracy\": 1.0, \"loss\": 0.0013590536545962095, \"time-step\": 1093}, {\"accuracy\": 1.0, \"loss\": 0.0013539578067138791, \"time-step\": 1094}, {\"accuracy\": 1.0, \"loss\": 0.0013480816269293427, \"time-step\": 1095}, {\"accuracy\": 1.0, \"loss\": 0.0013431091792881489, \"time-step\": 1096}, {\"accuracy\": 1.0, \"loss\": 0.001337307272478938, \"time-step\": 1097}, {\"accuracy\": 1.0, \"loss\": 0.0013324515894055367, \"time-step\": 1098}, {\"accuracy\": 1.0, \"loss\": 0.00132672896143049, \"time-step\": 1099}, {\"accuracy\": 1.0, \"loss\": 0.0013219912070780993, \"time-step\": 1100}, {\"accuracy\": 1.0, \"loss\": 0.0013163440162315965, \"time-step\": 1101}, {\"accuracy\": 1.0, \"loss\": 0.0013117248890921474, \"time-step\": 1102}, {\"accuracy\": 1.0, \"loss\": 0.001306159421801567, \"time-step\": 1103}, {\"accuracy\": 1.0, \"loss\": 0.0013016540324315429, \"time-step\": 1104}, {\"accuracy\": 1.0, \"loss\": 0.0012961584143340588, \"time-step\": 1105}, {\"accuracy\": 1.0, \"loss\": 0.001291764434427023, \"time-step\": 1106}, {\"accuracy\": 1.0, \"loss\": 0.0012863336596637964, \"time-step\": 1107}, {\"accuracy\": 1.0, \"loss\": 0.0012820513220503926, \"time-step\": 1108}, {\"accuracy\": 1.0, \"loss\": 0.001276691909879446, \"time-step\": 1109}, {\"accuracy\": 1.0, \"loss\": 0.0012725174892693758, \"time-step\": 1110}, {\"accuracy\": 1.0, \"loss\": 0.0012672243174165487, \"time-step\": 1111}, {\"accuracy\": 1.0, \"loss\": 0.001263157231733203, \"time-step\": 1112}, {\"accuracy\": 1.0, \"loss\": 0.00125792995095253, \"time-step\": 1113}, {\"accuracy\": 1.0, \"loss\": 0.0012539713643491268, \"time-step\": 1114}, {\"accuracy\": 1.0, \"loss\": 0.0012488015927374363, \"time-step\": 1115}, {\"accuracy\": 1.0, \"loss\": 0.0012449456844478846, \"time-step\": 1116}, {\"accuracy\": 1.0, \"loss\": 0.0012398320250213146, \"time-step\": 1117}, {\"accuracy\": 1.0, \"loss\": 0.0012360865948721766, \"time-step\": 1118}, {\"accuracy\": 1.0, \"loss\": 0.0012310317251831293, \"time-step\": 1119}, {\"accuracy\": 1.0, \"loss\": 0.0012273852480575442, \"time-step\": 1120}, {\"accuracy\": 1.0, \"loss\": 0.001222381368279457, \"time-step\": 1121}, {\"accuracy\": 1.0, \"loss\": 0.0012188407126814127, \"time-step\": 1122}, {\"accuracy\": 1.0, \"loss\": 0.0012138852616772056, \"time-step\": 1123}, {\"accuracy\": 1.0, \"loss\": 0.0012104433262720704, \"time-step\": 1124}, {\"accuracy\": 1.0, \"loss\": 0.0012055401457473636, \"time-step\": 1125}, {\"accuracy\": 1.0, \"loss\": 0.0012022050796076655, \"time-step\": 1126}, {\"accuracy\": 1.0, \"loss\": 0.0011973486980423331, \"time-step\": 1127}, {\"accuracy\": 1.0, \"loss\": 0.001194102456793189, \"time-step\": 1128}, {\"accuracy\": 1.0, \"loss\": 0.0011892898473888636, \"time-step\": 1129}, {\"accuracy\": 1.0, \"loss\": 0.001186152221634984, \"time-step\": 1130}, {\"accuracy\": 1.0, \"loss\": 0.0011813875753432512, \"time-step\": 1131}, {\"accuracy\": 1.0, \"loss\": 0.001178342616185546, \"time-step\": 1132}, {\"accuracy\": 1.0, \"loss\": 0.0011736180167645216, \"time-step\": 1133}, {\"accuracy\": 1.0, \"loss\": 0.001170662697404623, \"time-step\": 1134}, {\"accuracy\": 1.0, \"loss\": 0.0011659798910841346, \"time-step\": 1135}, {\"accuracy\": 1.0, \"loss\": 0.0011631231755018234, \"time-step\": 1136}, {\"accuracy\": 1.0, \"loss\": 0.0011584785534068942, \"time-step\": 1137}, {\"accuracy\": 1.0, \"loss\": 0.0011557183461263776, \"time-step\": 1138}, {\"accuracy\": 1.0, \"loss\": 0.0011511154007166624, \"time-step\": 1139}, {\"accuracy\": 1.0, \"loss\": 0.0011484415736049414, \"time-step\": 1140}, {\"accuracy\": 1.0, \"loss\": 0.0011438721558079123, \"time-step\": 1141}, {\"accuracy\": 1.0, \"loss\": 0.0011412962339818478, \"time-step\": 1142}, {\"accuracy\": 1.0, \"loss\": 0.0011367520783096552, \"time-step\": 1143}, {\"accuracy\": 1.0, \"loss\": 0.0011342688230797648, \"time-step\": 1144}, {\"accuracy\": 1.0, \"loss\": 0.0011297613382339478, \"time-step\": 1145}, {\"accuracy\": 1.0, \"loss\": 0.001127370516769588, \"time-step\": 1146}, {\"accuracy\": 1.0, \"loss\": 0.0011228920193389058, \"time-step\": 1147}, {\"accuracy\": 1.0, \"loss\": 0.0011205917689949274, \"time-step\": 1148}, {\"accuracy\": 1.0, \"loss\": 0.0011161415604874492, \"time-step\": 1149}, {\"accuracy\": 1.0, \"loss\": 0.0011139271082356572, \"time-step\": 1150}, {\"accuracy\": 1.0, \"loss\": 0.0011095036752521992, \"time-step\": 1151}, {\"accuracy\": 1.0, \"loss\": 0.0011073738569393754, \"time-step\": 1152}, {\"accuracy\": 1.0, \"loss\": 0.001102980226278305, \"time-step\": 1153}, {\"accuracy\": 1.0, \"loss\": 0.0011009435402229428, \"time-step\": 1154}, {\"accuracy\": 1.0, \"loss\": 0.0010965735418722034, \"time-step\": 1155}, {\"accuracy\": 1.0, \"loss\": 0.0010946303373202682, \"time-step\": 1156}, {\"accuracy\": 1.0, \"loss\": 0.0010902798967435956, \"time-step\": 1157}, {\"accuracy\": 1.0, \"loss\": 0.0010884202783927321, \"time-step\": 1158}, {\"accuracy\": 1.0, \"loss\": 0.0010840859031304717, \"time-step\": 1159}, {\"accuracy\": 1.0, \"loss\": 0.0010823056800290942, \"time-step\": 1160}, {\"accuracy\": 1.0, \"loss\": 0.0010779885342344642, \"time-step\": 1161}, {\"accuracy\": 1.0, \"loss\": 0.0010763050522655249, \"time-step\": 1162}, {\"accuracy\": 1.0, \"loss\": 0.0010720186401158571, \"time-step\": 1163}, {\"accuracy\": 1.0, \"loss\": 0.0010704118758440018, \"time-step\": 1164}, {\"accuracy\": 1.0, \"loss\": 0.0010661367559805512, \"time-step\": 1165}, {\"accuracy\": 1.0, \"loss\": 0.0010646195150911808, \"time-step\": 1166}, {\"accuracy\": 1.0, \"loss\": 0.0010603528935462236, \"time-step\": 1167}, {\"accuracy\": 1.0, \"loss\": 0.0010589156299829483, \"time-step\": 1168}, {\"accuracy\": 1.0, \"loss\": 0.0010546690318733454, \"time-step\": 1169}, {\"accuracy\": 1.0, \"loss\": 0.0010533207096159458, \"time-step\": 1170}, {\"accuracy\": 1.0, \"loss\": 0.001049070619046688, \"time-step\": 1171}, {\"accuracy\": 1.0, \"loss\": 0.0010478104231879115, \"time-step\": 1172}, {\"accuracy\": 1.0, \"loss\": 0.0010435899021103978, \"time-step\": 1173}, {\"accuracy\": 1.0, \"loss\": 0.0010424097999930382, \"time-step\": 1174}, {\"accuracy\": 1.0, \"loss\": 0.0010381966130807996, \"time-step\": 1175}, {\"accuracy\": 1.0, \"loss\": 0.001037095789797604, \"time-step\": 1176}, {\"accuracy\": 1.0, \"loss\": 0.0010328812059015036, \"time-step\": 1177}, {\"accuracy\": 1.0, \"loss\": 0.0010318635031580925, \"time-step\": 1178}, {\"accuracy\": 1.0, \"loss\": 0.0010276640532538295, \"time-step\": 1179}, {\"accuracy\": 1.0, \"loss\": 0.0010267317993566394, \"time-step\": 1180}, {\"accuracy\": 1.0, \"loss\": 0.0010225343285128474, \"time-step\": 1181}, {\"accuracy\": 1.0, \"loss\": 0.0010216868249699473, \"time-step\": 1182}, {\"accuracy\": 1.0, \"loss\": 0.0010174941271543503, \"time-step\": 1183}, {\"accuracy\": 1.0, \"loss\": 0.0010167313739657402, \"time-step\": 1184}, {\"accuracy\": 1.0, \"loss\": 0.0010125464759767056, \"time-step\": 1185}, {\"accuracy\": 1.0, \"loss\": 0.00101185729727149, \"time-step\": 1186}, {\"accuracy\": 1.0, \"loss\": 0.0010076618054881692, \"time-step\": 1187}, {\"accuracy\": 1.0, \"loss\": 0.0010070576099678874, \"time-step\": 1188}, {\"accuracy\": 1.0, \"loss\": 0.0010028693359345198, \"time-step\": 1189}, {\"accuracy\": 1.0, \"loss\": 0.0010023437207564712, \"time-step\": 1190}, {\"accuracy\": 1.0, \"loss\": 0.0009981516050174832, \"time-step\": 1191}, {\"accuracy\": 1.0, \"loss\": 0.0009977072477340698, \"time-step\": 1192}, {\"accuracy\": 1.0, \"loss\": 0.0009935139678418636, \"time-step\": 1193}, {\"accuracy\": 1.0, \"loss\": 0.0009931535460054874, \"time-step\": 1194}, {\"accuracy\": 1.0, \"loss\": 0.0009889593347907066, \"time-step\": 1195}, {\"accuracy\": 1.0, \"loss\": 0.0009886811021715403, \"time-step\": 1196}, {\"accuracy\": 1.0, \"loss\": 0.0009844753658398986, \"time-step\": 1197}, {\"accuracy\": 1.0, \"loss\": 0.000984269310720265, \"time-step\": 1198}, {\"accuracy\": 1.0, \"loss\": 0.000980061711743474, \"time-step\": 1199}, {\"accuracy\": 1.0, \"loss\": 0.0009799377294257283, \"time-step\": 1200}, {\"accuracy\": 1.0, \"loss\": 0.0009757252992130816, \"time-step\": 1201}, {\"accuracy\": 1.0, \"loss\": 0.0009756851941347122, \"time-step\": 1202}, {\"accuracy\": 1.0, \"loss\": 0.0009714618790894747, \"time-step\": 1203}, {\"accuracy\": 1.0, \"loss\": 0.0009714920306578279, \"time-step\": 1204}, {\"accuracy\": 1.0, \"loss\": 0.0009672630694694817, \"time-step\": 1205}, {\"accuracy\": 1.0, \"loss\": 0.0009673787280917168, \"time-step\": 1206}, {\"accuracy\": 1.0, \"loss\": 0.0009631393477320671, \"time-step\": 1207}, {\"accuracy\": 1.0, \"loss\": 0.0009633309673517942, \"time-step\": 1208}, {\"accuracy\": 1.0, \"loss\": 0.0009590767440386117, \"time-step\": 1209}, {\"accuracy\": 1.0, \"loss\": 0.0009593502036295831, \"time-step\": 1210}, {\"accuracy\": 1.0, \"loss\": 0.000955090974457562, \"time-step\": 1211}, {\"accuracy\": 1.0, \"loss\": 0.0009554371936246753, \"time-step\": 1212}, {\"accuracy\": 1.0, \"loss\": 0.0009511589305475354, \"time-step\": 1213}, {\"accuracy\": 1.0, \"loss\": 0.0009515827405266464, \"time-step\": 1214}, {\"accuracy\": 1.0, \"loss\": 0.0009472862584516406, \"time-step\": 1215}, {\"accuracy\": 1.0, \"loss\": 0.0009477919666096568, \"time-step\": 1216}, {\"accuracy\": 1.0, \"loss\": 0.0009434873936697841, \"time-step\": 1217}, {\"accuracy\": 1.0, \"loss\": 0.0009440672001801431, \"time-step\": 1218}, {\"accuracy\": 1.0, \"loss\": 0.0009397414396516979, \"time-step\": 1219}, {\"accuracy\": 1.0, \"loss\": 0.000940396566875279, \"time-step\": 1220}, {\"accuracy\": 1.0, \"loss\": 0.0009360713884234428, \"time-step\": 1221}, {\"accuracy\": 1.0, \"loss\": 0.0009368120227009058, \"time-step\": 1222}, {\"accuracy\": 1.0, \"loss\": 0.0009324532002210617, \"time-step\": 1223}, {\"accuracy\": 1.0, \"loss\": 0.0009332621702924371, \"time-step\": 1224}, {\"accuracy\": 1.0, \"loss\": 0.0009288911824114621, \"time-step\": 1225}, {\"accuracy\": 1.0, \"loss\": 0.0009297836804762483, \"time-step\": 1226}, {\"accuracy\": 1.0, \"loss\": 0.0009253950556740165, \"time-step\": 1227}, {\"accuracy\": 1.0, \"loss\": 0.0009263600222766399, \"time-step\": 1228}, {\"accuracy\": 1.0, \"loss\": 0.0009219427010975778, \"time-step\": 1229}, {\"accuracy\": 1.0, \"loss\": 0.0009229839197359979, \"time-step\": 1230}, {\"accuracy\": 1.0, \"loss\": 0.0009185427334159613, \"time-step\": 1231}, {\"accuracy\": 1.0, \"loss\": 0.0009196651517413557, \"time-step\": 1232}, {\"accuracy\": 1.0, \"loss\": 0.000915195734705776, \"time-step\": 1233}, {\"accuracy\": 1.0, \"loss\": 0.0009163934737443924, \"time-step\": 1234}, {\"accuracy\": 1.0, \"loss\": 0.0009119169553741813, \"time-step\": 1235}, {\"accuracy\": 1.0, \"loss\": 0.0009131944389082491, \"time-step\": 1236}, {\"accuracy\": 1.0, \"loss\": 0.0009086810750886798, \"time-step\": 1237}, {\"accuracy\": 1.0, \"loss\": 0.0009100333554670215, \"time-step\": 1238}, {\"accuracy\": 1.0, \"loss\": 0.0009055020054802299, \"time-step\": 1239}, {\"accuracy\": 1.0, \"loss\": 0.0009069255320355296, \"time-step\": 1240}, {\"accuracy\": 1.0, \"loss\": 0.0009023665334098041, \"time-step\": 1241}, {\"accuracy\": 1.0, \"loss\": 0.0009038635762408376, \"time-step\": 1242}, {\"accuracy\": 1.0, \"loss\": 0.0008992754155769944, \"time-step\": 1243}, {\"accuracy\": 1.0, \"loss\": 0.000900853076018393, \"time-step\": 1244}, {\"accuracy\": 1.0, \"loss\": 0.0008962348219938576, \"time-step\": 1245}, {\"accuracy\": 1.0, \"loss\": 0.0008978769183158875, \"time-step\": 1246}, {\"accuracy\": 1.0, \"loss\": 0.0008932337514124811, \"time-step\": 1247}, {\"accuracy\": 1.0, \"loss\": 0.0008949629263952374, \"time-step\": 1248}, {\"accuracy\": 1.0, \"loss\": 0.0008902920526452363, \"time-step\": 1249}, {\"accuracy\": 1.0, \"loss\": 0.0008920882828533649, \"time-step\": 1250}, {\"accuracy\": 1.0, \"loss\": 0.0008873883634805679, \"time-step\": 1251}, {\"accuracy\": 1.0, \"loss\": 0.0008892625337466598, \"time-step\": 1252}, {\"accuracy\": 1.0, \"loss\": 0.0008845255943015218, \"time-step\": 1253}, {\"accuracy\": 1.0, \"loss\": 0.0008864744449965656, \"time-step\": 1254}, {\"accuracy\": 1.0, \"loss\": 0.0008817172492854297, \"time-step\": 1255}, {\"accuracy\": 1.0, \"loss\": 0.0008837378118187189, \"time-step\": 1256}, {\"accuracy\": 1.0, \"loss\": 0.0008789433632045984, \"time-step\": 1257}, {\"accuracy\": 1.0, \"loss\": 0.0008810400031507015, \"time-step\": 1258}, {\"accuracy\": 1.0, \"loss\": 0.0008762176148593426, \"time-step\": 1259}, {\"accuracy\": 1.0, \"loss\": 0.0008783920202404261, \"time-step\": 1260}, {\"accuracy\": 1.0, \"loss\": 0.0008735315641388297, \"time-step\": 1261}, {\"accuracy\": 1.0, \"loss\": 0.0008757732575759292, \"time-step\": 1262}, {\"accuracy\": 1.0, \"loss\": 0.0008708713576197624, \"time-step\": 1263}, {\"accuracy\": 1.0, \"loss\": 0.0008731838315725327, \"time-step\": 1264}, {\"accuracy\": 1.0, \"loss\": 0.000868258299306035, \"time-step\": 1265}, {\"accuracy\": 1.0, \"loss\": 0.0008706502267159522, \"time-step\": 1266}, {\"accuracy\": 1.0, \"loss\": 0.0008656904101371765, \"time-step\": 1267}, {\"accuracy\": 1.0, \"loss\": 0.0008681531762704253, \"time-step\": 1268}, {\"accuracy\": 1.0, \"loss\": 0.0008631497621536255, \"time-step\": 1269}, {\"accuracy\": 1.0, \"loss\": 0.0008656835998408496, \"time-step\": 1270}, {\"accuracy\": 1.0, \"loss\": 0.0008606497431173921, \"time-step\": 1271}, {\"accuracy\": 1.0, \"loss\": 0.0008632525568827987, \"time-step\": 1272}, {\"accuracy\": 1.0, \"loss\": 0.0008581865695305169, \"time-step\": 1273}, {\"accuracy\": 1.0, \"loss\": 0.0008608647040091455, \"time-step\": 1274}, {\"accuracy\": 1.0, \"loss\": 0.0008557582041248679, \"time-step\": 1275}, {\"accuracy\": 1.0, \"loss\": 0.0008585104369558394, \"time-step\": 1276}, {\"accuracy\": 1.0, \"loss\": 0.0008533596410416067, \"time-step\": 1277}, {\"accuracy\": 1.0, \"loss\": 0.0008561799768358469, \"time-step\": 1278}, {\"accuracy\": 1.0, \"loss\": 0.0008509872131980956, \"time-step\": 1279}, {\"accuracy\": 1.0, \"loss\": 0.0008538816473446786, \"time-step\": 1280}, {\"accuracy\": 1.0, \"loss\": 0.0008486649603582919, \"time-step\": 1281}, {\"accuracy\": 1.0, \"loss\": 0.0008516244124621153, \"time-step\": 1282}, {\"accuracy\": 1.0, \"loss\": 0.000846362323500216, \"time-step\": 1283}, {\"accuracy\": 1.0, \"loss\": 0.0008493991335853934, \"time-step\": 1284}, {\"accuracy\": 1.0, \"loss\": 0.0008440932724624872, \"time-step\": 1285}, {\"accuracy\": 1.0, \"loss\": 0.0008471993496641517, \"time-step\": 1286}, {\"accuracy\": 1.0, \"loss\": 0.0008418592042289674, \"time-step\": 1287}, {\"accuracy\": 1.0, \"loss\": 0.0008450352470390499, \"time-step\": 1288}, {\"accuracy\": 1.0, \"loss\": 0.000839652435388416, \"time-step\": 1289}, {\"accuracy\": 1.0, \"loss\": 0.0008428941364400089, \"time-step\": 1290}, {\"accuracy\": 1.0, \"loss\": 0.0008374698809348047, \"time-step\": 1291}, {\"accuracy\": 1.0, \"loss\": 0.0008407896384596825, \"time-step\": 1292}, {\"accuracy\": 1.0, \"loss\": 0.0008353195153176785, \"time-step\": 1293}, {\"accuracy\": 1.0, \"loss\": 0.0008387099951505661, \"time-step\": 1294}, {\"accuracy\": 1.0, \"loss\": 0.0008332039578817785, \"time-step\": 1295}, {\"accuracy\": 1.0, \"loss\": 0.0008366607362404466, \"time-step\": 1296}, {\"accuracy\": 1.0, \"loss\": 0.0008311012643389404, \"time-step\": 1297}, {\"accuracy\": 1.0, \"loss\": 0.0008346339454874396, \"time-step\": 1298}, {\"accuracy\": 1.0, \"loss\": 0.0008290413534268737, \"time-step\": 1299}, {\"accuracy\": 1.0, \"loss\": 0.0008326403331011534, \"time-step\": 1300}, {\"accuracy\": 1.0, \"loss\": 0.0008270100224763155, \"time-step\": 1301}, {\"accuracy\": 1.0, \"loss\": 0.000830673729069531, \"time-step\": 1302}, {\"accuracy\": 1.0, \"loss\": 0.0008249991806223989, \"time-step\": 1303}, {\"accuracy\": 1.0, \"loss\": 0.000828731688670814, \"time-step\": 1304}, {\"accuracy\": 1.0, \"loss\": 0.0008230069652199745, \"time-step\": 1305}, {\"accuracy\": 1.0, \"loss\": 0.0008268042001873255, \"time-step\": 1306}, {\"accuracy\": 1.0, \"loss\": 0.0008210357627831399, \"time-step\": 1307}, {\"accuracy\": 1.0, \"loss\": 0.000824898888822645, \"time-step\": 1308}, {\"accuracy\": 1.0, \"loss\": 0.0008190900553017855, \"time-step\": 1309}, {\"accuracy\": 1.0, \"loss\": 0.0008230266976170242, \"time-step\": 1310}, {\"accuracy\": 1.0, \"loss\": 0.0008171743247658014, \"time-step\": 1311}, {\"accuracy\": 1.0, \"loss\": 0.0008211738895624876, \"time-step\": 1312}, {\"accuracy\": 1.0, \"loss\": 0.0008152681402862072, \"time-step\": 1313}, {\"accuracy\": 1.0, \"loss\": 0.0008193363901227713, \"time-step\": 1314}, {\"accuracy\": 1.0, \"loss\": 0.0008134006056934595, \"time-step\": 1315}, {\"accuracy\": 1.0, \"loss\": 0.0008175446419045329, \"time-step\": 1316}, {\"accuracy\": 1.0, \"loss\": 0.0008115584496408701, \"time-step\": 1317}, {\"accuracy\": 1.0, \"loss\": 0.0008157528936862946, \"time-step\": 1318}, {\"accuracy\": 1.0, \"loss\": 0.0008097123354673386, \"time-step\": 1319}, {\"accuracy\": 1.0, \"loss\": 0.0008139783749356866, \"time-step\": 1320}, {\"accuracy\": 1.0, \"loss\": 0.0008079000399447978, \"time-step\": 1321}, {\"accuracy\": 1.0, \"loss\": 0.000812221085652709, \"time-step\": 1322}, {\"accuracy\": 1.0, \"loss\": 0.000806101830676198, \"time-step\": 1323}, {\"accuracy\": 1.0, \"loss\": 0.0008104945882223547, \"time-step\": 1324}, {\"accuracy\": 1.0, \"loss\": 0.0008043277775868773, \"time-step\": 1325}, {\"accuracy\": 1.0, \"loss\": 0.0008087807800620794, \"time-step\": 1326}, {\"accuracy\": 1.0, \"loss\": 0.0008025709539651871, \"time-step\": 1327}, {\"accuracy\": 1.0, \"loss\": 0.0008070939802564681, \"time-step\": 1328}, {\"accuracy\": 1.0, \"loss\": 0.0008008418371900916, \"time-step\": 1329}, {\"accuracy\": 1.0, \"loss\": 0.000805423129349947, \"time-step\": 1330}, {\"accuracy\": 1.0, \"loss\": 0.0007991253514774144, \"time-step\": 1331}, {\"accuracy\": 1.0, \"loss\": 0.0008037769002839923, \"time-step\": 1332}, {\"accuracy\": 1.0, \"loss\": 0.000797431159298867, \"time-step\": 1333}, {\"accuracy\": 1.0, \"loss\": 0.0008021387620829046, \"time-step\": 1334}, {\"accuracy\": 1.0, \"loss\": 0.0007957496563903987, \"time-step\": 1335}, {\"accuracy\": 1.0, \"loss\": 0.000800527457613498, \"time-step\": 1336}, {\"accuracy\": 1.0, \"loss\": 0.0007940828800201416, \"time-step\": 1337}, {\"accuracy\": 1.0, \"loss\": 0.0007989166770130396, \"time-step\": 1338}, {\"accuracy\": 1.0, \"loss\": 0.0007924307719804347, \"time-step\": 1339}, {\"accuracy\": 1.0, \"loss\": 0.0007973242900334299, \"time-step\": 1340}, {\"accuracy\": 1.0, \"loss\": 0.0007907886174507439, \"time-step\": 1341}, {\"accuracy\": 1.0, \"loss\": 0.0007957536727190018, \"time-step\": 1342}, {\"accuracy\": 1.0, \"loss\": 0.0007891817367635667, \"time-step\": 1343}, {\"accuracy\": 1.0, \"loss\": 0.0007942018564790487, \"time-step\": 1344}, {\"accuracy\": 1.0, \"loss\": 0.000787581317126751, \"time-step\": 1345}, {\"accuracy\": 1.0, \"loss\": 0.0007926715770736337, \"time-step\": 1346}, {\"accuracy\": 1.0, \"loss\": 0.0007860016776248813, \"time-step\": 1347}, {\"accuracy\": 1.0, \"loss\": 0.0007911428110674024, \"time-step\": 1348}, {\"accuracy\": 1.0, \"loss\": 0.0007844238425604999, \"time-step\": 1349}, {\"accuracy\": 1.0, \"loss\": 0.0007896266179159284, \"time-step\": 1350}, {\"accuracy\": 1.0, \"loss\": 0.0007828688248991966, \"time-step\": 1351}, {\"accuracy\": 1.0, \"loss\": 0.0007881303899921477, \"time-step\": 1352}, {\"accuracy\": 1.0, \"loss\": 0.0007813292322680354, \"time-step\": 1353}, {\"accuracy\": 1.0, \"loss\": 0.0007866537780500948, \"time-step\": 1354}, {\"accuracy\": 1.0, \"loss\": 0.0007798000006005168, \"time-step\": 1355}, {\"accuracy\": 1.0, \"loss\": 0.000785180542152375, \"time-step\": 1356}, {\"accuracy\": 1.0, \"loss\": 0.0007782846223562956, \"time-step\": 1357}, {\"accuracy\": 1.0, \"loss\": 0.0007837212760932744, \"time-step\": 1358}, {\"accuracy\": 1.0, \"loss\": 0.0007767757633700967, \"time-step\": 1359}, {\"accuracy\": 1.0, \"loss\": 0.0007822649204172194, \"time-step\": 1360}, {\"accuracy\": 1.0, \"loss\": 0.000775277498178184, \"time-step\": 1361}, {\"accuracy\": 1.0, \"loss\": 0.0007808214286342263, \"time-step\": 1362}, {\"accuracy\": 1.0, \"loss\": 0.0007737856358289719, \"time-step\": 1363}, {\"accuracy\": 1.0, \"loss\": 0.0007793967379257083, \"time-step\": 1364}, {\"accuracy\": 1.0, \"loss\": 0.0007723094895482063, \"time-step\": 1365}, {\"accuracy\": 1.0, \"loss\": 0.0007779693114571273, \"time-step\": 1366}, {\"accuracy\": 1.0, \"loss\": 0.0007708414923399687, \"time-step\": 1367}, {\"accuracy\": 1.0, \"loss\": 0.0007765595219098032, \"time-step\": 1368}, {\"accuracy\": 1.0, \"loss\": 0.0007693840307183564, \"time-step\": 1369}, {\"accuracy\": 1.0, \"loss\": 0.0007751549128443003, \"time-step\": 1370}, {\"accuracy\": 1.0, \"loss\": 0.0007679364643990993, \"time-step\": 1371}, {\"accuracy\": 1.0, \"loss\": 0.0007737571140751243, \"time-step\": 1372}, {\"accuracy\": 1.0, \"loss\": 0.0007665021694265306, \"time-step\": 1373}, {\"accuracy\": 1.0, \"loss\": 0.0007723961025476456, \"time-step\": 1374}, {\"accuracy\": 1.0, \"loss\": 0.000765094649977982, \"time-step\": 1375}, {\"accuracy\": 1.0, \"loss\": 0.0007710286299698055, \"time-step\": 1376}, {\"accuracy\": 1.0, \"loss\": 0.0007636761874891818, \"time-step\": 1377}, {\"accuracy\": 1.0, \"loss\": 0.0007696691318415105, \"time-step\": 1378}, {\"accuracy\": 1.0, \"loss\": 0.0007622771663591266, \"time-step\": 1379}, {\"accuracy\": 1.0, \"loss\": 0.0007683250005356967, \"time-step\": 1380}, {\"accuracy\": 1.0, \"loss\": 0.0007608940359205008, \"time-step\": 1381}, {\"accuracy\": 1.0, \"loss\": 0.0007670000777579844, \"time-step\": 1382}, {\"accuracy\": 1.0, \"loss\": 0.0007595269707962871, \"time-step\": 1383}, {\"accuracy\": 1.0, \"loss\": 0.0007656803936697543, \"time-step\": 1384}, {\"accuracy\": 1.0, \"loss\": 0.000758154084905982, \"time-step\": 1385}, {\"accuracy\": 1.0, \"loss\": 0.0007643603021278977, \"time-step\": 1386}, {\"accuracy\": 1.0, \"loss\": 0.0007567950524389744, \"time-step\": 1387}, {\"accuracy\": 1.0, \"loss\": 0.0007630441687069833, \"time-step\": 1388}, {\"accuracy\": 1.0, \"loss\": 0.0007554288604296744, \"time-step\": 1389}, {\"accuracy\": 1.0, \"loss\": 0.000761732691898942, \"time-step\": 1390}, {\"accuracy\": 1.0, \"loss\": 0.000754068954847753, \"time-step\": 1391}, {\"accuracy\": 1.0, \"loss\": 0.0007604292477481067, \"time-step\": 1392}, {\"accuracy\": 1.0, \"loss\": 0.0007527359994128346, \"time-step\": 1393}, {\"accuracy\": 1.0, \"loss\": 0.000759142218157649, \"time-step\": 1394}, {\"accuracy\": 1.0, \"loss\": 0.0007514039753004909, \"time-step\": 1395}, {\"accuracy\": 1.0, \"loss\": 0.0007578571094200015, \"time-step\": 1396}, {\"accuracy\": 1.0, \"loss\": 0.0007500715437345207, \"time-step\": 1397}, {\"accuracy\": 1.0, \"loss\": 0.000756575318519026, \"time-step\": 1398}, {\"accuracy\": 1.0, \"loss\": 0.0007487527327612042, \"time-step\": 1399}, {\"accuracy\": 1.0, \"loss\": 0.0007553035975433886, \"time-step\": 1400}, {\"accuracy\": 1.0, \"loss\": 0.0007474385201931, \"time-step\": 1401}, {\"accuracy\": 1.0, \"loss\": 0.0007540376391261816, \"time-step\": 1402}, {\"accuracy\": 1.0, \"loss\": 0.0007461217464879155, \"time-step\": 1403}, {\"accuracy\": 1.0, \"loss\": 0.0007527695270255208, \"time-step\": 1404}, {\"accuracy\": 1.0, \"loss\": 0.0007448244141414762, \"time-step\": 1405}, {\"accuracy\": 1.0, \"loss\": 0.0007515117176808417, \"time-step\": 1406}, {\"accuracy\": 1.0, \"loss\": 0.0007435210281983018, \"time-step\": 1407}, {\"accuracy\": 1.0, \"loss\": 0.0007502579246647656, \"time-step\": 1408}, {\"accuracy\": 1.0, \"loss\": 0.0007422323105856776, \"time-step\": 1409}, {\"accuracy\": 1.0, \"loss\": 0.0007490236312150955, \"time-step\": 1410}, {\"accuracy\": 1.0, \"loss\": 0.0007409617537632585, \"time-step\": 1411}, {\"accuracy\": 1.0, \"loss\": 0.0007477890467271209, \"time-step\": 1412}, {\"accuracy\": 1.0, \"loss\": 0.0007396771106868982, \"time-step\": 1413}, {\"accuracy\": 1.0, \"loss\": 0.0007465564412996173, \"time-step\": 1414}, {\"accuracy\": 1.0, \"loss\": 0.0007384145283140242, \"time-step\": 1415}, {\"accuracy\": 1.0, \"loss\": 0.0007453309372067451, \"time-step\": 1416}, {\"accuracy\": 1.0, \"loss\": 0.0007371515384875238, \"time-step\": 1417}, {\"accuracy\": 1.0, \"loss\": 0.0007441189954988658, \"time-step\": 1418}, {\"accuracy\": 1.0, \"loss\": 0.0007358938455581665, \"time-step\": 1419}, {\"accuracy\": 1.0, \"loss\": 0.0007428985554724932, \"time-step\": 1420}, {\"accuracy\": 1.0, \"loss\": 0.0007346358615905046, \"time-step\": 1421}, {\"accuracy\": 1.0, \"loss\": 0.0007416863809339702, \"time-step\": 1422}, {\"accuracy\": 1.0, \"loss\": 0.0007333827670663595, \"time-step\": 1423}, {\"accuracy\": 1.0, \"loss\": 0.0007404781063087285, \"time-step\": 1424}, {\"accuracy\": 1.0, \"loss\": 0.0007321361335925758, \"time-step\": 1425}, {\"accuracy\": 1.0, \"loss\": 0.0007392739644274116, \"time-step\": 1426}, {\"accuracy\": 1.0, \"loss\": 0.000730906322132796, \"time-step\": 1427}, {\"accuracy\": 1.0, \"loss\": 0.0007380815222859383, \"time-step\": 1428}, {\"accuracy\": 1.0, \"loss\": 0.0007296685944311321, \"time-step\": 1429}, {\"accuracy\": 1.0, \"loss\": 0.0007368822698481381, \"time-step\": 1430}, {\"accuracy\": 1.0, \"loss\": 0.0007284340099431574, \"time-step\": 1431}, {\"accuracy\": 1.0, \"loss\": 0.0007357000722549856, \"time-step\": 1432}, {\"accuracy\": 1.0, \"loss\": 0.0007272170041687787, \"time-step\": 1433}, {\"accuracy\": 1.0, \"loss\": 0.0007345157209783792, \"time-step\": 1434}, {\"accuracy\": 1.0, \"loss\": 0.0007259990088641644, \"time-step\": 1435}, {\"accuracy\": 1.0, \"loss\": 0.0007333321264013648, \"time-step\": 1436}, {\"accuracy\": 1.0, \"loss\": 0.0007247708272188902, \"time-step\": 1437}, {\"accuracy\": 1.0, \"loss\": 0.0007321442244574428, \"time-step\": 1438}, {\"accuracy\": 1.0, \"loss\": 0.0007235475350171328, \"time-step\": 1439}, {\"accuracy\": 1.0, \"loss\": 0.0007309585926122963, \"time-step\": 1440}, {\"accuracy\": 1.0, \"loss\": 0.0007223336142487824, \"time-step\": 1441}, {\"accuracy\": 1.0, \"loss\": 0.0007297860574908555, \"time-step\": 1442}, {\"accuracy\": 1.0, \"loss\": 0.0007211179472506046, \"time-step\": 1443}, {\"accuracy\": 1.0, \"loss\": 0.0007286087493412197, \"time-step\": 1444}, {\"accuracy\": 1.0, \"loss\": 0.0007199221872724593, \"time-step\": 1445}, {\"accuracy\": 1.0, \"loss\": 0.0007274530362337828, \"time-step\": 1446}, {\"accuracy\": 1.0, \"loss\": 0.000718729745130986, \"time-step\": 1447}, {\"accuracy\": 1.0, \"loss\": 0.0007262928993441164, \"time-step\": 1448}, {\"accuracy\": 1.0, \"loss\": 0.00071752566145733, \"time-step\": 1449}, {\"accuracy\": 1.0, \"loss\": 0.0007251249626278877, \"time-step\": 1450}, {\"accuracy\": 1.0, \"loss\": 0.0007163324626162648, \"time-step\": 1451}, {\"accuracy\": 1.0, \"loss\": 0.0007239733822643757, \"time-step\": 1452}, {\"accuracy\": 1.0, \"loss\": 0.00071514374576509, \"time-step\": 1453}, {\"accuracy\": 1.0, \"loss\": 0.0007228141184896231, \"time-step\": 1454}, {\"accuracy\": 1.0, \"loss\": 0.0007139631197787821, \"time-step\": 1455}, {\"accuracy\": 1.0, \"loss\": 0.0007216733647510409, \"time-step\": 1456}, {\"accuracy\": 1.0, \"loss\": 0.0007127862190827727, \"time-step\": 1457}, {\"accuracy\": 1.0, \"loss\": 0.0007205350557342172, \"time-step\": 1458}, {\"accuracy\": 1.0, \"loss\": 0.0007116106571629643, \"time-step\": 1459}, {\"accuracy\": 1.0, \"loss\": 0.0007193837082013488, \"time-step\": 1460}, {\"accuracy\": 1.0, \"loss\": 0.0007104261312633753, \"time-step\": 1461}, {\"accuracy\": 1.0, \"loss\": 0.0007182346889749169, \"time-step\": 1462}, {\"accuracy\": 1.0, \"loss\": 0.0007092522573657334, \"time-step\": 1463}, {\"accuracy\": 1.0, \"loss\": 0.0007170946337282658, \"time-step\": 1464}, {\"accuracy\": 1.0, \"loss\": 0.0007080859504640102, \"time-step\": 1465}, {\"accuracy\": 1.0, \"loss\": 0.0007159548113122582, \"time-step\": 1466}, {\"accuracy\": 1.0, \"loss\": 0.0007069101557135582, \"time-step\": 1467}, {\"accuracy\": 1.0, \"loss\": 0.0007148142904043198, \"time-step\": 1468}, {\"accuracy\": 1.0, \"loss\": 0.0007057401817291975, \"time-step\": 1469}, {\"accuracy\": 1.0, \"loss\": 0.0007136788335628808, \"time-step\": 1470}, {\"accuracy\": 1.0, \"loss\": 0.0007045770180411637, \"time-step\": 1471}, {\"accuracy\": 1.0, \"loss\": 0.000712545239366591, \"time-step\": 1472}, {\"accuracy\": 1.0, \"loss\": 0.0007034179288893938, \"time-step\": 1473}, {\"accuracy\": 1.0, \"loss\": 0.0007114156614989042, \"time-step\": 1474}, {\"accuracy\": 1.0, \"loss\": 0.0007022556965239346, \"time-step\": 1475}, {\"accuracy\": 1.0, \"loss\": 0.0007102823001332581, \"time-step\": 1476}, {\"accuracy\": 1.0, \"loss\": 0.0007011019624769688, \"time-step\": 1477}, {\"accuracy\": 1.0, \"loss\": 0.0007091612787917256, \"time-step\": 1478}, {\"accuracy\": 1.0, \"loss\": 0.0006999514298513532, \"time-step\": 1479}, {\"accuracy\": 1.0, \"loss\": 0.0007080433424562216, \"time-step\": 1480}, {\"accuracy\": 1.0, \"loss\": 0.0006988043896853924, \"time-step\": 1481}, {\"accuracy\": 1.0, \"loss\": 0.0007069173734635115, \"time-step\": 1482}, {\"accuracy\": 1.0, \"loss\": 0.0006976491422392428, \"time-step\": 1483}, {\"accuracy\": 1.0, \"loss\": 0.0007057965267449617, \"time-step\": 1484}, {\"accuracy\": 1.0, \"loss\": 0.0006965083885006607, \"time-step\": 1485}, {\"accuracy\": 1.0, \"loss\": 0.0007046789396554232, \"time-step\": 1486}, {\"accuracy\": 1.0, \"loss\": 0.0006953620468266308, \"time-step\": 1487}, {\"accuracy\": 1.0, \"loss\": 0.0007035625167191029, \"time-step\": 1488}, {\"accuracy\": 1.0, \"loss\": 0.0006942258332856, \"time-step\": 1489}, {\"accuracy\": 1.0, \"loss\": 0.000702467979863286, \"time-step\": 1490}, {\"accuracy\": 1.0, \"loss\": 0.0006930985255166888, \"time-step\": 1491}, {\"accuracy\": 1.0, \"loss\": 0.0007013592403382063, \"time-step\": 1492}, {\"accuracy\": 1.0, \"loss\": 0.0006919580046087503, \"time-step\": 1493}, {\"accuracy\": 1.0, \"loss\": 0.0007002290803939104, \"time-step\": 1494}, {\"accuracy\": 1.0, \"loss\": 0.0006908071227371693, \"time-step\": 1495}, {\"accuracy\": 1.0, \"loss\": 0.0006991183618083596, \"time-step\": 1496}, {\"accuracy\": 1.0, \"loss\": 0.0006896779523231089, \"time-step\": 1497}, {\"accuracy\": 1.0, \"loss\": 0.0006980056059546769, \"time-step\": 1498}, {\"accuracy\": 1.0, \"loss\": 0.0006885439506731927, \"time-step\": 1499}, {\"accuracy\": 1.0, \"loss\": 0.0006969034438952804, \"time-step\": 1500}, {\"accuracy\": 1.0, \"loss\": 0.0006874153623357415, \"time-step\": 1501}, {\"accuracy\": 1.0, \"loss\": 0.0006957975565455854, \"time-step\": 1502}, {\"accuracy\": 1.0, \"loss\": 0.000686280953232199, \"time-step\": 1503}, {\"accuracy\": 1.0, \"loss\": 0.0006946898065507412, \"time-step\": 1504}, {\"accuracy\": 1.0, \"loss\": 0.00068515888415277, \"time-step\": 1505}, {\"accuracy\": 1.0, \"loss\": 0.0006935971323400736, \"time-step\": 1506}, {\"accuracy\": 1.0, \"loss\": 0.0006840461865067482, \"time-step\": 1507}, {\"accuracy\": 1.0, \"loss\": 0.0006924987537786365, \"time-step\": 1508}, {\"accuracy\": 1.0, \"loss\": 0.0006829278427176178, \"time-step\": 1509}, {\"accuracy\": 1.0, \"loss\": 0.000691403925884515, \"time-step\": 1510}, {\"accuracy\": 1.0, \"loss\": 0.0006818058900535107, \"time-step\": 1511}, {\"accuracy\": 1.0, \"loss\": 0.0006903017638251185, \"time-step\": 1512}, {\"accuracy\": 1.0, \"loss\": 0.0006806754972785711, \"time-step\": 1513}, {\"accuracy\": 1.0, \"loss\": 0.0006891898228786886, \"time-step\": 1514}, {\"accuracy\": 1.0, \"loss\": 0.0006795490044169128, \"time-step\": 1515}, {\"accuracy\": 1.0, \"loss\": 0.0006880967412143946, \"time-step\": 1516}, {\"accuracy\": 1.0, \"loss\": 0.0006784448050893843, \"time-step\": 1517}, {\"accuracy\": 1.0, \"loss\": 0.0006870243232697248, \"time-step\": 1518}, {\"accuracy\": 1.0, \"loss\": 0.0006773496861569583, \"time-step\": 1519}, {\"accuracy\": 1.0, \"loss\": 0.0006859286804683506, \"time-step\": 1520}, {\"accuracy\": 1.0, \"loss\": 0.0006762309931218624, \"time-step\": 1521}, {\"accuracy\": 1.0, \"loss\": 0.0006848389166407287, \"time-step\": 1522}, {\"accuracy\": 1.0, \"loss\": 0.0006751270266249776, \"time-step\": 1523}, {\"accuracy\": 1.0, \"loss\": 0.0006837549735791981, \"time-step\": 1524}, {\"accuracy\": 1.0, \"loss\": 0.000674012117087841, \"time-step\": 1525}, {\"accuracy\": 1.0, \"loss\": 0.000682657933793962, \"time-step\": 1526}, {\"accuracy\": 1.0, \"loss\": 0.0006729092565365136, \"time-step\": 1527}, {\"accuracy\": 1.0, \"loss\": 0.0006815761444158852, \"time-step\": 1528}, {\"accuracy\": 1.0, \"loss\": 0.000671799061819911, \"time-step\": 1529}, {\"accuracy\": 1.0, \"loss\": 0.0006804821896366775, \"time-step\": 1530}, {\"accuracy\": 1.0, \"loss\": 0.0006706952699460089, \"time-step\": 1531}, {\"accuracy\": 1.0, \"loss\": 0.0006794066866859794, \"time-step\": 1532}, {\"accuracy\": 1.0, \"loss\": 0.0006696015479974449, \"time-step\": 1533}, {\"accuracy\": 1.0, \"loss\": 0.0006783247808925807, \"time-step\": 1534}, {\"accuracy\": 1.0, \"loss\": 0.0006685046828351915, \"time-step\": 1535}, {\"accuracy\": 1.0, \"loss\": 0.0006772509077563882, \"time-step\": 1536}, {\"accuracy\": 1.0, \"loss\": 0.0006674128817394376, \"time-step\": 1537}, {\"accuracy\": 1.0, \"loss\": 0.000676170748192817, \"time-step\": 1538}, {\"accuracy\": 1.0, \"loss\": 0.000666322186589241, \"time-step\": 1539}, {\"accuracy\": 1.0, \"loss\": 0.0006750974571332335, \"time-step\": 1540}, {\"accuracy\": 1.0, \"loss\": 0.0006652271840721369, \"time-step\": 1541}, {\"accuracy\": 1.0, \"loss\": 0.0006740227108821273, \"time-step\": 1542}, {\"accuracy\": 1.0, \"loss\": 0.0006641469663009048, \"time-step\": 1543}, {\"accuracy\": 1.0, \"loss\": 0.0006729676388204098, \"time-step\": 1544}, {\"accuracy\": 1.0, \"loss\": 0.0006630741409026086, \"time-step\": 1545}, {\"accuracy\": 1.0, \"loss\": 0.0006719038356095552, \"time-step\": 1546}, {\"accuracy\": 1.0, \"loss\": 0.0006619811756536365, \"time-step\": 1547}, {\"accuracy\": 1.0, \"loss\": 0.0006708237924613059, \"time-step\": 1548}, {\"accuracy\": 1.0, \"loss\": 0.0006609033443965018, \"time-step\": 1549}, {\"accuracy\": 1.0, \"loss\": 0.0006697601638734341, \"time-step\": 1550}, {\"accuracy\": 1.0, \"loss\": 0.0006598086329177022, \"time-step\": 1551}, {\"accuracy\": 1.0, \"loss\": 0.0006686830311082304, \"time-step\": 1552}, {\"accuracy\": 1.0, \"loss\": 0.0006587306270375848, \"time-step\": 1553}, {\"accuracy\": 1.0, \"loss\": 0.0006676226621493697, \"time-step\": 1554}, {\"accuracy\": 1.0, \"loss\": 0.0006576618179678917, \"time-step\": 1555}, {\"accuracy\": 1.0, \"loss\": 0.000666564330458641, \"time-step\": 1556}, {\"accuracy\": 1.0, \"loss\": 0.0006565892836079001, \"time-step\": 1557}, {\"accuracy\": 1.0, \"loss\": 0.0006655199103988707, \"time-step\": 1558}, {\"accuracy\": 1.0, \"loss\": 0.0006555257714353502, \"time-step\": 1559}, {\"accuracy\": 1.0, \"loss\": 0.0006644579116255045, \"time-step\": 1560}, {\"accuracy\": 1.0, \"loss\": 0.0006544546922668815, \"time-step\": 1561}, {\"accuracy\": 1.0, \"loss\": 0.000663410872220993, \"time-step\": 1562}, {\"accuracy\": 1.0, \"loss\": 0.0006534007843583822, \"time-step\": 1563}, {\"accuracy\": 1.0, \"loss\": 0.0006623638910241425, \"time-step\": 1564}, {\"accuracy\": 1.0, \"loss\": 0.0006523339543491602, \"time-step\": 1565}, {\"accuracy\": 1.0, \"loss\": 0.0006613189470954239, \"time-step\": 1566}, {\"accuracy\": 1.0, \"loss\": 0.0006512851687148213, \"time-step\": 1567}, {\"accuracy\": 1.0, \"loss\": 0.0006602772627957165, \"time-step\": 1568}, {\"accuracy\": 1.0, \"loss\": 0.0006502179894596338, \"time-step\": 1569}, {\"accuracy\": 1.0, \"loss\": 0.0006592161953449249, \"time-step\": 1570}, {\"accuracy\": 1.0, \"loss\": 0.0006491502281278372, \"time-step\": 1571}, {\"accuracy\": 1.0, \"loss\": 0.0006581658963114023, \"time-step\": 1572}, {\"accuracy\": 1.0, \"loss\": 0.0006480869487859309, \"time-step\": 1573}, {\"accuracy\": 1.0, \"loss\": 0.0006571128615178168, \"time-step\": 1574}, {\"accuracy\": 1.0, \"loss\": 0.0006470257649198174, \"time-step\": 1575}, {\"accuracy\": 1.0, \"loss\": 0.0006560638430528343, \"time-step\": 1576}, {\"accuracy\": 1.0, \"loss\": 0.0006459724972955883, \"time-step\": 1577}, {\"accuracy\": 1.0, \"loss\": 0.0006550242542289197, \"time-step\": 1578}, {\"accuracy\": 1.0, \"loss\": 0.0006449125939980149, \"time-step\": 1579}, {\"accuracy\": 1.0, \"loss\": 0.0006539722671732306, \"time-step\": 1580}, {\"accuracy\": 1.0, \"loss\": 0.0006438422715291381, \"time-step\": 1581}, {\"accuracy\": 1.0, \"loss\": 0.000652909919153899, \"time-step\": 1582}, {\"accuracy\": 1.0, \"loss\": 0.0006427853368222713, \"time-step\": 1583}, {\"accuracy\": 1.0, \"loss\": 0.0006518743466585875, \"time-step\": 1584}, {\"accuracy\": 1.0, \"loss\": 0.0006417515105567873, \"time-step\": 1585}, {\"accuracy\": 1.0, \"loss\": 0.0006508509395644069, \"time-step\": 1586}, {\"accuracy\": 1.0, \"loss\": 0.0006407134933397174, \"time-step\": 1587}, {\"accuracy\": 1.0, \"loss\": 0.0006498186266981065, \"time-step\": 1588}, {\"accuracy\": 1.0, \"loss\": 0.0006396663375198841, \"time-step\": 1589}, {\"accuracy\": 1.0, \"loss\": 0.0006487717619165778, \"time-step\": 1590}, {\"accuracy\": 1.0, \"loss\": 0.0006386182503774762, \"time-step\": 1591}, {\"accuracy\": 1.0, \"loss\": 0.0006477420101873577, \"time-step\": 1592}, {\"accuracy\": 1.0, \"loss\": 0.0006375765078701079, \"time-step\": 1593}, {\"accuracy\": 1.0, \"loss\": 0.0006467251223511994, \"time-step\": 1594}, {\"accuracy\": 1.0, \"loss\": 0.0006365534500218928, \"time-step\": 1595}, {\"accuracy\": 1.0, \"loss\": 0.0006456958362832665, \"time-step\": 1596}, {\"accuracy\": 1.0, \"loss\": 0.0006355104851536453, \"time-step\": 1597}, {\"accuracy\": 1.0, \"loss\": 0.0006446609040722251, \"time-step\": 1598}, {\"accuracy\": 1.0, \"loss\": 0.0006344717112369835, \"time-step\": 1599}, {\"accuracy\": 1.0, \"loss\": 0.0006436363328248262, \"time-step\": 1600}, {\"accuracy\": 1.0, \"loss\": 0.000633448245935142, \"time-step\": 1601}, {\"accuracy\": 1.0, \"loss\": 0.000642618746496737, \"time-step\": 1602}, {\"accuracy\": 1.0, \"loss\": 0.0006324052228592336, \"time-step\": 1603}, {\"accuracy\": 1.0, \"loss\": 0.0006415760144591331, \"time-step\": 1604}, {\"accuracy\": 1.0, \"loss\": 0.0006313652847893536, \"time-step\": 1605}, {\"accuracy\": 1.0, \"loss\": 0.0006405534804798663, \"time-step\": 1606}, {\"accuracy\": 1.0, \"loss\": 0.0006303467089310288, \"time-step\": 1607}, {\"accuracy\": 1.0, \"loss\": 0.0006395411910489202, \"time-step\": 1608}, {\"accuracy\": 1.0, \"loss\": 0.0006293138721957803, \"time-step\": 1609}, {\"accuracy\": 1.0, \"loss\": 0.0006385180167853832, \"time-step\": 1610}, {\"accuracy\": 1.0, \"loss\": 0.0006282908143475652, \"time-step\": 1611}, {\"accuracy\": 1.0, \"loss\": 0.0006374980439431965, \"time-step\": 1612}, {\"accuracy\": 1.0, \"loss\": 0.0006272737518884242, \"time-step\": 1613}, {\"accuracy\": 1.0, \"loss\": 0.000636499491520226, \"time-step\": 1614}, {\"accuracy\": 1.0, \"loss\": 0.0006262724054977298, \"time-step\": 1615}, {\"accuracy\": 1.0, \"loss\": 0.0006354958168230951, \"time-step\": 1616}, {\"accuracy\": 1.0, \"loss\": 0.000625253131147474, \"time-step\": 1617}, {\"accuracy\": 1.0, \"loss\": 0.000634486903436482, \"time-step\": 1618}, {\"accuracy\": 1.0, \"loss\": 0.000624240143224597, \"time-step\": 1619}, {\"accuracy\": 1.0, \"loss\": 0.000633472518529743, \"time-step\": 1620}, {\"accuracy\": 1.0, \"loss\": 0.0006232266896404326, \"time-step\": 1621}, {\"accuracy\": 1.0, \"loss\": 0.0006324697751551867, \"time-step\": 1622}, {\"accuracy\": 1.0, \"loss\": 0.0006222124211490154, \"time-step\": 1623}, {\"accuracy\": 1.0, \"loss\": 0.0006314509082585573, \"time-step\": 1624}, {\"accuracy\": 1.0, \"loss\": 0.0006212010630406439, \"time-step\": 1625}, {\"accuracy\": 1.0, \"loss\": 0.0006304645212367177, \"time-step\": 1626}, {\"accuracy\": 1.0, \"loss\": 0.0006201997748576105, \"time-step\": 1627}, {\"accuracy\": 1.0, \"loss\": 0.0006294645136222243, \"time-step\": 1628}, {\"accuracy\": 1.0, \"loss\": 0.0006192050641402602, \"time-step\": 1629}, {\"accuracy\": 1.0, \"loss\": 0.0006284736446104944, \"time-step\": 1630}, {\"accuracy\": 1.0, \"loss\": 0.0006181995267979801, \"time-step\": 1631}, {\"accuracy\": 1.0, \"loss\": 0.0006274704355746508, \"time-step\": 1632}, {\"accuracy\": 1.0, \"loss\": 0.0006172000430524349, \"time-step\": 1633}, {\"accuracy\": 1.0, \"loss\": 0.0006264773546718061, \"time-step\": 1634}, {\"accuracy\": 1.0, \"loss\": 0.0006161986384540796, \"time-step\": 1635}, {\"accuracy\": 1.0, \"loss\": 0.0006254778127186, \"time-step\": 1636}, {\"accuracy\": 1.0, \"loss\": 0.0006152071873657405, \"time-step\": 1637}, {\"accuracy\": 1.0, \"loss\": 0.0006244963733479381, \"time-step\": 1638}, {\"accuracy\": 1.0, \"loss\": 0.0006142144557088614, \"time-step\": 1639}, {\"accuracy\": 1.0, \"loss\": 0.0006235017790459096, \"time-step\": 1640}, {\"accuracy\": 1.0, \"loss\": 0.0006132147391326725, \"time-step\": 1641}, {\"accuracy\": 1.0, \"loss\": 0.0006225117249414325, \"time-step\": 1642}, {\"accuracy\": 1.0, \"loss\": 0.0006122275372035801, \"time-step\": 1643}, {\"accuracy\": 1.0, \"loss\": 0.000621524581220001, \"time-step\": 1644}, {\"accuracy\": 1.0, \"loss\": 0.0006112423725426197, \"time-step\": 1645}, {\"accuracy\": 1.0, \"loss\": 0.0006205482641234994, \"time-step\": 1646}, {\"accuracy\": 1.0, \"loss\": 0.0006102650659158826, \"time-step\": 1647}, {\"accuracy\": 1.0, \"loss\": 0.0006195719470269978, \"time-step\": 1648}, {\"accuracy\": 1.0, \"loss\": 0.000609283393714577, \"time-step\": 1649}, {\"accuracy\": 1.0, \"loss\": 0.0006186006939969957, \"time-step\": 1650}, {\"accuracy\": 1.0, \"loss\": 0.0006083105690777302, \"time-step\": 1651}, {\"accuracy\": 1.0, \"loss\": 0.0006176220485940576, \"time-step\": 1652}, {\"accuracy\": 1.0, \"loss\": 0.0006073274998925626, \"time-step\": 1653}, {\"accuracy\": 1.0, \"loss\": 0.0006166420644149184, \"time-step\": 1654}, {\"accuracy\": 1.0, \"loss\": 0.0006063480977900326, \"time-step\": 1655}, {\"accuracy\": 1.0, \"loss\": 0.0006156773306429386, \"time-step\": 1656}, {\"accuracy\": 1.0, \"loss\": 0.0006053795223124325, \"time-step\": 1657}, {\"accuracy\": 1.0, \"loss\": 0.0006146982195787132, \"time-step\": 1658}, {\"accuracy\": 1.0, \"loss\": 0.0006044043111614883, \"time-step\": 1659}, {\"accuracy\": 1.0, \"loss\": 0.0006137276068329811, \"time-step\": 1660}, {\"accuracy\": 1.0, \"loss\": 0.0006034325924701989, \"time-step\": 1661}, {\"accuracy\": 1.0, \"loss\": 0.0006127642700448632, \"time-step\": 1662}, {\"accuracy\": 1.0, \"loss\": 0.0006024674512445927, \"time-step\": 1663}, {\"accuracy\": 1.0, \"loss\": 0.0006118002929724753, \"time-step\": 1664}, {\"accuracy\": 1.0, \"loss\": 0.0006015020189806819, \"time-step\": 1665}, {\"accuracy\": 1.0, \"loss\": 0.0006108386442065239, \"time-step\": 1666}, {\"accuracy\": 1.0, \"loss\": 0.0006005537579767406, \"time-step\": 1667}, {\"accuracy\": 1.0, \"loss\": 0.0006098857847973704, \"time-step\": 1668}, {\"accuracy\": 1.0, \"loss\": 0.0005995909450575709, \"time-step\": 1669}, {\"accuracy\": 1.0, \"loss\": 0.0006089262897148728, \"time-step\": 1670}, {\"accuracy\": 1.0, \"loss\": 0.0005986287724226713, \"time-step\": 1671}, {\"accuracy\": 1.0, \"loss\": 0.0006079755257815123, \"time-step\": 1672}, {\"accuracy\": 1.0, \"loss\": 0.0005976827815175056, \"time-step\": 1673}, {\"accuracy\": 1.0, \"loss\": 0.0006070189992897213, \"time-step\": 1674}, {\"accuracy\": 1.0, \"loss\": 0.0005967202596366405, \"time-step\": 1675}, {\"accuracy\": 1.0, \"loss\": 0.0006060683517716825, \"time-step\": 1676}, {\"accuracy\": 1.0, \"loss\": 0.0005957786925137043, \"time-step\": 1677}, {\"accuracy\": 1.0, \"loss\": 0.000605122884735465, \"time-step\": 1678}, {\"accuracy\": 1.0, \"loss\": 0.0005948321195319295, \"time-step\": 1679}, {\"accuracy\": 1.0, \"loss\": 0.0006041832384653389, \"time-step\": 1680}, {\"accuracy\": 1.0, \"loss\": 0.0005938874091953039, \"time-step\": 1681}, {\"accuracy\": 1.0, \"loss\": 0.0006032304372638464, \"time-step\": 1682}, {\"accuracy\": 1.0, \"loss\": 0.0005929357139393687, \"time-step\": 1683}, {\"accuracy\": 1.0, \"loss\": 0.0006022803718224168, \"time-step\": 1684}, {\"accuracy\": 1.0, \"loss\": 0.0005919878603890538, \"time-step\": 1685}, {\"accuracy\": 1.0, \"loss\": 0.0006013396778143942, \"time-step\": 1686}, {\"accuracy\": 1.0, \"loss\": 0.0005910529871471226, \"time-step\": 1687}, {\"accuracy\": 1.0, \"loss\": 0.0006003957241773605, \"time-step\": 1688}, {\"accuracy\": 1.0, \"loss\": 0.0005901084514334798, \"time-step\": 1689}, {\"accuracy\": 1.0, \"loss\": 0.0005994652165099978, \"time-step\": 1690}, {\"accuracy\": 1.0, \"loss\": 0.0005891731125302613, \"time-step\": 1691}, {\"accuracy\": 1.0, \"loss\": 0.000598520040512085, \"time-step\": 1692}, {\"accuracy\": 1.0, \"loss\": 0.0005882316618226469, \"time-step\": 1693}, {\"accuracy\": 1.0, \"loss\": 0.0005975863896310329, \"time-step\": 1694}, {\"accuracy\": 1.0, \"loss\": 0.0005873146001249552, \"time-step\": 1695}, {\"accuracy\": 1.0, \"loss\": 0.0005966698518022895, \"time-step\": 1696}, {\"accuracy\": 1.0, \"loss\": 0.0005863907863385975, \"time-step\": 1697}, {\"accuracy\": 1.0, \"loss\": 0.0005957421381026506, \"time-step\": 1698}, {\"accuracy\": 1.0, \"loss\": 0.0005854709888808429, \"time-step\": 1699}, {\"accuracy\": 1.0, \"loss\": 0.0005948239704594016, \"time-step\": 1700}, {\"accuracy\": 1.0, \"loss\": 0.0005845522973686457, \"time-step\": 1701}, {\"accuracy\": 1.0, \"loss\": 0.0005939091788604856, \"time-step\": 1702}, {\"accuracy\": 1.0, \"loss\": 0.0005836361669935286, \"time-step\": 1703}, {\"accuracy\": 1.0, \"loss\": 0.0005929842009209096, \"time-step\": 1704}, {\"accuracy\": 1.0, \"loss\": 0.0005827175918966532, \"time-step\": 1705}, {\"accuracy\": 1.0, \"loss\": 0.0005920809926465154, \"time-step\": 1706}, {\"accuracy\": 1.0, \"loss\": 0.000581823056563735, \"time-step\": 1707}, {\"accuracy\": 1.0, \"loss\": 0.0005911763291805983, \"time-step\": 1708}, {\"accuracy\": 1.0, \"loss\": 0.0005809125723317266, \"time-step\": 1709}, {\"accuracy\": 1.0, \"loss\": 0.0005902625853195786, \"time-step\": 1710}, {\"accuracy\": 1.0, \"loss\": 0.0005799956852570176, \"time-step\": 1711}, {\"accuracy\": 1.0, \"loss\": 0.0005893417401239276, \"time-step\": 1712}, {\"accuracy\": 1.0, \"loss\": 0.0005790821160189807, \"time-step\": 1713}, {\"accuracy\": 1.0, \"loss\": 0.0005884302081540227, \"time-step\": 1714}, {\"accuracy\": 1.0, \"loss\": 0.0005781736690551043, \"time-step\": 1715}, {\"accuracy\": 1.0, \"loss\": 0.0005875235656276345, \"time-step\": 1716}, {\"accuracy\": 1.0, \"loss\": 0.0005772663280367851, \"time-step\": 1717}, {\"accuracy\": 1.0, \"loss\": 0.0005866124993190169, \"time-step\": 1718}, {\"accuracy\": 1.0, \"loss\": 0.000576363003347069, \"time-step\": 1719}, {\"accuracy\": 1.0, \"loss\": 0.0005857055657543242, \"time-step\": 1720}, {\"accuracy\": 1.0, \"loss\": 0.0005754501326009631, \"time-step\": 1721}, {\"accuracy\": 1.0, \"loss\": 0.000584796944167465, \"time-step\": 1722}, {\"accuracy\": 1.0, \"loss\": 0.0005745610105805099, \"time-step\": 1723}, {\"accuracy\": 1.0, \"loss\": 0.0005839013610966504, \"time-step\": 1724}, {\"accuracy\": 1.0, \"loss\": 0.0005736628663726151, \"time-step\": 1725}, {\"accuracy\": 1.0, \"loss\": 0.0005830094451084733, \"time-step\": 1726}, {\"accuracy\": 1.0, \"loss\": 0.0005727763054892421, \"time-step\": 1727}, {\"accuracy\": 1.0, \"loss\": 0.0005821125232614577, \"time-step\": 1728}, {\"accuracy\": 1.0, \"loss\": 0.000571871642023325, \"time-step\": 1729}, {\"accuracy\": 1.0, \"loss\": 0.0005812100134789944, \"time-step\": 1730}, {\"accuracy\": 1.0, \"loss\": 0.0005709903780370951, \"time-step\": 1731}, {\"accuracy\": 1.0, \"loss\": 0.0005803239764645696, \"time-step\": 1732}, {\"accuracy\": 1.0, \"loss\": 0.000570107891689986, \"time-step\": 1733}, {\"accuracy\": 1.0, \"loss\": 0.0005794440512545407, \"time-step\": 1734}, {\"accuracy\": 1.0, \"loss\": 0.0005692231934517622, \"time-step\": 1735}, {\"accuracy\": 1.0, \"loss\": 0.0005785563844256103, \"time-step\": 1736}, {\"accuracy\": 1.0, \"loss\": 0.0005683509516529739, \"time-step\": 1737}, {\"accuracy\": 1.0, \"loss\": 0.0005776812904514372, \"time-step\": 1738}, {\"accuracy\": 1.0, \"loss\": 0.0005674741696566343, \"time-step\": 1739}, {\"accuracy\": 1.0, \"loss\": 0.0005768011906184256, \"time-step\": 1740}, {\"accuracy\": 1.0, \"loss\": 0.000566601287573576, \"time-step\": 1741}, {\"accuracy\": 1.0, \"loss\": 0.0005759262130595744, \"time-step\": 1742}, {\"accuracy\": 1.0, \"loss\": 0.0005657104775309563, \"time-step\": 1743}, {\"accuracy\": 1.0, \"loss\": 0.0005750351119786501, \"time-step\": 1744}, {\"accuracy\": 1.0, \"loss\": 0.0005648351507261395, \"time-step\": 1745}, {\"accuracy\": 1.0, \"loss\": 0.0005741590284742415, \"time-step\": 1746}, {\"accuracy\": 1.0, \"loss\": 0.0005639721639454365, \"time-step\": 1747}, {\"accuracy\": 1.0, \"loss\": 0.000573295692447573, \"time-step\": 1748}, {\"accuracy\": 1.0, \"loss\": 0.0005631063249893486, \"time-step\": 1749}, {\"accuracy\": 1.0, \"loss\": 0.0005724241491407156, \"time-step\": 1750}, {\"accuracy\": 1.0, \"loss\": 0.0005622546304948628, \"time-step\": 1751}, {\"accuracy\": 1.0, \"loss\": 0.0005715748993679881, \"time-step\": 1752}, {\"accuracy\": 1.0, \"loss\": 0.0005613989196717739, \"time-step\": 1753}, {\"accuracy\": 1.0, \"loss\": 0.0005707103409804404, \"time-step\": 1754}, {\"accuracy\": 1.0, \"loss\": 0.0005605361075140536, \"time-step\": 1755}, {\"accuracy\": 1.0, \"loss\": 0.0005698482855223119, \"time-step\": 1756}, {\"accuracy\": 1.0, \"loss\": 0.0005596887203864753, \"time-step\": 1757}, {\"accuracy\": 1.0, \"loss\": 0.000569001364056021, \"time-step\": 1758}, {\"accuracy\": 1.0, \"loss\": 0.0005588412750512362, \"time-step\": 1759}, {\"accuracy\": 1.0, \"loss\": 0.0005681548500433564, \"time-step\": 1760}, {\"accuracy\": 1.0, \"loss\": 0.0005579957505688071, \"time-step\": 1761}, {\"accuracy\": 1.0, \"loss\": 0.0005672984989359975, \"time-step\": 1762}, {\"accuracy\": 1.0, \"loss\": 0.0005571424262598157, \"time-step\": 1763}, {\"accuracy\": 1.0, \"loss\": 0.0005664479685947299, \"time-step\": 1764}, {\"accuracy\": 1.0, \"loss\": 0.0005562991718761623, \"time-step\": 1765}, {\"accuracy\": 1.0, \"loss\": 0.0005655998829752207, \"time-step\": 1766}, {\"accuracy\": 1.0, \"loss\": 0.0005554576637223363, \"time-step\": 1767}, {\"accuracy\": 1.0, \"loss\": 0.0005647490033879876, \"time-step\": 1768}, {\"accuracy\": 1.0, \"loss\": 0.0005546194734051824, \"time-step\": 1769}, {\"accuracy\": 1.0, \"loss\": 0.0005639191949740052, \"time-step\": 1770}, {\"accuracy\": 1.0, \"loss\": 0.0005537918768823147, \"time-step\": 1771}, {\"accuracy\": 1.0, \"loss\": 0.000563079840503633, \"time-step\": 1772}, {\"accuracy\": 1.0, \"loss\": 0.0005529529298655689, \"time-step\": 1773}, {\"accuracy\": 1.0, \"loss\": 0.0005622318712994456, \"time-step\": 1774}, {\"accuracy\": 1.0, \"loss\": 0.0005521170678548515, \"time-step\": 1775}, {\"accuracy\": 1.0, \"loss\": 0.0005614034598693252, \"time-step\": 1776}, {\"accuracy\": 1.0, \"loss\": 0.0005512914503924549, \"time-step\": 1777}, {\"accuracy\": 1.0, \"loss\": 0.0005605682963505387, \"time-step\": 1778}, {\"accuracy\": 1.0, \"loss\": 0.0005504512810148299, \"time-step\": 1779}, {\"accuracy\": 1.0, \"loss\": 0.000559723237529397, \"time-step\": 1780}, {\"accuracy\": 1.0, \"loss\": 0.0005496129742823541, \"time-step\": 1781}, {\"accuracy\": 1.0, \"loss\": 0.0005588879575952888, \"time-step\": 1782}, {\"accuracy\": 1.0, \"loss\": 0.0005487913731485605, \"time-step\": 1783}, {\"accuracy\": 1.0, \"loss\": 0.0005580537253990769, \"time-step\": 1784}, {\"accuracy\": 1.0, \"loss\": 0.0005479640676639974, \"time-step\": 1785}, {\"accuracy\": 1.0, \"loss\": 0.0005572346271947026, \"time-step\": 1786}, {\"accuracy\": 1.0, \"loss\": 0.0005471525364555418, \"time-step\": 1787}, {\"accuracy\": 1.0, \"loss\": 0.0005564095336012542, \"time-step\": 1788}, {\"accuracy\": 1.0, \"loss\": 0.0005463304114528, \"time-step\": 1789}, {\"accuracy\": 1.0, \"loss\": 0.0005555902607738972, \"time-step\": 1790}, {\"accuracy\": 1.0, \"loss\": 0.0005455223144963384, \"time-step\": 1791}, {\"accuracy\": 1.0, \"loss\": 0.0005547684850171208, \"time-step\": 1792}, {\"accuracy\": 1.0, \"loss\": 0.0005447021685540676, \"time-step\": 1793}, {\"accuracy\": 1.0, \"loss\": 0.000553950376342982, \"time-step\": 1794}, {\"accuracy\": 1.0, \"loss\": 0.00054388795979321, \"time-step\": 1795}, {\"accuracy\": 1.0, \"loss\": 0.0005531299393624067, \"time-step\": 1796}, {\"accuracy\": 1.0, \"loss\": 0.0005430732853710651, \"time-step\": 1797}, {\"accuracy\": 1.0, \"loss\": 0.0005523150321096182, \"time-step\": 1798}, {\"accuracy\": 1.0, \"loss\": 0.0005422724061645567, \"time-step\": 1799}, {\"accuracy\": 1.0, \"loss\": 0.0005515224765986204, \"time-step\": 1800}, {\"accuracy\": 1.0, \"loss\": 0.0005414830520749092, \"time-step\": 1801}, {\"accuracy\": 1.0, \"loss\": 0.0005507150781340897, \"time-step\": 1802}, {\"accuracy\": 1.0, \"loss\": 0.0005406757118180394, \"time-step\": 1803}, {\"accuracy\": 1.0, \"loss\": 0.0005499027320183814, \"time-step\": 1804}, {\"accuracy\": 1.0, \"loss\": 0.0005398704088293016, \"time-step\": 1805}, {\"accuracy\": 1.0, \"loss\": 0.0005491023184731603, \"time-step\": 1806}, {\"accuracy\": 1.0, \"loss\": 0.0005390786682255566, \"time-step\": 1807}, {\"accuracy\": 1.0, \"loss\": 0.0005483010318130255, \"time-step\": 1808}, {\"accuracy\": 1.0, \"loss\": 0.0005382822710089386, \"time-step\": 1809}, {\"accuracy\": 1.0, \"loss\": 0.0005475016077980399, \"time-step\": 1810}, {\"accuracy\": 1.0, \"loss\": 0.0005374943139031529, \"time-step\": 1811}, {\"accuracy\": 1.0, \"loss\": 0.0005467082373797894, \"time-step\": 1812}, {\"accuracy\": 1.0, \"loss\": 0.0005367065314203501, \"time-step\": 1813}, {\"accuracy\": 1.0, \"loss\": 0.0005459195817820728, \"time-step\": 1814}, {\"accuracy\": 1.0, \"loss\": 0.0005359210772439837, \"time-step\": 1815}, {\"accuracy\": 1.0, \"loss\": 0.0005451327306218445, \"time-step\": 1816}, {\"accuracy\": 1.0, \"loss\": 0.0005351470317691565, \"time-step\": 1817}, {\"accuracy\": 1.0, \"loss\": 0.0005443480331450701, \"time-step\": 1818}, {\"accuracy\": 1.0, \"loss\": 0.0005343559314496815, \"time-step\": 1819}, {\"accuracy\": 1.0, \"loss\": 0.0005435454659163952, \"time-step\": 1820}, {\"accuracy\": 1.0, \"loss\": 0.0005335688474588096, \"time-step\": 1821}, {\"accuracy\": 1.0, \"loss\": 0.0005427544238045812, \"time-step\": 1822}, {\"accuracy\": 1.0, \"loss\": 0.0005327804246917367, \"time-step\": 1823}, {\"accuracy\": 1.0, \"loss\": 0.000541966175660491, \"time-step\": 1824}, {\"accuracy\": 1.0, \"loss\": 0.0005320045165717602, \"time-step\": 1825}, {\"accuracy\": 1.0, \"loss\": 0.0005411873571574688, \"time-step\": 1826}, {\"accuracy\": 1.0, \"loss\": 0.0005312306457199156, \"time-step\": 1827}, {\"accuracy\": 1.0, \"loss\": 0.0005404003313742578, \"time-step\": 1828}, {\"accuracy\": 1.0, \"loss\": 0.0005304523510858417, \"time-step\": 1829}, {\"accuracy\": 1.0, \"loss\": 0.0005396330961957574, \"time-step\": 1830}, {\"accuracy\": 1.0, \"loss\": 0.0005296867457218468, \"time-step\": 1831}, {\"accuracy\": 1.0, \"loss\": 0.0005388536956161261, \"time-step\": 1832}, {\"accuracy\": 1.0, \"loss\": 0.0005289138061925769, \"time-step\": 1833}, {\"accuracy\": 1.0, \"loss\": 0.0005380782531574368, \"time-step\": 1834}, {\"accuracy\": 1.0, \"loss\": 0.0005281453486531973, \"time-step\": 1835}, {\"accuracy\": 1.0, \"loss\": 0.0005373063031584024, \"time-step\": 1836}, {\"accuracy\": 1.0, \"loss\": 0.0005273916176520288, \"time-step\": 1837}, {\"accuracy\": 1.0, \"loss\": 0.0005365577526390553, \"time-step\": 1838}, {\"accuracy\": 1.0, \"loss\": 0.00052664551185444, \"time-step\": 1839}, {\"accuracy\": 1.0, \"loss\": 0.0005357955815270543, \"time-step\": 1840}, {\"accuracy\": 1.0, \"loss\": 0.0005258884630165994, \"time-step\": 1841}, {\"accuracy\": 1.0, \"loss\": 0.0005350352730602026, \"time-step\": 1842}, {\"accuracy\": 1.0, \"loss\": 0.0005251339171081781, \"time-step\": 1843}, {\"accuracy\": 1.0, \"loss\": 0.0005342697259038687, \"time-step\": 1844}, {\"accuracy\": 1.0, \"loss\": 0.000524368544574827, \"time-step\": 1845}, {\"accuracy\": 1.0, \"loss\": 0.000533512094989419, \"time-step\": 1846}, {\"accuracy\": 1.0, \"loss\": 0.0005236390861682594, \"time-step\": 1847}, {\"accuracy\": 1.0, \"loss\": 0.0005327662802301347, \"time-step\": 1848}, {\"accuracy\": 1.0, \"loss\": 0.0005228829686529934, \"time-step\": 1849}, {\"accuracy\": 1.0, \"loss\": 0.0005320084746927023, \"time-step\": 1850}, {\"accuracy\": 1.0, \"loss\": 0.0005221303435973823, \"time-step\": 1851}, {\"accuracy\": 1.0, \"loss\": 0.0005312527064234018, \"time-step\": 1852}, {\"accuracy\": 1.0, \"loss\": 0.0005213808617554605, \"time-step\": 1853}, {\"accuracy\": 1.0, \"loss\": 0.000530489778611809, \"time-step\": 1854}, {\"accuracy\": 1.0, \"loss\": 0.0005206329515203834, \"time-step\": 1855}, {\"accuracy\": 1.0, \"loss\": 0.0005297501338645816, \"time-step\": 1856}, {\"accuracy\": 1.0, \"loss\": 0.0005198990693315864, \"time-step\": 1857}, {\"accuracy\": 1.0, \"loss\": 0.0005290096160024405, \"time-step\": 1858}, {\"accuracy\": 1.0, \"loss\": 0.0005191664677113295, \"time-step\": 1859}, {\"accuracy\": 1.0, \"loss\": 0.0005282645579427481, \"time-step\": 1860}, {\"accuracy\": 1.0, \"loss\": 0.0005184340989217162, \"time-step\": 1861}, {\"accuracy\": 1.0, \"loss\": 0.0005275349249131978, \"time-step\": 1862}, {\"accuracy\": 1.0, \"loss\": 0.000517703709192574, \"time-step\": 1863}, {\"accuracy\": 1.0, \"loss\": 0.0005267808446660638, \"time-step\": 1864}, {\"accuracy\": 1.0, \"loss\": 0.0005169516080059111, \"time-step\": 1865}, {\"accuracy\": 1.0, \"loss\": 0.00052604143274948, \"time-step\": 1866}, {\"accuracy\": 1.0, \"loss\": 0.0005162281449884176, \"time-step\": 1867}, {\"accuracy\": 1.0, \"loss\": 0.0005253193667158484, \"time-step\": 1868}, {\"accuracy\": 1.0, \"loss\": 0.0005155164981260896, \"time-step\": 1869}, {\"accuracy\": 1.0, \"loss\": 0.0005245804786682129, \"time-step\": 1870}, {\"accuracy\": 1.0, \"loss\": 0.0005147717311047018, \"time-step\": 1871}, {\"accuracy\": 1.0, \"loss\": 0.0005238374578766525, \"time-step\": 1872}, {\"accuracy\": 1.0, \"loss\": 0.0005140509456396103, \"time-step\": 1873}, {\"accuracy\": 1.0, \"loss\": 0.0005231241229921579, \"time-step\": 1874}, {\"accuracy\": 1.0, \"loss\": 0.0005133413360454142, \"time-step\": 1875}, {\"accuracy\": 1.0, \"loss\": 0.0005224095657467842, \"time-step\": 1876}, {\"accuracy\": 1.0, \"loss\": 0.0005126363248564303, \"time-step\": 1877}, {\"accuracy\": 1.0, \"loss\": 0.0005216924473643303, \"time-step\": 1878}, {\"accuracy\": 1.0, \"loss\": 0.0005119145498611033, \"time-step\": 1879}, {\"accuracy\": 1.0, \"loss\": 0.0005209596129134297, \"time-step\": 1880}, {\"accuracy\": 1.0, \"loss\": 0.0005112039507366717, \"time-step\": 1881}, {\"accuracy\": 1.0, \"loss\": 0.0005202429601922631, \"time-step\": 1882}, {\"accuracy\": 1.0, \"loss\": 0.0005104849115014076, \"time-step\": 1883}, {\"accuracy\": 1.0, \"loss\": 0.0005195292760618031, \"time-step\": 1884}, {\"accuracy\": 1.0, \"loss\": 0.0005097839748486876, \"time-step\": 1885}, {\"accuracy\": 1.0, \"loss\": 0.000518814311362803, \"time-step\": 1886}, {\"accuracy\": 1.0, \"loss\": 0.0005090730264782906, \"time-step\": 1887}, {\"accuracy\": 1.0, \"loss\": 0.0005180988810025156, \"time-step\": 1888}, {\"accuracy\": 1.0, \"loss\": 0.0005083641735836864, \"time-step\": 1889}, {\"accuracy\": 1.0, \"loss\": 0.0005173817044124007, \"time-step\": 1890}, {\"accuracy\": 1.0, \"loss\": 0.000507658114656806, \"time-step\": 1891}, {\"accuracy\": 1.0, \"loss\": 0.0005166759947314858, \"time-step\": 1892}, {\"accuracy\": 1.0, \"loss\": 0.0005069662001915276, \"time-step\": 1893}, {\"accuracy\": 1.0, \"loss\": 0.000515977677423507, \"time-step\": 1894}, {\"accuracy\": 1.0, \"loss\": 0.0005062728305347264, \"time-step\": 1895}, {\"accuracy\": 1.0, \"loss\": 0.0005152829107828438, \"time-step\": 1896}, {\"accuracy\": 1.0, \"loss\": 0.0005055843503214419, \"time-step\": 1897}, {\"accuracy\": 1.0, \"loss\": 0.0005145906470716, \"time-step\": 1898}, {\"accuracy\": 1.0, \"loss\": 0.0005048976745456457, \"time-step\": 1899}, {\"accuracy\": 1.0, \"loss\": 0.0005139022250659764, \"time-step\": 1900}, {\"accuracy\": 1.0, \"loss\": 0.0005042130360379815, \"time-step\": 1901}, {\"accuracy\": 1.0, \"loss\": 0.0005131982616148889, \"time-step\": 1902}, {\"accuracy\": 1.0, \"loss\": 0.0005035154172219336, \"time-step\": 1903}, {\"accuracy\": 1.0, \"loss\": 0.0005124923773109913, \"time-step\": 1904}, {\"accuracy\": 1.0, \"loss\": 0.0005028218729421496, \"time-step\": 1905}, {\"accuracy\": 1.0, \"loss\": 0.0005118015105836093, \"time-step\": 1906}, {\"accuracy\": 1.0, \"loss\": 0.0005021399119868875, \"time-step\": 1907}, {\"accuracy\": 1.0, \"loss\": 0.0005111157661303878, \"time-step\": 1908}, {\"accuracy\": 1.0, \"loss\": 0.0005014583584852517, \"time-step\": 1909}, {\"accuracy\": 1.0, \"loss\": 0.0005104236188344657, \"time-step\": 1910}, {\"accuracy\": 1.0, \"loss\": 0.0005007641157135367, \"time-step\": 1911}, {\"accuracy\": 1.0, \"loss\": 0.0005097169196233153, \"time-step\": 1912}, {\"accuracy\": 1.0, \"loss\": 0.0005000712117180228, \"time-step\": 1913}, {\"accuracy\": 1.0, \"loss\": 0.000509026343934238, \"time-step\": 1914}, {\"accuracy\": 1.0, \"loss\": 0.0004993938491679728, \"time-step\": 1915}, {\"accuracy\": 1.0, \"loss\": 0.0005083403084427118, \"time-step\": 1916}, {\"accuracy\": 1.0, \"loss\": 0.000498716370202601, \"time-step\": 1917}, {\"accuracy\": 1.0, \"loss\": 0.000507658114656806, \"time-step\": 1918}, {\"accuracy\": 1.0, \"loss\": 0.0004980365629307926, \"time-step\": 1919}, {\"accuracy\": 1.0, \"loss\": 0.0005069675389677286, \"time-step\": 1920}, {\"accuracy\": 1.0, \"loss\": 0.0004973600734956563, \"time-step\": 1921}, {\"accuracy\": 1.0, \"loss\": 0.0005062965210527182, \"time-step\": 1922}, {\"accuracy\": 1.0, \"loss\": 0.0004966930137015879, \"time-step\": 1923}, {\"accuracy\": 1.0, \"loss\": 0.0005056245718151331, \"time-step\": 1924}, {\"accuracy\": 1.0, \"loss\": 0.0004960338119417429, \"time-step\": 1925}, {\"accuracy\": 1.0, \"loss\": 0.0005049581523053348, \"time-step\": 1926}, {\"accuracy\": 1.0, \"loss\": 0.0004953715251758695, \"time-step\": 1927}, {\"accuracy\": 1.0, \"loss\": 0.0005042831762693822, \"time-step\": 1928}, {\"accuracy\": 1.0, \"loss\": 0.0004947076085954905, \"time-step\": 1929}, {\"accuracy\": 1.0, \"loss\": 0.0005036277580074966, \"time-step\": 1930}, {\"accuracy\": 1.0, \"loss\": 0.0004940544604323804, \"time-step\": 1931}, {\"accuracy\": 1.0, \"loss\": 0.0005029567400924861, \"time-step\": 1932}, {\"accuracy\": 1.0, \"loss\": 0.0004933940945193172, \"time-step\": 1933}, {\"accuracy\": 1.0, \"loss\": 0.0005022968980483711, \"time-step\": 1934}, {\"accuracy\": 1.0, \"loss\": 0.0004927402478642762, \"time-step\": 1935}, {\"accuracy\": 1.0, \"loss\": 0.0005016361246816814, \"time-step\": 1936}, {\"accuracy\": 1.0, \"loss\": 0.000492087216116488, \"time-step\": 1937}, {\"accuracy\": 1.0, \"loss\": 0.0005009709857404232, \"time-step\": 1938}, {\"accuracy\": 1.0, \"loss\": 0.0004914269084110856, \"time-step\": 1939}, {\"accuracy\": 1.0, \"loss\": 0.0005003099795430899, \"time-step\": 1940}, {\"accuracy\": 1.0, \"loss\": 0.0004907762631773949, \"time-step\": 1941}, {\"accuracy\": 1.0, \"loss\": 0.0004996525240130723, \"time-step\": 1942}, {\"accuracy\": 1.0, \"loss\": 0.0004901321954093874, \"time-step\": 1943}, {\"accuracy\": 1.0, \"loss\": 0.0004990048473700881, \"time-step\": 1944}, {\"accuracy\": 1.0, \"loss\": 0.0004894909798167646, \"time-step\": 1945}, {\"accuracy\": 1.0, \"loss\": 0.0004983639810234308, \"time-step\": 1946}, {\"accuracy\": 1.0, \"loss\": 0.0004888498806394637, \"time-step\": 1947}, {\"accuracy\": 1.0, \"loss\": 0.0004977039061486721, \"time-step\": 1948}, {\"accuracy\": 1.0, \"loss\": 0.0004881993809249252, \"time-step\": 1949}, {\"accuracy\": 1.0, \"loss\": 0.0004970489535480738, \"time-step\": 1950}, {\"accuracy\": 1.0, \"loss\": 0.0004875512095168233, \"time-step\": 1951}, {\"accuracy\": 1.0, \"loss\": 0.0004963951068930328, \"time-step\": 1952}, {\"accuracy\": 1.0, \"loss\": 0.0004868938121944666, \"time-step\": 1953}, {\"accuracy\": 1.0, \"loss\": 0.0004957224591635168, \"time-step\": 1954}, {\"accuracy\": 1.0, \"loss\": 0.0004862425848841667, \"time-step\": 1955}, {\"accuracy\": 1.0, \"loss\": 0.0004950795555487275, \"time-step\": 1956}, {\"accuracy\": 1.0, \"loss\": 0.0004856161249335855, \"time-step\": 1957}, {\"accuracy\": 1.0, \"loss\": 0.0004944527754560113, \"time-step\": 1958}, {\"accuracy\": 1.0, \"loss\": 0.0004849823599215597, \"time-step\": 1959}, {\"accuracy\": 1.0, \"loss\": 0.0004937922349199653, \"time-step\": 1960}, {\"accuracy\": 1.0, \"loss\": 0.0004843342467211187, \"time-step\": 1961}, {\"accuracy\": 1.0, \"loss\": 0.0004931645235046744, \"time-step\": 1962}, {\"accuracy\": 1.0, \"loss\": 0.00048371960292570293, \"time-step\": 1963}, {\"accuracy\": 1.0, \"loss\": 0.0004925341927446425, \"time-step\": 1964}, {\"accuracy\": 1.0, \"loss\": 0.00048309899284504354, \"time-step\": 1965}, {\"accuracy\": 1.0, \"loss\": 0.0004919028142467141, \"time-step\": 1966}, {\"accuracy\": 1.0, \"loss\": 0.0004824661882594228, \"time-step\": 1967}, {\"accuracy\": 1.0, \"loss\": 0.0004912680597044528, \"time-step\": 1968}, {\"accuracy\": 1.0, \"loss\": 0.000481839117128402, \"time-step\": 1969}, {\"accuracy\": 1.0, \"loss\": 0.000490629521664232, \"time-step\": 1970}, {\"accuracy\": 1.0, \"loss\": 0.00048120616702362895, \"time-step\": 1971}, {\"accuracy\": 1.0, \"loss\": 0.0004899944178760052, \"time-step\": 1972}, {\"accuracy\": 1.0, \"loss\": 0.0004805814241990447, \"time-step\": 1973}, {\"accuracy\": 1.0, \"loss\": 0.0004893667064607143, \"time-step\": 1974}, {\"accuracy\": 1.0, \"loss\": 0.0004799728048965335, \"time-step\": 1975}, {\"accuracy\": 1.0, \"loss\": 0.0004887485411018133, \"time-step\": 1976}, {\"accuracy\": 1.0, \"loss\": 0.0004793524567503482, \"time-step\": 1977}, {\"accuracy\": 1.0, \"loss\": 0.0004881152417510748, \"time-step\": 1978}, {\"accuracy\": 1.0, \"loss\": 0.0004787264042533934, \"time-step\": 1979}, {\"accuracy\": 1.0, \"loss\": 0.0004874940204899758, \"time-step\": 1980}, {\"accuracy\": 1.0, \"loss\": 0.0004781134193763137, \"time-step\": 1981}, {\"accuracy\": 1.0, \"loss\": 0.00048686869558878243, \"time-step\": 1982}, {\"accuracy\": 1.0, \"loss\": 0.0004775000852532685, \"time-step\": 1983}, {\"accuracy\": 1.0, \"loss\": 0.0004862625210080296, \"time-step\": 1984}, {\"accuracy\": 1.0, \"loss\": 0.0004768921062350273, \"time-step\": 1985}, {\"accuracy\": 1.0, \"loss\": 0.00048563408199697733, \"time-step\": 1986}, {\"accuracy\": 1.0, \"loss\": 0.0004762830794788897, \"time-step\": 1987}, {\"accuracy\": 1.0, \"loss\": 0.0004850234545301646, \"time-step\": 1988}, {\"accuracy\": 1.0, \"loss\": 0.0004756774287670851, \"time-step\": 1989}, {\"accuracy\": 1.0, \"loss\": 0.0004844097711611539, \"time-step\": 1990}, {\"accuracy\": 1.0, \"loss\": 0.0004750711959786713, \"time-step\": 1991}, {\"accuracy\": 1.0, \"loss\": 0.00048380077350884676, \"time-step\": 1992}, {\"accuracy\": 1.0, \"loss\": 0.00047447357792407274, \"time-step\": 1993}, {\"accuracy\": 1.0, \"loss\": 0.0004831978294532746, \"time-step\": 1994}, {\"accuracy\": 1.0, \"loss\": 0.0004738791030831635, \"time-step\": 1995}, {\"accuracy\": 1.0, \"loss\": 0.0004826050717383623, \"time-step\": 1996}, {\"accuracy\": 1.0, \"loss\": 0.00047328986693173647, \"time-step\": 1997}, {\"accuracy\": 1.0, \"loss\": 0.0004819981986656785, \"time-step\": 1998}, {\"accuracy\": 1.0, \"loss\": 0.0004726819461211562, \"time-step\": 1999}, {\"accuracy\": 1.0, \"loss\": 0.00048138367128558457, \"time-step\": 2000}, {\"accuracy\": 1.0, \"loss\": 0.00047208709293045104, \"time-step\": 2001}, {\"accuracy\": 1.0, \"loss\": 0.000480787392007187, \"time-step\": 2002}, {\"accuracy\": 1.0, \"loss\": 0.00047149602323770523, \"time-step\": 2003}, {\"accuracy\": 1.0, \"loss\": 0.00048018639790825546, \"time-step\": 2004}, {\"accuracy\": 1.0, \"loss\": 0.0004708998603746295, \"time-step\": 2005}, {\"accuracy\": 1.0, \"loss\": 0.00047958845971152186, \"time-step\": 2006}, {\"accuracy\": 1.0, \"loss\": 0.0004703157756011933, \"time-step\": 2007}, {\"accuracy\": 1.0, \"loss\": 0.00047899805940687656, \"time-step\": 2008}, {\"accuracy\": 1.0, \"loss\": 0.00046972488053143024, \"time-step\": 2009}, {\"accuracy\": 1.0, \"loss\": 0.0004784019256476313, \"time-step\": 2010}, {\"accuracy\": 1.0, \"loss\": 0.00046913951518945396, \"time-step\": 2011}, {\"accuracy\": 1.0, \"loss\": 0.00047780806198716164, \"time-step\": 2012}, {\"accuracy\": 1.0, \"loss\": 0.00046854838728904724, \"time-step\": 2013}, {\"accuracy\": 1.0, \"loss\": 0.00047721737064421177, \"time-step\": 2014}, {\"accuracy\": 1.0, \"loss\": 0.000467972073238343, \"time-step\": 2015}, {\"accuracy\": 1.0, \"loss\": 0.00047662697033956647, \"time-step\": 2016}, {\"accuracy\": 1.0, \"loss\": 0.0004673827497754246, \"time-step\": 2017}, {\"accuracy\": 1.0, \"loss\": 0.00047603860730305314, \"time-step\": 2018}, {\"accuracy\": 1.0, \"loss\": 0.00046680771629326046, \"time-step\": 2019}, {\"accuracy\": 1.0, \"loss\": 0.000475455482956022, \"time-step\": 2020}, {\"accuracy\": 1.0, \"loss\": 0.0004662266292143613, \"time-step\": 2021}, {\"accuracy\": 1.0, \"loss\": 0.00047487166011705995, \"time-step\": 2022}, {\"accuracy\": 1.0, \"loss\": 0.00046566000673919916, \"time-step\": 2023}, {\"accuracy\": 1.0, \"loss\": 0.00047429592814296484, \"time-step\": 2024}, {\"accuracy\": 1.0, \"loss\": 0.000465080956928432, \"time-step\": 2025}, {\"accuracy\": 1.0, \"loss\": 0.00047370861284434795, \"time-step\": 2026}, {\"accuracy\": 1.0, \"loss\": 0.00046449893852695823, \"time-step\": 2027}, {\"accuracy\": 1.0, \"loss\": 0.0004731126828119159, \"time-step\": 2028}, {\"accuracy\": 1.0, \"loss\": 0.00046392061631195247, \"time-step\": 2029}, {\"accuracy\": 1.0, \"loss\": 0.0004725226608570665, \"time-step\": 2030}, {\"accuracy\": 1.0, \"loss\": 0.0004633319331333041, \"time-step\": 2031}, {\"accuracy\": 1.0, \"loss\": 0.0004719363059848547, \"time-step\": 2032}, {\"accuracy\": 1.0, \"loss\": 0.0004627661546692252, \"time-step\": 2033}, {\"accuracy\": 1.0, \"loss\": 0.0004713735543191433, \"time-step\": 2034}, {\"accuracy\": 1.0, \"loss\": 0.00046219932846724987, \"time-step\": 2035}, {\"accuracy\": 1.0, \"loss\": 0.0004707982880063355, \"time-step\": 2036}, {\"accuracy\": 1.0, \"loss\": 0.0004616371588781476, \"time-step\": 2037}, {\"accuracy\": 1.0, \"loss\": 0.0004702251171693206, \"time-step\": 2038}, {\"accuracy\": 1.0, \"loss\": 0.0004610657342709601, \"time-step\": 2039}, {\"accuracy\": 1.0, \"loss\": 0.0004696535470429808, \"time-step\": 2040}, {\"accuracy\": 1.0, \"loss\": 0.0004605082795023918, \"time-step\": 2041}, {\"accuracy\": 1.0, \"loss\": 0.0004690899222623557, \"time-step\": 2042}, {\"accuracy\": 1.0, \"loss\": 0.00045994744868949056, \"time-step\": 2043}, {\"accuracy\": 1.0, \"loss\": 0.0004685186722781509, \"time-step\": 2044}, {\"accuracy\": 1.0, \"loss\": 0.0004593844059854746, \"time-step\": 2045}, {\"accuracy\": 1.0, \"loss\": 0.0004679573467001319, \"time-step\": 2046}, {\"accuracy\": 1.0, \"loss\": 0.0004588320152834058, \"time-step\": 2047}, {\"accuracy\": 1.0, \"loss\": 0.0004673891235142946, \"time-step\": 2048}, {\"accuracy\": 1.0, \"loss\": 0.00045826693531125784, \"time-step\": 2049}, {\"accuracy\": 1.0, \"loss\": 0.00046681755338795483, \"time-step\": 2050}, {\"accuracy\": 1.0, \"loss\": 0.0004577082290779799, \"time-step\": 2051}, {\"accuracy\": 1.0, \"loss\": 0.00046626306721009314, \"time-step\": 2052}, {\"accuracy\": 1.0, \"loss\": 0.00045716625754721463, \"time-step\": 2053}, {\"accuracy\": 1.0, \"loss\": 0.0004657164972741157, \"time-step\": 2054}, {\"accuracy\": 1.0, \"loss\": 0.00045661936746910214, \"time-step\": 2055}, {\"accuracy\": 1.0, \"loss\": 0.0004651548806577921, \"time-step\": 2056}, {\"accuracy\": 1.0, \"loss\": 0.0004560595261864364, \"time-step\": 2057}, {\"accuracy\": 1.0, \"loss\": 0.0004645833687391132, \"time-step\": 2058}, {\"accuracy\": 1.0, \"loss\": 0.00045550178037956357, \"time-step\": 2059}, {\"accuracy\": 1.0, \"loss\": 0.00046402894076891243, \"time-step\": 2060}, {\"accuracy\": 1.0, \"loss\": 0.0004549538134597242, \"time-step\": 2061}, {\"accuracy\": 1.0, \"loss\": 0.0004634732031263411, \"time-step\": 2062}, {\"accuracy\": 1.0, \"loss\": 0.000454411085229367, \"time-step\": 2063}, {\"accuracy\": 1.0, \"loss\": 0.00046292314073070884, \"time-step\": 2064}, {\"accuracy\": 1.0, \"loss\": 0.00045386620331555605, \"time-step\": 2065}, {\"accuracy\": 1.0, \"loss\": 0.00046236871276050806, \"time-step\": 2066}, {\"accuracy\": 1.0, \"loss\": 0.0004533162573352456, \"time-step\": 2067}, {\"accuracy\": 1.0, \"loss\": 0.00046181888319551945, \"time-step\": 2068}, {\"accuracy\": 1.0, \"loss\": 0.00045277332537807524, \"time-step\": 2069}, {\"accuracy\": 1.0, \"loss\": 0.0004612691409420222, \"time-step\": 2070}, {\"accuracy\": 1.0, \"loss\": 0.00045223310007713735, \"time-step\": 2071}, {\"accuracy\": 1.0, \"loss\": 0.0004607300506904721, \"time-step\": 2072}, {\"accuracy\": 1.0, \"loss\": 0.00045171246165409684, \"time-step\": 2073}, {\"accuracy\": 1.0, \"loss\": 0.00046019998262636364, \"time-step\": 2074}, {\"accuracy\": 1.0, \"loss\": 0.0004511777078732848, \"time-step\": 2075}, {\"accuracy\": 1.0, \"loss\": 0.00045965355820953846, \"time-step\": 2076}, {\"accuracy\": 1.0, \"loss\": 0.0004506407130975276, \"time-step\": 2077}, {\"accuracy\": 1.0, \"loss\": 0.00045910855988040566, \"time-step\": 2078}, {\"accuracy\": 1.0, \"loss\": 0.000450101972091943, \"time-step\": 2079}, {\"accuracy\": 1.0, \"loss\": 0.0004585832648444921, \"time-step\": 2080}, {\"accuracy\": 1.0, \"loss\": 0.0004495880857575685, \"time-step\": 2081}, {\"accuracy\": 1.0, \"loss\": 0.00045804621186107397, \"time-step\": 2082}, {\"accuracy\": 1.0, \"loss\": 0.00044905015965923667, \"time-step\": 2083}, {\"accuracy\": 1.0, \"loss\": 0.0004575063940137625, \"time-step\": 2084}, {\"accuracy\": 1.0, \"loss\": 0.00044852899736724794, \"time-step\": 2085}, {\"accuracy\": 1.0, \"loss\": 0.0004569795273710042, \"time-step\": 2086}, {\"accuracy\": 1.0, \"loss\": 0.00044801324838772416, \"time-step\": 2087}, {\"accuracy\": 1.0, \"loss\": 0.0004564604605548084, \"time-step\": 2088}, {\"accuracy\": 1.0, \"loss\": 0.00044748798245564103, \"time-step\": 2089}, {\"accuracy\": 1.0, \"loss\": 0.000455917528597638, \"time-step\": 2090}, {\"accuracy\": 1.0, \"loss\": 0.0004469479317776859, \"time-step\": 2091}, {\"accuracy\": 1.0, \"loss\": 0.00045537756523117423, \"time-step\": 2092}, {\"accuracy\": 1.0, \"loss\": 0.0004464322701096535, \"time-step\": 2093}, {\"accuracy\": 1.0, \"loss\": 0.00045486350427381694, \"time-step\": 2094}, {\"accuracy\": 1.0, \"loss\": 0.00044592644553631544, \"time-step\": 2095}, {\"accuracy\": 1.0, \"loss\": 0.00045434734784066677, \"time-step\": 2096}, {\"accuracy\": 1.0, \"loss\": 0.0004454060399439186, \"time-step\": 2097}, {\"accuracy\": 1.0, \"loss\": 0.00045382091775536537, \"time-step\": 2098}, {\"accuracy\": 1.0, \"loss\": 0.0004448882828000933, \"time-step\": 2099}, {\"accuracy\": 1.0, \"loss\": 0.00045329489512369037, \"time-step\": 2100}, {\"accuracy\": 1.0, \"loss\": 0.0004443658690433949, \"time-step\": 2101}, {\"accuracy\": 1.0, \"loss\": 0.0004527665441855788, \"time-step\": 2102}, {\"accuracy\": 1.0, \"loss\": 0.00044385064393281937, \"time-step\": 2103}, {\"accuracy\": 1.0, \"loss\": 0.0004522507661022246, \"time-step\": 2104}, {\"accuracy\": 1.0, \"loss\": 0.00044334025005809963, \"time-step\": 2105}, {\"accuracy\": 1.0, \"loss\": 0.000451734202215448, \"time-step\": 2106}, {\"accuracy\": 1.0, \"loss\": 0.00044283168972469866, \"time-step\": 2107}, {\"accuracy\": 1.0, \"loss\": 0.0004512224695645273, \"time-step\": 2108}, {\"accuracy\": 1.0, \"loss\": 0.0004423402715474367, \"time-step\": 2109}, {\"accuracy\": 1.0, \"loss\": 0.00045072208740748465, \"time-step\": 2110}, {\"accuracy\": 1.0, \"loss\": 0.00044183628051541746, \"time-step\": 2111}, {\"accuracy\": 1.0, \"loss\": 0.00045021259575150907, \"time-step\": 2112}, {\"accuracy\": 1.0, \"loss\": 0.0004413315618876368, \"time-step\": 2113}, {\"accuracy\": 1.0, \"loss\": 0.00044968511792831123, \"time-step\": 2114}, {\"accuracy\": 1.0, \"loss\": 0.0004408090899232775, \"time-step\": 2115}, {\"accuracy\": 1.0, \"loss\": 0.0004491812433116138, \"time-step\": 2116}, {\"accuracy\": 1.0, \"loss\": 0.00044032465666532516, \"time-step\": 2117}, {\"accuracy\": 1.0, \"loss\": 0.00044868228724226356, \"time-step\": 2118}, {\"accuracy\": 1.0, \"loss\": 0.00043982479837723076, \"time-step\": 2119}, {\"accuracy\": 1.0, \"loss\": 0.0004481688083615154, \"time-step\": 2120}, {\"accuracy\": 1.0, \"loss\": 0.0004393131530378014, \"time-step\": 2121}, {\"accuracy\": 1.0, \"loss\": 0.000447657163022086, \"time-step\": 2122}, {\"accuracy\": 1.0, \"loss\": 0.0004388121305964887, \"time-step\": 2123}, {\"accuracy\": 1.0, \"loss\": 0.00044715069816447794, \"time-step\": 2124}, {\"accuracy\": 1.0, \"loss\": 0.0004383145715110004, \"time-step\": 2125}, {\"accuracy\": 1.0, \"loss\": 0.0004466386162675917, \"time-step\": 2126}, {\"accuracy\": 1.0, \"loss\": 0.0004377966106403619, \"time-step\": 2127}, {\"accuracy\": 1.0, \"loss\": 0.0004461208882275969, \"time-step\": 2128}, {\"accuracy\": 1.0, \"loss\": 0.0004373042902443558, \"time-step\": 2129}, {\"accuracy\": 1.0, \"loss\": 0.000445628073066473, \"time-step\": 2130}, {\"accuracy\": 1.0, \"loss\": 0.0004368049558252096, \"time-step\": 2131}, {\"accuracy\": 1.0, \"loss\": 0.00044511837768368423, \"time-step\": 2132}, {\"accuracy\": 1.0, \"loss\": 0.00043630896834656596, \"time-step\": 2133}, {\"accuracy\": 1.0, \"loss\": 0.0004446139500942081, \"time-step\": 2134}, {\"accuracy\": 1.0, \"loss\": 0.00043581181671470404, \"time-step\": 2135}, {\"accuracy\": 1.0, \"loss\": 0.0004441222990863025, \"time-step\": 2136}, {\"accuracy\": 1.0, \"loss\": 0.0004353370168246329, \"time-step\": 2137}, {\"accuracy\": 1.0, \"loss\": 0.0004436410963535309, \"time-step\": 2138}, {\"accuracy\": 1.0, \"loss\": 0.000434854970080778, \"time-step\": 2139}, {\"accuracy\": 1.0, \"loss\": 0.00044314312981441617, \"time-step\": 2140}, {\"accuracy\": 1.0, \"loss\": 0.00043435985571704805, \"time-step\": 2141}, {\"accuracy\": 1.0, \"loss\": 0.0004426504601724446, \"time-step\": 2142}, {\"accuracy\": 1.0, \"loss\": 0.00043388514313846827, \"time-step\": 2143}, {\"accuracy\": 1.0, \"loss\": 0.00044216870446689427, \"time-step\": 2144}, {\"accuracy\": 1.0, \"loss\": 0.00043340481352061033, \"time-step\": 2145}, {\"accuracy\": 1.0, \"loss\": 0.0004416767042130232, \"time-step\": 2146}, {\"accuracy\": 1.0, \"loss\": 0.00043291901238262653, \"time-step\": 2147}, {\"accuracy\": 1.0, \"loss\": 0.0004411874688230455, \"time-step\": 2148}, {\"accuracy\": 1.0, \"loss\": 0.000432428001658991, \"time-step\": 2149}, {\"accuracy\": 1.0, \"loss\": 0.00044068993884138763, \"time-step\": 2150}, {\"accuracy\": 1.0, \"loss\": 0.0004319424042478204, \"time-step\": 2151}, {\"accuracy\": 1.0, \"loss\": 0.00044019767665304244, \"time-step\": 2152}, {\"accuracy\": 1.0, \"loss\": 0.0004314645193517208, \"time-step\": 2153}, {\"accuracy\": 1.0, \"loss\": 0.00043972034472972155, \"time-step\": 2154}, {\"accuracy\": 1.0, \"loss\": 0.00043098817695863545, \"time-step\": 2155}, {\"accuracy\": 1.0, \"loss\": 0.00043923986959271133, \"time-step\": 2156}, {\"accuracy\": 1.0, \"loss\": 0.00043051844113506377, \"time-step\": 2157}, {\"accuracy\": 1.0, \"loss\": 0.00043875290430150926, \"time-step\": 2158}, {\"accuracy\": 1.0, \"loss\": 0.00043003304745070636, \"time-step\": 2159}, {\"accuracy\": 1.0, \"loss\": 0.0004382639308460057, \"time-step\": 2160}, {\"accuracy\": 1.0, \"loss\": 0.00042954969103448093, \"time-step\": 2161}, {\"accuracy\": 1.0, \"loss\": 0.0004377757431939244, \"time-step\": 2162}, {\"accuracy\": 1.0, \"loss\": 0.00042907174793072045, \"time-step\": 2163}, {\"accuracy\": 1.0, \"loss\": 0.0004372888943180442, \"time-step\": 2164}, {\"accuracy\": 1.0, \"loss\": 0.0004285931936465204, \"time-step\": 2165}, {\"accuracy\": 1.0, \"loss\": 0.00043680728413164616, \"time-step\": 2166}, {\"accuracy\": 1.0, \"loss\": 0.00042811527964659035, \"time-step\": 2167}, {\"accuracy\": 1.0, \"loss\": 0.0004363226471468806, \"time-step\": 2168}, {\"accuracy\": 1.0, \"loss\": 0.00042763882083818316, \"time-step\": 2169}, {\"accuracy\": 1.0, \"loss\": 0.0004358448786661029, \"time-step\": 2170}, {\"accuracy\": 1.0, \"loss\": 0.0004271663201507181, \"time-step\": 2171}, {\"accuracy\": 1.0, \"loss\": 0.00043536050361581147, \"time-step\": 2172}, {\"accuracy\": 1.0, \"loss\": 0.0004267039184924215, \"time-step\": 2173}, {\"accuracy\": 1.0, \"loss\": 0.00043490860844030976, \"time-step\": 2174}, {\"accuracy\": 1.0, \"loss\": 0.0004262540314812213, \"time-step\": 2175}, {\"accuracy\": 1.0, \"loss\": 0.0004344485350884497, \"time-step\": 2176}, {\"accuracy\": 1.0, \"loss\": 0.0004257969558238983, \"time-step\": 2177}, {\"accuracy\": 1.0, \"loss\": 0.00043397629633545876, \"time-step\": 2178}, {\"accuracy\": 1.0, \"loss\": 0.00042533804662525654, \"time-step\": 2179}, {\"accuracy\": 1.0, \"loss\": 0.00043351767817512155, \"time-step\": 2180}, {\"accuracy\": 1.0, \"loss\": 0.00042487678001634777, \"time-step\": 2181}, {\"accuracy\": 1.0, \"loss\": 0.0004330500087235123, \"time-step\": 2182}, {\"accuracy\": 1.0, \"loss\": 0.00042441816185601056, \"time-step\": 2183}, {\"accuracy\": 1.0, \"loss\": 0.0004325825721025467, \"time-step\": 2184}, {\"accuracy\": 1.0, \"loss\": 0.0004239638801664114, \"time-step\": 2185}, {\"accuracy\": 1.0, \"loss\": 0.0004321265150792897, \"time-step\": 2186}, {\"accuracy\": 1.0, \"loss\": 0.00042351154843345284, \"time-step\": 2187}, {\"accuracy\": 1.0, \"loss\": 0.00043166300747543573, \"time-step\": 2188}, {\"accuracy\": 1.0, \"loss\": 0.00042304745875298977, \"time-step\": 2189}, {\"accuracy\": 1.0, \"loss\": 0.0004311999073252082, \"time-step\": 2190}, {\"accuracy\": 1.0, \"loss\": 0.0004225934389978647, \"time-step\": 2191}, {\"accuracy\": 1.0, \"loss\": 0.0004307354975026101, \"time-step\": 2192}, {\"accuracy\": 1.0, \"loss\": 0.00042213679989799857, \"time-step\": 2193}, {\"accuracy\": 1.0, \"loss\": 0.00043027778156101704, \"time-step\": 2194}, {\"accuracy\": 1.0, \"loss\": 0.00042169162770733237, \"time-step\": 2195}, {\"accuracy\": 1.0, \"loss\": 0.00042982149170711637, \"time-step\": 2196}, {\"accuracy\": 1.0, \"loss\": 0.0004212325147818774, \"time-step\": 2197}, {\"accuracy\": 1.0, \"loss\": 0.00042935137753374875, \"time-step\": 2198}, {\"accuracy\": 1.0, \"loss\": 0.00042077628313563764, \"time-step\": 2199}, {\"accuracy\": 1.0, \"loss\": 0.00042890029726549983, \"time-step\": 2200}, {\"accuracy\": 1.0, \"loss\": 0.00042033317731693387, \"time-step\": 2201}, {\"accuracy\": 1.0, \"loss\": 0.0004284493043087423, \"time-step\": 2202}, {\"accuracy\": 1.0, \"loss\": 0.0004198774113319814, \"time-step\": 2203}, {\"accuracy\": 1.0, \"loss\": 0.00042798282811418176, \"time-step\": 2204}, {\"accuracy\": 1.0, \"loss\": 0.00041942507959902287, \"time-step\": 2205}, {\"accuracy\": 1.0, \"loss\": 0.0004275253741070628, \"time-step\": 2206}, {\"accuracy\": 1.0, \"loss\": 0.00041897938353940845, \"time-step\": 2207}, {\"accuracy\": 1.0, \"loss\": 0.0004270811623428017, \"time-step\": 2208}, {\"accuracy\": 1.0, \"loss\": 0.0004185403522569686, \"time-step\": 2209}, {\"accuracy\": 1.0, \"loss\": 0.00042663596104830503, \"time-step\": 2210}, {\"accuracy\": 1.0, \"loss\": 0.00041810396942310035, \"time-step\": 2211}, {\"accuracy\": 1.0, \"loss\": 0.00042619227315299213, \"time-step\": 2212}, {\"accuracy\": 1.0, \"loss\": 0.00041766383219510317, \"time-step\": 2213}, {\"accuracy\": 1.0, \"loss\": 0.0004257462569512427, \"time-step\": 2214}, {\"accuracy\": 1.0, \"loss\": 0.00041722250171005726, \"time-step\": 2215}, {\"accuracy\": 1.0, \"loss\": 0.0004253043734934181, \"time-step\": 2216}, {\"accuracy\": 1.0, \"loss\": 0.00041678882553242147, \"time-step\": 2217}, {\"accuracy\": 1.0, \"loss\": 0.00042485311860218644, \"time-step\": 2218}, {\"accuracy\": 1.0, \"loss\": 0.0004163365811109543, \"time-step\": 2219}, {\"accuracy\": 1.0, \"loss\": 0.0004243961302563548, \"time-step\": 2220}, {\"accuracy\": 1.0, \"loss\": 0.0004158912051934749, \"time-step\": 2221}, {\"accuracy\": 1.0, \"loss\": 0.0004239483969286084, \"time-step\": 2222}, {\"accuracy\": 1.0, \"loss\": 0.0004154537746217102, \"time-step\": 2223}, {\"accuracy\": 1.0, \"loss\": 0.0004235062515363097, \"time-step\": 2224}, {\"accuracy\": 1.0, \"loss\": 0.00041501567466184497, \"time-step\": 2225}, {\"accuracy\": 1.0, \"loss\": 0.0004230680060572922, \"time-step\": 2226}, {\"accuracy\": 1.0, \"loss\": 0.000414589187130332, \"time-step\": 2227}, {\"accuracy\": 1.0, \"loss\": 0.0004226304590702057, \"time-step\": 2228}, {\"accuracy\": 1.0, \"loss\": 0.0004141477693337947, \"time-step\": 2229}, {\"accuracy\": 1.0, \"loss\": 0.00042217952432110906, \"time-step\": 2230}, {\"accuracy\": 1.0, \"loss\": 0.00041371400584466755, \"time-step\": 2231}, {\"accuracy\": 1.0, \"loss\": 0.0004217489040456712, \"time-step\": 2232}, {\"accuracy\": 1.0, \"loss\": 0.0004132930771447718, \"time-step\": 2233}, {\"accuracy\": 1.0, \"loss\": 0.00042132119415327907, \"time-step\": 2234}, {\"accuracy\": 1.0, \"loss\": 0.0004128555010538548, \"time-step\": 2235}, {\"accuracy\": 1.0, \"loss\": 0.0004208732570987195, \"time-step\": 2236}, {\"accuracy\": 1.0, \"loss\": 0.0004124183324165642, \"time-step\": 2237}, {\"accuracy\": 1.0, \"loss\": 0.0004204305005259812, \"time-step\": 2238}, {\"accuracy\": 1.0, \"loss\": 0.0004119959194213152, \"time-step\": 2239}, {\"accuracy\": 1.0, \"loss\": 0.00042000217945314944, \"time-step\": 2240}, {\"accuracy\": 1.0, \"loss\": 0.0004115664050914347, \"time-step\": 2241}, {\"accuracy\": 1.0, \"loss\": 0.00041956413770094514, \"time-step\": 2242}, {\"accuracy\": 1.0, \"loss\": 0.0004111303423997015, \"time-step\": 2243}, {\"accuracy\": 1.0, \"loss\": 0.00041912865708582103, \"time-step\": 2244}, {\"accuracy\": 1.0, \"loss\": 0.0004107159620616585, \"time-step\": 2245}, {\"accuracy\": 1.0, \"loss\": 0.0004187161976005882, \"time-step\": 2246}, {\"accuracy\": 1.0, \"loss\": 0.0004103043465875089, \"time-step\": 2247}, {\"accuracy\": 1.0, \"loss\": 0.0004182938137091696, \"time-step\": 2248}, {\"accuracy\": 1.0, \"loss\": 0.0004098829231224954, \"time-step\": 2249}, {\"accuracy\": 1.0, \"loss\": 0.0004178577510174364, \"time-step\": 2250}, {\"accuracy\": 1.0, \"loss\": 0.00040944485226646066, \"time-step\": 2251}, {\"accuracy\": 1.0, \"loss\": 0.00041742163011804223, \"time-step\": 2252}, {\"accuracy\": 1.0, \"loss\": 0.0004090258735232055, \"time-step\": 2253}, {\"accuracy\": 1.0, \"loss\": 0.0004169914172962308, \"time-step\": 2254}, {\"accuracy\": 1.0, \"loss\": 0.0004086033732164651, \"time-step\": 2255}, {\"accuracy\": 1.0, \"loss\": 0.0004165631835348904, \"time-step\": 2256}, {\"accuracy\": 1.0, \"loss\": 0.0004081868100911379, \"time-step\": 2257}, {\"accuracy\": 1.0, \"loss\": 0.00041615331429056823, \"time-step\": 2258}, {\"accuracy\": 1.0, \"loss\": 0.0004077802295796573, \"time-step\": 2259}, {\"accuracy\": 1.0, \"loss\": 0.00041573724593035877, \"time-step\": 2260}, {\"accuracy\": 1.0, \"loss\": 0.00040736989467404783, \"time-step\": 2261}, {\"accuracy\": 1.0, \"loss\": 0.00041532021714374423, \"time-step\": 2262}, {\"accuracy\": 1.0, \"loss\": 0.0004069546703249216, \"time-step\": 2263}, {\"accuracy\": 1.0, \"loss\": 0.0004148914013057947, \"time-step\": 2264}, {\"accuracy\": 1.0, \"loss\": 0.0004065349348820746, \"time-step\": 2265}, {\"accuracy\": 1.0, \"loss\": 0.0004144747799728066, \"time-step\": 2266}, {\"accuracy\": 1.0, \"loss\": 0.00040611805161461234, \"time-step\": 2267}, {\"accuracy\": 1.0, \"loss\": 0.00041404447983950377, \"time-step\": 2268}, {\"accuracy\": 1.0, \"loss\": 0.0004056995385326445, \"time-step\": 2269}, {\"accuracy\": 1.0, \"loss\": 0.00041362346382811666, \"time-step\": 2270}, {\"accuracy\": 1.0, \"loss\": 0.00040528469253331423, \"time-step\": 2271}, {\"accuracy\": 1.0, \"loss\": 0.0004132085887249559, \"time-step\": 2272}, {\"accuracy\": 1.0, \"loss\": 0.00040488431113772094, \"time-step\": 2273}, {\"accuracy\": 1.0, \"loss\": 0.00041279522702097893, \"time-step\": 2274}, {\"accuracy\": 1.0, \"loss\": 0.00040447013452649117, \"time-step\": 2275}, {\"accuracy\": 1.0, \"loss\": 0.00041238474659621716, \"time-step\": 2276}, {\"accuracy\": 1.0, \"loss\": 0.00040407752385362983, \"time-step\": 2277}, {\"accuracy\": 1.0, \"loss\": 0.0004119928926229477, \"time-step\": 2278}, {\"accuracy\": 1.0, \"loss\": 0.00040368924965150654, \"time-step\": 2279}, {\"accuracy\": 1.0, \"loss\": 0.00041158602107316256, \"time-step\": 2280}, {\"accuracy\": 1.0, \"loss\": 0.0004032833385281265, \"time-step\": 2281}, {\"accuracy\": 1.0, \"loss\": 0.00041117746150121093, \"time-step\": 2282}, {\"accuracy\": 1.0, \"loss\": 0.0004028732655569911, \"time-step\": 2283}, {\"accuracy\": 1.0, \"loss\": 0.00041075312765315175, \"time-step\": 2284}, {\"accuracy\": 1.0, \"loss\": 0.00040246048592962325, \"time-step\": 2285}, {\"accuracy\": 1.0, \"loss\": 0.00041034119203686714, \"time-step\": 2286}, {\"accuracy\": 1.0, \"loss\": 0.0004020556225441396, \"time-step\": 2287}, {\"accuracy\": 1.0, \"loss\": 0.00040993053698912263, \"time-step\": 2288}, {\"accuracy\": 1.0, \"loss\": 0.0004016490711364895, \"time-step\": 2289}, {\"accuracy\": 1.0, \"loss\": 0.0004095205513294786, \"time-step\": 2290}, {\"accuracy\": 1.0, \"loss\": 0.0004012511926703155, \"time-step\": 2291}, {\"accuracy\": 1.0, \"loss\": 0.0004091127775609493, \"time-step\": 2292}, {\"accuracy\": 1.0, \"loss\": 0.0004008548567071557, \"time-step\": 2293}, {\"accuracy\": 1.0, \"loss\": 0.00040871891542337835, \"time-step\": 2294}, {\"accuracy\": 1.0, \"loss\": 0.000400456425268203, \"time-step\": 2295}, {\"accuracy\": 1.0, \"loss\": 0.00040830468060448766, \"time-step\": 2296}, {\"accuracy\": 1.0, \"loss\": 0.0004000467888545245, \"time-step\": 2297}, {\"accuracy\": 1.0, \"loss\": 0.0004078936472069472, \"time-step\": 2298}, {\"accuracy\": 1.0, \"loss\": 0.00039964888128452003, \"time-step\": 2299}, {\"accuracy\": 1.0, \"loss\": 0.00040750071639195085, \"time-step\": 2300}, {\"accuracy\": 1.0, \"loss\": 0.0003992602287326008, \"time-step\": 2301}, {\"accuracy\": 1.0, \"loss\": 0.00040709771565161645, \"time-step\": 2302}, {\"accuracy\": 1.0, \"loss\": 0.0003988614771515131, \"time-step\": 2303}, {\"accuracy\": 1.0, \"loss\": 0.0004066926194354892, \"time-step\": 2304}, {\"accuracy\": 1.0, \"loss\": 0.0003984697686973959, \"time-step\": 2305}, {\"accuracy\": 1.0, \"loss\": 0.00040630792500451207, \"time-step\": 2306}, {\"accuracy\": 1.0, \"loss\": 0.0003980889741797, \"time-step\": 2307}, {\"accuracy\": 1.0, \"loss\": 0.00040590757271274924, \"time-step\": 2308}, {\"accuracy\": 1.0, \"loss\": 0.00039768702117726207, \"time-step\": 2309}, {\"accuracy\": 1.0, \"loss\": 0.0004055098397657275, \"time-step\": 2310}, {\"accuracy\": 1.0, \"loss\": 0.00039730139542371035, \"time-step\": 2311}, {\"accuracy\": 1.0, \"loss\": 0.00040511065162718296, \"time-step\": 2312}, {\"accuracy\": 1.0, \"loss\": 0.0003969091339968145, \"time-step\": 2313}, {\"accuracy\": 1.0, \"loss\": 0.0004047170514240861, \"time-step\": 2314}, {\"accuracy\": 1.0, \"loss\": 0.0003965176292695105, \"time-step\": 2315}, {\"accuracy\": 1.0, \"loss\": 0.00040432135574519634, \"time-step\": 2316}, {\"accuracy\": 1.0, \"loss\": 0.00039613363333046436, \"time-step\": 2317}, {\"accuracy\": 1.0, \"loss\": 0.00040392708615399897, \"time-step\": 2318}, {\"accuracy\": 1.0, \"loss\": 0.0003957482986152172, \"time-step\": 2319}, {\"accuracy\": 1.0, \"loss\": 0.0004035456804558635, \"time-step\": 2320}, {\"accuracy\": 1.0, \"loss\": 0.00039536517579108477, \"time-step\": 2321}, {\"accuracy\": 1.0, \"loss\": 0.0004031509452033788, \"time-step\": 2322}, {\"accuracy\": 1.0, \"loss\": 0.0003949830716010183, \"time-step\": 2323}, {\"accuracy\": 1.0, \"loss\": 0.0004027689865324646, \"time-step\": 2324}, {\"accuracy\": 1.0, \"loss\": 0.00039461805135942996, \"time-step\": 2325}, {\"accuracy\": 1.0, \"loss\": 0.00040240611997433007, \"time-step\": 2326}, {\"accuracy\": 1.0, \"loss\": 0.00039425259456038475, \"time-step\": 2327}, {\"accuracy\": 1.0, \"loss\": 0.00040203070966526866, \"time-step\": 2328}, {\"accuracy\": 1.0, \"loss\": 0.0003938775334972888, \"time-step\": 2329}, {\"accuracy\": 1.0, \"loss\": 0.00040164843085221946, \"time-step\": 2330}, {\"accuracy\": 1.0, \"loss\": 0.0003935062268283218, \"time-step\": 2331}, {\"accuracy\": 1.0, \"loss\": 0.00040127208922058344, \"time-step\": 2332}, {\"accuracy\": 1.0, \"loss\": 0.0003931232204195112, \"time-step\": 2333}, {\"accuracy\": 1.0, \"loss\": 0.00040087493835017085, \"time-step\": 2334}, {\"accuracy\": 1.0, \"loss\": 0.0003927378566004336, \"time-step\": 2335}, {\"accuracy\": 1.0, \"loss\": 0.00040048727532848716, \"time-step\": 2336}, {\"accuracy\": 1.0, \"loss\": 0.0003923522890545428, \"time-step\": 2337}, {\"accuracy\": 1.0, \"loss\": 0.0004000946064479649, \"time-step\": 2338}, {\"accuracy\": 1.0, \"loss\": 0.00039196142461150885, \"time-step\": 2339}, {\"accuracy\": 1.0, \"loss\": 0.00039970045327208936, \"time-step\": 2340}, {\"accuracy\": 1.0, \"loss\": 0.00039158453000709414, \"time-step\": 2341}, {\"accuracy\": 1.0, \"loss\": 0.00039932207437232137, \"time-step\": 2342}, {\"accuracy\": 1.0, \"loss\": 0.0003912134561687708, \"time-step\": 2343}, {\"accuracy\": 1.0, \"loss\": 0.0003989457036368549, \"time-step\": 2344}, {\"accuracy\": 1.0, \"loss\": 0.00039083982119336724, \"time-step\": 2345}, {\"accuracy\": 1.0, \"loss\": 0.00039857177762314677, \"time-step\": 2346}, {\"accuracy\": 1.0, \"loss\": 0.0003904799232259393, \"time-step\": 2347}, {\"accuracy\": 1.0, \"loss\": 0.0003982137131970376, \"time-step\": 2348}, {\"accuracy\": 1.0, \"loss\": 0.00039012249908410013, \"time-step\": 2349}, {\"accuracy\": 1.0, \"loss\": 0.0003978414461016655, \"time-step\": 2350}, {\"accuracy\": 1.0, \"loss\": 0.00038975838106125593, \"time-step\": 2351}, {\"accuracy\": 1.0, \"loss\": 0.0003974692663177848, \"time-step\": 2352}, {\"accuracy\": 1.0, \"loss\": 0.00038938288344070315, \"time-step\": 2353}, {\"accuracy\": 1.0, \"loss\": 0.00039708882104605436, \"time-step\": 2354}, {\"accuracy\": 1.0, \"loss\": 0.0003890172520186752, \"time-step\": 2355}, {\"accuracy\": 1.0, \"loss\": 0.0003967181546613574, \"time-step\": 2356}, {\"accuracy\": 1.0, \"loss\": 0.00038864248199388385, \"time-step\": 2357}, {\"accuracy\": 1.0, \"loss\": 0.0003963373019360006, \"time-step\": 2358}, {\"accuracy\": 1.0, \"loss\": 0.0003882648888975382, \"time-step\": 2359}, {\"accuracy\": 1.0, \"loss\": 0.0003959504538215697, \"time-step\": 2360}, {\"accuracy\": 1.0, \"loss\": 0.0003878908173646778, \"time-step\": 2361}, {\"accuracy\": 1.0, \"loss\": 0.0003955676220357418, \"time-step\": 2362}, {\"accuracy\": 1.0, \"loss\": 0.0003875114780385047, \"time-step\": 2363}, {\"accuracy\": 1.0, \"loss\": 0.0003951794933527708, \"time-step\": 2364}, {\"accuracy\": 1.0, \"loss\": 0.0003871343214996159, \"time-step\": 2365}, {\"accuracy\": 1.0, \"loss\": 0.00039481359999626875, \"time-step\": 2366}, {\"accuracy\": 1.0, \"loss\": 0.00038678571581840515, \"time-step\": 2367}, {\"accuracy\": 1.0, \"loss\": 0.0003944621130358428, \"time-step\": 2368}, {\"accuracy\": 1.0, \"loss\": 0.00038643484003841877, \"time-step\": 2369}, {\"accuracy\": 1.0, \"loss\": 0.00039409619057551026, \"time-step\": 2370}, {\"accuracy\": 1.0, \"loss\": 0.00038606461021117866, \"time-step\": 2371}, {\"accuracy\": 1.0, \"loss\": 0.0003937317233067006, \"time-step\": 2372}, {\"accuracy\": 1.0, \"loss\": 0.00038570305332541466, \"time-step\": 2373}, {\"accuracy\": 1.0, \"loss\": 0.0003933468833565712, \"time-step\": 2374}, {\"accuracy\": 1.0, \"loss\": 0.0003853255766443908, \"time-step\": 2375}, {\"accuracy\": 1.0, \"loss\": 0.00039297546027228236, \"time-step\": 2376}, {\"accuracy\": 1.0, \"loss\": 0.0003849674540106207, \"time-step\": 2377}, {\"accuracy\": 1.0, \"loss\": 0.0003926100325770676, \"time-step\": 2378}, {\"accuracy\": 1.0, \"loss\": 0.00038460828363895416, \"time-step\": 2379}, {\"accuracy\": 1.0, \"loss\": 0.00039224501233547926, \"time-step\": 2380}, {\"accuracy\": 1.0, \"loss\": 0.00038425312959589064, \"time-step\": 2381}, {\"accuracy\": 1.0, \"loss\": 0.0003918949223589152, \"time-step\": 2382}, {\"accuracy\": 1.0, \"loss\": 0.0003839041164610535, \"time-step\": 2383}, {\"accuracy\": 1.0, \"loss\": 0.0003915384004358202, \"time-step\": 2384}, {\"accuracy\": 1.0, \"loss\": 0.0003835557436104864, \"time-step\": 2385}, {\"accuracy\": 1.0, \"loss\": 0.00039117992855608463, \"time-step\": 2386}, {\"accuracy\": 1.0, \"loss\": 0.00038320638122968376, \"time-step\": 2387}, {\"accuracy\": 1.0, \"loss\": 0.0003908275393769145, \"time-step\": 2388}, {\"accuracy\": 1.0, \"loss\": 0.00038285503978841007, \"time-step\": 2389}, {\"accuracy\": 1.0, \"loss\": 0.0003904678742401302, \"time-step\": 2390}, {\"accuracy\": 1.0, \"loss\": 0.00038249645149335265, \"time-step\": 2391}, {\"accuracy\": 1.0, \"loss\": 0.00039009988540783525, \"time-step\": 2392}, {\"accuracy\": 1.0, \"loss\": 0.0003821427817456424, \"time-step\": 2393}, {\"accuracy\": 1.0, \"loss\": 0.000389752967748791, \"time-step\": 2394}, {\"accuracy\": 1.0, \"loss\": 0.00038179359398782253, \"time-step\": 2395}, {\"accuracy\": 1.0, \"loss\": 0.00038939822115935385, \"time-step\": 2396}, {\"accuracy\": 1.0, \"loss\": 0.0003814490628428757, \"time-step\": 2397}, {\"accuracy\": 1.0, \"loss\": 0.00038903916720300913, \"time-step\": 2398}, {\"accuracy\": 1.0, \"loss\": 0.00038109198794700205, \"time-step\": 2399}, {\"accuracy\": 1.0, \"loss\": 0.0003886871854774654, \"time-step\": 2400}, {\"accuracy\": 1.0, \"loss\": 0.00038075787597335875, \"time-step\": 2401}, {\"accuracy\": 1.0, \"loss\": 0.0003883414901793003, \"time-step\": 2402}, {\"accuracy\": 1.0, \"loss\": 0.0003804054867941886, \"time-step\": 2403}, {\"accuracy\": 1.0, \"loss\": 0.00038798776222392917, \"time-step\": 2404}, {\"accuracy\": 1.0, \"loss\": 0.0003800596168730408, \"time-step\": 2405}, {\"accuracy\": 1.0, \"loss\": 0.00038762239273637533, \"time-step\": 2406}, {\"accuracy\": 1.0, \"loss\": 0.0003796872333623469, \"time-step\": 2407}, {\"accuracy\": 1.0, \"loss\": 0.00038724704063497484, \"time-step\": 2408}, {\"accuracy\": 1.0, \"loss\": 0.0003793271316681057, \"time-step\": 2409}, {\"accuracy\": 1.0, \"loss\": 0.00038689165376126766, \"time-step\": 2410}, {\"accuracy\": 1.0, \"loss\": 0.0003789957263506949, \"time-step\": 2411}, {\"accuracy\": 1.0, \"loss\": 0.00038656004471704364, \"time-step\": 2412}, {\"accuracy\": 1.0, \"loss\": 0.0003786558227147907, \"time-step\": 2413}, {\"accuracy\": 1.0, \"loss\": 0.00038621947169303894, \"time-step\": 2414}, {\"accuracy\": 1.0, \"loss\": 0.00037831845111213624, \"time-step\": 2415}, {\"accuracy\": 1.0, \"loss\": 0.0003858736308757216, \"time-step\": 2416}, {\"accuracy\": 1.0, \"loss\": 0.000377983640646562, \"time-step\": 2417}, {\"accuracy\": 1.0, \"loss\": 0.000385531282518059, \"time-step\": 2418}, {\"accuracy\": 1.0, \"loss\": 0.00037763541331514716, \"time-step\": 2419}, {\"accuracy\": 1.0, \"loss\": 0.0003851625951938331, \"time-step\": 2420}, {\"accuracy\": 1.0, \"loss\": 0.0003772831114474684, \"time-step\": 2421}, {\"accuracy\": 1.0, \"loss\": 0.00038482563104480505, \"time-step\": 2422}, {\"accuracy\": 1.0, \"loss\": 0.0003769513568840921, \"time-step\": 2423}, {\"accuracy\": 1.0, \"loss\": 0.0003844817401841283, \"time-step\": 2424}, {\"accuracy\": 1.0, \"loss\": 0.0003766102599911392, \"time-step\": 2425}, {\"accuracy\": 1.0, \"loss\": 0.0003841303405351937, \"time-step\": 2426}, {\"accuracy\": 1.0, \"loss\": 0.0003762753040064126, \"time-step\": 2427}, {\"accuracy\": 1.0, \"loss\": 0.00038380236946977675, \"time-step\": 2428}, {\"accuracy\": 1.0, \"loss\": 0.0003759443061426282, \"time-step\": 2429}, {\"accuracy\": 1.0, \"loss\": 0.0003834656672552228, \"time-step\": 2430}, {\"accuracy\": 1.0, \"loss\": 0.0003756132209673524, \"time-step\": 2431}, {\"accuracy\": 1.0, \"loss\": 0.0003831212525255978, \"time-step\": 2432}, {\"accuracy\": 1.0, \"loss\": 0.0003752709599211812, \"time-step\": 2433}, {\"accuracy\": 1.0, \"loss\": 0.0003827749751508236, \"time-step\": 2434}, {\"accuracy\": 1.0, \"loss\": 0.00037493224954232574, \"time-step\": 2435}, {\"accuracy\": 1.0, \"loss\": 0.00038243274320848286, \"time-step\": 2436}, {\"accuracy\": 1.0, \"loss\": 0.00037459348095580935, \"time-step\": 2437}, {\"accuracy\": 1.0, \"loss\": 0.00038209313061088324, \"time-step\": 2438}, {\"accuracy\": 1.0, \"loss\": 0.0003742682165466249, \"time-step\": 2439}, {\"accuracy\": 1.0, \"loss\": 0.0003817665856331587, \"time-step\": 2440}, {\"accuracy\": 1.0, \"loss\": 0.00037393844104371965, \"time-step\": 2441}, {\"accuracy\": 1.0, \"loss\": 0.00038142045377753675, \"time-step\": 2442}, {\"accuracy\": 1.0, \"loss\": 0.0003735999925993383, \"time-step\": 2443}, {\"accuracy\": 1.0, \"loss\": 0.000381083955289796, \"time-step\": 2444}, {\"accuracy\": 1.0, \"loss\": 0.0003732636687345803, \"time-step\": 2445}, {\"accuracy\": 1.0, \"loss\": 0.0003807441098615527, \"time-step\": 2446}, {\"accuracy\": 1.0, \"loss\": 0.000372939626686275, \"time-step\": 2447}, {\"accuracy\": 1.0, \"loss\": 0.00038041354855522513, \"time-step\": 2448}, {\"accuracy\": 1.0, \"loss\": 0.0003726163995452225, \"time-step\": 2449}, {\"accuracy\": 1.0, \"loss\": 0.0003800902341026813, \"time-step\": 2450}, {\"accuracy\": 1.0, \"loss\": 0.00037229410372674465, \"time-step\": 2451}, {\"accuracy\": 1.0, \"loss\": 0.0003797563258558512, \"time-step\": 2452}, {\"accuracy\": 1.0, \"loss\": 0.000371962261851877, \"time-step\": 2453}, {\"accuracy\": 1.0, \"loss\": 0.00037941939081065357, \"time-step\": 2454}, {\"accuracy\": 1.0, \"loss\": 0.00037163382512517273, \"time-step\": 2455}, {\"accuracy\": 1.0, \"loss\": 0.00037909229286015034, \"time-step\": 2456}, {\"accuracy\": 1.0, \"loss\": 0.0003713059995789081, \"time-step\": 2457}, {\"accuracy\": 1.0, \"loss\": 0.0003787584719248116, \"time-step\": 2458}, {\"accuracy\": 1.0, \"loss\": 0.0003709856537170708, \"time-step\": 2459}, {\"accuracy\": 1.0, \"loss\": 0.0003784218861255795, \"time-step\": 2460}, {\"accuracy\": 1.0, \"loss\": 0.0003706467105075717, \"time-step\": 2461}, {\"accuracy\": 1.0, \"loss\": 0.00037808436900377274, \"time-step\": 2462}, {\"accuracy\": 1.0, \"loss\": 0.00037031617830507457, \"time-step\": 2463}, {\"accuracy\": 1.0, \"loss\": 0.00037774673546664417, \"time-step\": 2464}, {\"accuracy\": 1.0, \"loss\": 0.0003699747030623257, \"time-step\": 2465}, {\"accuracy\": 1.0, \"loss\": 0.0003773967328015715, \"time-step\": 2466}, {\"accuracy\": 1.0, \"loss\": 0.0003696387866511941, \"time-step\": 2467}, {\"accuracy\": 1.0, \"loss\": 0.0003770665789488703, \"time-step\": 2468}, {\"accuracy\": 1.0, \"loss\": 0.0003693251928780228, \"time-step\": 2469}, {\"accuracy\": 1.0, \"loss\": 0.0003767416928894818, \"time-step\": 2470}, {\"accuracy\": 1.0, \"loss\": 0.0003689980076160282, \"time-step\": 2471}, {\"accuracy\": 1.0, \"loss\": 0.00037641171365976334, \"time-step\": 2472}, {\"accuracy\": 1.0, \"loss\": 0.0003686813870444894, \"time-step\": 2473}, {\"accuracy\": 1.0, \"loss\": 0.00037609966238960624, \"time-step\": 2474}, {\"accuracy\": 1.0, \"loss\": 0.000368366832844913, \"time-step\": 2475}, {\"accuracy\": 1.0, \"loss\": 0.0003757716331165284, \"time-step\": 2476}, {\"accuracy\": 1.0, \"loss\": 0.000368040578905493, \"time-step\": 2477}, {\"accuracy\": 1.0, \"loss\": 0.00037543734651990235, \"time-step\": 2478}, {\"accuracy\": 1.0, \"loss\": 0.0003677202039398253, \"time-step\": 2479}, {\"accuracy\": 1.0, \"loss\": 0.00037511769914999604, \"time-step\": 2480}, {\"accuracy\": 1.0, \"loss\": 0.0003674098406918347, \"time-step\": 2481}, {\"accuracy\": 1.0, \"loss\": 0.00037480410537682474, \"time-step\": 2482}, {\"accuracy\": 1.0, \"loss\": 0.0003670938313007355, \"time-step\": 2483}, {\"accuracy\": 1.0, \"loss\": 0.0003744777350220829, \"time-step\": 2484}, {\"accuracy\": 1.0, \"loss\": 0.00036677566822618246, \"time-step\": 2485}, {\"accuracy\": 1.0, \"loss\": 0.0003741583786904812, \"time-step\": 2486}, {\"accuracy\": 1.0, \"loss\": 0.00036646515945903957, \"time-step\": 2487}, {\"accuracy\": 1.0, \"loss\": 0.0003738472005352378, \"time-step\": 2488}, {\"accuracy\": 1.0, \"loss\": 0.0003661583468783647, \"time-step\": 2489}, {\"accuracy\": 1.0, \"loss\": 0.0003735329082701355, \"time-step\": 2490}, {\"accuracy\": 1.0, \"loss\": 0.0003658439964056015, \"time-step\": 2491}, {\"accuracy\": 1.0, \"loss\": 0.000373209040844813, \"time-step\": 2492}, {\"accuracy\": 1.0, \"loss\": 0.00036552580422721803, \"time-step\": 2493}, {\"accuracy\": 1.0, \"loss\": 0.00037289696047082543, \"time-step\": 2494}, {\"accuracy\": 1.0, \"loss\": 0.0003652279556263238, \"time-step\": 2495}, {\"accuracy\": 1.0, \"loss\": 0.0003725881688296795, \"time-step\": 2496}, {\"accuracy\": 1.0, \"loss\": 0.0003649169811978936, \"time-step\": 2497}, {\"accuracy\": 1.0, \"loss\": 0.00037227445864118636, \"time-step\": 2498}, {\"accuracy\": 1.0, \"loss\": 0.00036460976116359234, \"time-step\": 2499}, {\"accuracy\": 1.0, \"loss\": 0.0003719562664628029, \"time-step\": 2500}, {\"accuracy\": 1.0, \"loss\": 0.0003642909578047693, \"time-step\": 2501}, {\"accuracy\": 1.0, \"loss\": 0.0003716325736604631, \"time-step\": 2502}, {\"accuracy\": 1.0, \"loss\": 0.00036397314397618175, \"time-step\": 2503}, {\"accuracy\": 1.0, \"loss\": 0.0003713199985213578, \"time-step\": 2504}, {\"accuracy\": 1.0, \"loss\": 0.00036367206485010684, \"time-step\": 2505}, {\"accuracy\": 1.0, \"loss\": 0.00037100620102137327, \"time-step\": 2506}, {\"accuracy\": 1.0, \"loss\": 0.0003633610322140157, \"time-step\": 2507}, {\"accuracy\": 1.0, \"loss\": 0.0003706981078721583, \"time-step\": 2508}, {\"accuracy\": 1.0, \"loss\": 0.0003630627179518342, \"time-step\": 2509}, {\"accuracy\": 1.0, \"loss\": 0.00037040116149000823, \"time-step\": 2510}, {\"accuracy\": 1.0, \"loss\": 0.0003627744736149907, \"time-step\": 2511}, {\"accuracy\": 1.0, \"loss\": 0.00037010182859376073, \"time-step\": 2512}, {\"accuracy\": 1.0, \"loss\": 0.0003624706878326833, \"time-step\": 2513}, {\"accuracy\": 1.0, \"loss\": 0.00036979158176109195, \"time-step\": 2514}, {\"accuracy\": 1.0, \"loss\": 0.00036217470187693834, \"time-step\": 2515}, {\"accuracy\": 1.0, \"loss\": 0.0003694916085805744, \"time-step\": 2516}, {\"accuracy\": 1.0, \"loss\": 0.00036186614306643605, \"time-step\": 2517}, {\"accuracy\": 1.0, \"loss\": 0.00036917856778018177, \"time-step\": 2518}, {\"accuracy\": 1.0, \"loss\": 0.00036156762507744133, \"time-step\": 2519}, {\"accuracy\": 1.0, \"loss\": 0.0003688715514726937, \"time-step\": 2520}, {\"accuracy\": 1.0, \"loss\": 0.0003612507716752589, \"time-step\": 2521}, {\"accuracy\": 1.0, \"loss\": 0.0003685501287691295, \"time-step\": 2522}, {\"accuracy\": 1.0, \"loss\": 0.00036094285314902663, \"time-step\": 2523}, {\"accuracy\": 1.0, \"loss\": 0.0003682358074001968, \"time-step\": 2524}, {\"accuracy\": 1.0, \"loss\": 0.00036064296728000045, \"time-step\": 2525}, {\"accuracy\": 1.0, \"loss\": 0.00036794343031942844, \"time-step\": 2526}, {\"accuracy\": 1.0, \"loss\": 0.00036034456570632756, \"time-step\": 2527}, {\"accuracy\": 1.0, \"loss\": 0.0003676341148093343, \"time-step\": 2528}, {\"accuracy\": 1.0, \"loss\": 0.0003600431082304567, \"time-step\": 2529}, {\"accuracy\": 1.0, \"loss\": 0.00036732532316818833, \"time-step\": 2530}, {\"accuracy\": 1.0, \"loss\": 0.0003597409522626549, \"time-step\": 2531}, {\"accuracy\": 1.0, \"loss\": 0.0003670156584121287, \"time-step\": 2532}, {\"accuracy\": 1.0, \"loss\": 0.0003594363515730947, \"time-step\": 2533}, {\"accuracy\": 1.0, \"loss\": 0.0003667095734272152, \"time-step\": 2534}, {\"accuracy\": 1.0, \"loss\": 0.0003591342247091234, \"time-step\": 2535}, {\"accuracy\": 1.0, \"loss\": 0.00036640415783040226, \"time-step\": 2536}, {\"accuracy\": 1.0, \"loss\": 0.0003588291583582759, \"time-step\": 2537}, {\"accuracy\": 1.0, \"loss\": 0.00036609251401387155, \"time-step\": 2538}, {\"accuracy\": 1.0, \"loss\": 0.0003585330559872091, \"time-step\": 2539}, {\"accuracy\": 1.0, \"loss\": 0.0003657945489976555, \"time-step\": 2540}, {\"accuracy\": 1.0, \"loss\": 0.0003582309582270682, \"time-step\": 2541}, {\"accuracy\": 1.0, \"loss\": 0.0003654752508737147, \"time-step\": 2542}, {\"accuracy\": 1.0, \"loss\": 0.0003579144540708512, \"time-step\": 2543}, {\"accuracy\": 1.0, \"loss\": 0.00036516773980110884, \"time-step\": 2544}, {\"accuracy\": 1.0, \"loss\": 0.00035762033076025546, \"time-step\": 2545}, {\"accuracy\": 1.0, \"loss\": 0.000364867300959304, \"time-step\": 2546}, {\"accuracy\": 1.0, \"loss\": 0.00035732463584281504, \"time-step\": 2547}, {\"accuracy\": 1.0, \"loss\": 0.00036456657107919455, \"time-step\": 2548}, {\"accuracy\": 1.0, \"loss\": 0.00035703106550499797, \"time-step\": 2549}, {\"accuracy\": 1.0, \"loss\": 0.00036426997394300997, \"time-step\": 2550}, {\"accuracy\": 1.0, \"loss\": 0.00035673577804118395, \"time-step\": 2551}, {\"accuracy\": 1.0, \"loss\": 0.00036396118230186403, \"time-step\": 2552}, {\"accuracy\": 1.0, \"loss\": 0.0003564373473636806, \"time-step\": 2553}, {\"accuracy\": 1.0, \"loss\": 0.0003636717447079718, \"time-step\": 2554}, {\"accuracy\": 1.0, \"loss\": 0.000356155913323164, \"time-step\": 2555}, {\"accuracy\": 1.0, \"loss\": 0.00036338169593364, \"time-step\": 2556}, {\"accuracy\": 1.0, \"loss\": 0.0003558708122000098, \"time-step\": 2557}, {\"accuracy\": 1.0, \"loss\": 0.00036309065762907267, \"time-step\": 2558}, {\"accuracy\": 1.0, \"loss\": 0.00035557776573114097, \"time-step\": 2559}, {\"accuracy\": 1.0, \"loss\": 0.0003627948462963104, \"time-step\": 2560}, {\"accuracy\": 1.0, \"loss\": 0.00035529182059690356, \"time-step\": 2561}, {\"accuracy\": 1.0, \"loss\": 0.0003625109966378659, \"time-step\": 2562}, {\"accuracy\": 1.0, \"loss\": 0.00035500916419550776, \"time-step\": 2563}, {\"accuracy\": 1.0, \"loss\": 0.0003622120129875839, \"time-step\": 2564}, {\"accuracy\": 1.0, \"loss\": 0.00035471058799885213, \"time-step\": 2565}, {\"accuracy\": 1.0, \"loss\": 0.0003619061317294836, \"time-step\": 2566}, {\"accuracy\": 1.0, \"loss\": 0.0003544129431247711, \"time-step\": 2567}, {\"accuracy\": 1.0, \"loss\": 0.0003616119211073965, \"time-step\": 2568}, {\"accuracy\": 1.0, \"loss\": 0.0003541290934663266, \"time-step\": 2569}, {\"accuracy\": 1.0, \"loss\": 0.00036132626701146364, \"time-step\": 2570}, {\"accuracy\": 1.0, \"loss\": 0.000353849318344146, \"time-step\": 2571}, {\"accuracy\": 1.0, \"loss\": 0.00036104326136410236, \"time-step\": 2572}, {\"accuracy\": 1.0, \"loss\": 0.00035357120214030147, \"time-step\": 2573}, {\"accuracy\": 1.0, \"loss\": 0.00036076156538911164, \"time-step\": 2574}, {\"accuracy\": 1.0, \"loss\": 0.0003532889240887016, \"time-step\": 2575}, {\"accuracy\": 1.0, \"loss\": 0.00036046659806743264, \"time-step\": 2576}, {\"accuracy\": 1.0, \"loss\": 0.000352996401488781, \"time-step\": 2577}, {\"accuracy\": 1.0, \"loss\": 0.0003601659554988146, \"time-step\": 2578}, {\"accuracy\": 1.0, \"loss\": 0.00035270091029815376, \"time-step\": 2579}, {\"accuracy\": 1.0, \"loss\": 0.0003598782350309193, \"time-step\": 2580}, {\"accuracy\": 1.0, \"loss\": 0.0003524266357999295, \"time-step\": 2581}, {\"accuracy\": 1.0, \"loss\": 0.00035958702210336924, \"time-step\": 2582}, {\"accuracy\": 1.0, \"loss\": 0.0003521318722050637, \"time-step\": 2583}, {\"accuracy\": 1.0, \"loss\": 0.0003592963330447674, \"time-step\": 2584}, {\"accuracy\": 1.0, \"loss\": 0.0003518495359458029, \"time-step\": 2585}, {\"accuracy\": 1.0, \"loss\": 0.00035901504452340305, \"time-step\": 2586}, {\"accuracy\": 1.0, \"loss\": 0.000351572351064533, \"time-step\": 2587}, {\"accuracy\": 1.0, \"loss\": 0.00035872316220775247, \"time-step\": 2588}, {\"accuracy\": 1.0, \"loss\": 0.0003512802650220692, \"time-step\": 2589}, {\"accuracy\": 1.0, \"loss\": 0.0003584289224818349, \"time-step\": 2590}, {\"accuracy\": 1.0, \"loss\": 0.00035099772503599524, \"time-step\": 2591}, {\"accuracy\": 1.0, \"loss\": 0.0003581357013899833, \"time-step\": 2592}, {\"accuracy\": 1.0, \"loss\": 0.00035070665762759745, \"time-step\": 2593}, {\"accuracy\": 1.0, \"loss\": 0.0003578493488021195, \"time-step\": 2594}, {\"accuracy\": 1.0, \"loss\": 0.00035042903618887067, \"time-step\": 2595}, {\"accuracy\": 1.0, \"loss\": 0.0003575741429813206, \"time-step\": 2596}, {\"accuracy\": 1.0, \"loss\": 0.0003501645987853408, \"time-step\": 2597}, {\"accuracy\": 1.0, \"loss\": 0.0003573013236746192, \"time-step\": 2598}, {\"accuracy\": 1.0, \"loss\": 0.00034989265259355307, \"time-step\": 2599}, {\"accuracy\": 1.0, \"loss\": 0.0003570191329345107, \"time-step\": 2600}, {\"accuracy\": 1.0, \"loss\": 0.0003496129356790334, \"time-step\": 2601}, {\"accuracy\": 1.0, \"loss\": 0.0003567392413970083, \"time-step\": 2602}, {\"accuracy\": 1.0, \"loss\": 0.00034933548886328936, \"time-step\": 2603}, {\"accuracy\": 1.0, \"loss\": 0.0003564546932466328, \"time-step\": 2604}, {\"accuracy\": 1.0, \"loss\": 0.0003490555682219565, \"time-step\": 2605}, {\"accuracy\": 1.0, \"loss\": 0.0003561746852938086, \"time-step\": 2606}, {\"accuracy\": 1.0, \"loss\": 0.0003487879876047373, \"time-step\": 2607}, {\"accuracy\": 1.0, \"loss\": 0.00035589479375630617, \"time-step\": 2608}, {\"accuracy\": 1.0, \"loss\": 0.0003484982589725405, \"time-step\": 2609}, {\"accuracy\": 1.0, \"loss\": 0.00035560422111302614, \"time-step\": 2610}, {\"accuracy\": 1.0, \"loss\": 0.00034821676672436297, \"time-step\": 2611}, {\"accuracy\": 1.0, \"loss\": 0.00035531906178221107, \"time-step\": 2612}, {\"accuracy\": 1.0, \"loss\": 0.0003479467413853854, \"time-step\": 2613}, {\"accuracy\": 1.0, \"loss\": 0.0003550481051206589, \"time-step\": 2614}, {\"accuracy\": 1.0, \"loss\": 0.00034767104079946876, \"time-step\": 2615}, {\"accuracy\": 1.0, \"loss\": 0.0003547639353200793, \"time-step\": 2616}, {\"accuracy\": 1.0, \"loss\": 0.0003474066033959389, \"time-step\": 2617}, {\"accuracy\": 1.0, \"loss\": 0.00035450226278044283, \"time-step\": 2618}, {\"accuracy\": 1.0, \"loss\": 0.0003471332311164588, \"time-step\": 2619}, {\"accuracy\": 1.0, \"loss\": 0.0003542224585544318, \"time-step\": 2620}, {\"accuracy\": 1.0, \"loss\": 0.0003468682989478111, \"time-step\": 2621}, {\"accuracy\": 1.0, \"loss\": 0.00035394285805523396, \"time-step\": 2622}, {\"accuracy\": 1.0, \"loss\": 0.00034658669028431177, \"time-step\": 2623}, {\"accuracy\": 1.0, \"loss\": 0.00035366497468203306, \"time-step\": 2624}, {\"accuracy\": 1.0, \"loss\": 0.00034630592563189566, \"time-step\": 2625}, {\"accuracy\": 1.0, \"loss\": 0.00035337425651960075, \"time-step\": 2626}, {\"accuracy\": 1.0, \"loss\": 0.000346026208717376, \"time-step\": 2627}, {\"accuracy\": 1.0, \"loss\": 0.00035309273516759276, \"time-step\": 2628}, {\"accuracy\": 1.0, \"loss\": 0.00034575071185827255, \"time-step\": 2629}, {\"accuracy\": 1.0, \"loss\": 0.0003528111847117543, \"time-step\": 2630}, {\"accuracy\": 1.0, \"loss\": 0.0003454723337199539, \"time-step\": 2631}, {\"accuracy\": 1.0, \"loss\": 0.00035253018722869456, \"time-step\": 2632}, {\"accuracy\": 1.0, \"loss\": 0.00034519581822678447, \"time-step\": 2633}, {\"accuracy\": 1.0, \"loss\": 0.000352250732248649, \"time-step\": 2634}, {\"accuracy\": 1.0, \"loss\": 0.00034492721897549927, \"time-step\": 2635}, {\"accuracy\": 1.0, \"loss\": 0.00035197692341171205, \"time-step\": 2636}, {\"accuracy\": 1.0, \"loss\": 0.0003446582704782486, \"time-step\": 2637}, {\"accuracy\": 1.0, \"loss\": 0.0003517056757118553, \"time-step\": 2638}, {\"accuracy\": 1.0, \"loss\": 0.00034439851879142225, \"time-step\": 2639}, {\"accuracy\": 1.0, \"loss\": 0.0003514460986480117, \"time-step\": 2640}, {\"accuracy\": 1.0, \"loss\": 0.00034413678804412484, \"time-step\": 2641}, {\"accuracy\": 1.0, \"loss\": 0.00035117825609631836, \"time-step\": 2642}, {\"accuracy\": 1.0, \"loss\": 0.00034388189669698477, \"time-step\": 2643}, {\"accuracy\": 1.0, \"loss\": 0.0003509163507260382, \"time-step\": 2644}, {\"accuracy\": 1.0, \"loss\": 0.0003436120168771595, \"time-step\": 2645}, {\"accuracy\": 1.0, \"loss\": 0.0003506392822600901, \"time-step\": 2646}, {\"accuracy\": 1.0, \"loss\": 0.00034334632800891995, \"time-step\": 2647}, {\"accuracy\": 1.0, \"loss\": 0.00035037388443015516, \"time-step\": 2648}, {\"accuracy\": 1.0, \"loss\": 0.00034308229805901647, \"time-step\": 2649}, {\"accuracy\": 1.0, \"loss\": 0.0003501113096717745, \"time-step\": 2650}, {\"accuracy\": 1.0, \"loss\": 0.0003428219351917505, \"time-step\": 2651}, {\"accuracy\": 1.0, \"loss\": 0.0003498370060697198, \"time-step\": 2652}, {\"accuracy\": 1.0, \"loss\": 0.0003425490576773882, \"time-step\": 2653}, {\"accuracy\": 1.0, \"loss\": 0.00034956002491526306, \"time-step\": 2654}, {\"accuracy\": 1.0, \"loss\": 0.0003422954468987882, \"time-step\": 2655}, {\"accuracy\": 1.0, \"loss\": 0.0003493171534501016, \"time-step\": 2656}, {\"accuracy\": 1.0, \"loss\": 0.000342049403116107, \"time-step\": 2657}, {\"accuracy\": 1.0, \"loss\": 0.0003490608651190996, \"time-step\": 2658}, {\"accuracy\": 1.0, \"loss\": 0.0003417863044887781, \"time-step\": 2659}, {\"accuracy\": 1.0, \"loss\": 0.0003487816429696977, \"time-step\": 2660}, {\"accuracy\": 1.0, \"loss\": 0.00034152367152273655, \"time-step\": 2661}, {\"accuracy\": 1.0, \"loss\": 0.00034852619864977896, \"time-step\": 2662}, {\"accuracy\": 1.0, \"loss\": 0.0003412639198359102, \"time-step\": 2663}, {\"accuracy\": 1.0, \"loss\": 0.00034825789043679833, \"time-step\": 2664}, {\"accuracy\": 1.0, \"loss\": 0.0003410036151763052, \"time-step\": 2665}, {\"accuracy\": 1.0, \"loss\": 0.00034799400600604713, \"time-step\": 2666}, {\"accuracy\": 1.0, \"loss\": 0.0003407403128221631, \"time-step\": 2667}, {\"accuracy\": 1.0, \"loss\": 0.0003477277932688594, \"time-step\": 2668}, {\"accuracy\": 1.0, \"loss\": 0.00034048405359499156, \"time-step\": 2669}, {\"accuracy\": 1.0, \"loss\": 0.00034745410084724426, \"time-step\": 2670}, {\"accuracy\": 1.0, \"loss\": 0.00034020707244053483, \"time-step\": 2671}, {\"accuracy\": 1.0, \"loss\": 0.00034718430833891034, \"time-step\": 2672}, {\"accuracy\": 1.0, \"loss\": 0.00033994982368312776, \"time-step\": 2673}, {\"accuracy\": 1.0, \"loss\": 0.0003469272924121469, \"time-step\": 2674}, {\"accuracy\": 1.0, \"loss\": 0.0003397015680093318, \"time-step\": 2675}, {\"accuracy\": 1.0, \"loss\": 0.00034666815190576017, \"time-step\": 2676}, {\"accuracy\": 1.0, \"loss\": 0.0003394375671632588, \"time-step\": 2677}, {\"accuracy\": 1.0, \"loss\": 0.00034640211379155517, \"time-step\": 2678}, {\"accuracy\": 1.0, \"loss\": 0.0003391758946236223, \"time-step\": 2679}, {\"accuracy\": 1.0, \"loss\": 0.0003461371816229075, \"time-step\": 2680}, {\"accuracy\": 1.0, \"loss\": 0.00033891203929670155, \"time-step\": 2681}, {\"accuracy\": 1.0, \"loss\": 0.000345858366927132, \"time-step\": 2682}, {\"accuracy\": 1.0, \"loss\": 0.00033864000579342246, \"time-step\": 2683}, {\"accuracy\": 1.0, \"loss\": 0.0003455986734479666, \"time-step\": 2684}, {\"accuracy\": 1.0, \"loss\": 0.00033839541720226407, \"time-step\": 2685}, {\"accuracy\": 1.0, \"loss\": 0.0003453534736763686, \"time-step\": 2686}, {\"accuracy\": 1.0, \"loss\": 0.0003381547867320478, \"time-step\": 2687}, {\"accuracy\": 1.0, \"loss\": 0.00034510635305196047, \"time-step\": 2688}, {\"accuracy\": 1.0, \"loss\": 0.00033791293390095234, \"time-step\": 2689}, {\"accuracy\": 1.0, \"loss\": 0.0003448511124588549, \"time-step\": 2690}, {\"accuracy\": 1.0, \"loss\": 0.00033765300759114325, \"time-step\": 2691}, {\"accuracy\": 1.0, \"loss\": 0.0003445912734605372, \"time-step\": 2692}, {\"accuracy\": 1.0, \"loss\": 0.000337398232659325, \"time-step\": 2693}, {\"accuracy\": 1.0, \"loss\": 0.000344337458955124, \"time-step\": 2694}, {\"accuracy\": 1.0, \"loss\": 0.0003371605998836458, \"time-step\": 2695}, {\"accuracy\": 1.0, \"loss\": 0.00034409904037602246, \"time-step\": 2696}, {\"accuracy\": 1.0, \"loss\": 0.00033691752469167113, \"time-step\": 2697}, {\"accuracy\": 1.0, \"loss\": 0.0003438429266680032, \"time-step\": 2698}, {\"accuracy\": 1.0, \"loss\": 0.0003366615856066346, \"time-step\": 2699}, {\"accuracy\": 1.0, \"loss\": 0.0003435871913097799, \"time-step\": 2700}, {\"accuracy\": 1.0, \"loss\": 0.0003364196454640478, \"time-step\": 2701}, {\"accuracy\": 1.0, \"loss\": 0.00034334350493736565, \"time-step\": 2702}, {\"accuracy\": 1.0, \"loss\": 0.00033617886947467923, \"time-step\": 2703}, {\"accuracy\": 1.0, \"loss\": 0.00034309810143895447, \"time-step\": 2704}, {\"accuracy\": 1.0, \"loss\": 0.00033592275576666, \"time-step\": 2705}, {\"accuracy\": 1.0, \"loss\": 0.00034282985143363476, \"time-step\": 2706}, {\"accuracy\": 1.0, \"loss\": 0.00033566454658284783, \"time-step\": 2707}, {\"accuracy\": 1.0, \"loss\": 0.00034256724757142365, \"time-step\": 2708}, {\"accuracy\": 1.0, \"loss\": 0.00033540549338795245, \"time-step\": 2709}, {\"accuracy\": 1.0, \"loss\": 0.0003423058078624308, \"time-step\": 2710}, {\"accuracy\": 1.0, \"loss\": 0.00033515237737447023, \"time-step\": 2711}, {\"accuracy\": 1.0, \"loss\": 0.0003420526336412877, \"time-step\": 2712}, {\"accuracy\": 1.0, \"loss\": 0.0003349029866512865, \"time-step\": 2713}, {\"accuracy\": 1.0, \"loss\": 0.0003417910193093121, \"time-step\": 2714}, {\"accuracy\": 1.0, \"loss\": 0.00033464550506323576, \"time-step\": 2715}, {\"accuracy\": 1.0, \"loss\": 0.0003415397659409791, \"time-step\": 2716}, {\"accuracy\": 1.0, \"loss\": 0.0003344118595123291, \"time-step\": 2717}, {\"accuracy\": 1.0, \"loss\": 0.0003413065569475293, \"time-step\": 2718}, {\"accuracy\": 1.0, \"loss\": 0.0003341767005622387, \"time-step\": 2719}, {\"accuracy\": 1.0, \"loss\": 0.0003410531207919121, \"time-step\": 2720}, {\"accuracy\": 1.0, \"loss\": 0.0003339202667120844, \"time-step\": 2721}, {\"accuracy\": 1.0, \"loss\": 0.0003407951444387436, \"time-step\": 2722}, {\"accuracy\": 1.0, \"loss\": 0.00033366301795467734, \"time-step\": 2723}, {\"accuracy\": 1.0, \"loss\": 0.00034052677801810205, \"time-step\": 2724}, {\"accuracy\": 1.0, \"loss\": 0.0003334033826831728, \"time-step\": 2725}, {\"accuracy\": 1.0, \"loss\": 0.0003402705187909305, \"time-step\": 2726}, {\"accuracy\": 1.0, \"loss\": 0.0003331588231958449, \"time-step\": 2727}, {\"accuracy\": 1.0, \"loss\": 0.00034003096516244113, \"time-step\": 2728}, {\"accuracy\": 1.0, \"loss\": 0.00033292252919636667, \"time-step\": 2729}, {\"accuracy\": 1.0, \"loss\": 0.00033978885039687157, \"time-step\": 2730}, {\"accuracy\": 1.0, \"loss\": 0.000332686526235193, \"time-step\": 2731}, {\"accuracy\": 1.0, \"loss\": 0.00033954193349927664, \"time-step\": 2732}, {\"accuracy\": 1.0, \"loss\": 0.0003324396093375981, \"time-step\": 2733}, {\"accuracy\": 1.0, \"loss\": 0.00033928948687389493, \"time-step\": 2734}, {\"accuracy\": 1.0, \"loss\": 0.0003321886179037392, \"time-step\": 2735}, {\"accuracy\": 1.0, \"loss\": 0.00033903957228176296, \"time-step\": 2736}, {\"accuracy\": 1.0, \"loss\": 0.0003319516545161605, \"time-step\": 2737}, {\"accuracy\": 1.0, \"loss\": 0.0003388000186532736, \"time-step\": 2738}, {\"accuracy\": 1.0, \"loss\": 0.00033171515678986907, \"time-step\": 2739}, {\"accuracy\": 1.0, \"loss\": 0.00033856084337458014, \"time-step\": 2740}, {\"accuracy\": 1.0, \"loss\": 0.0003314774949103594, \"time-step\": 2741}, {\"accuracy\": 1.0, \"loss\": 0.00033831497421488166, \"time-step\": 2742}, {\"accuracy\": 1.0, \"loss\": 0.00033122595050372183, \"time-step\": 2743}, {\"accuracy\": 1.0, \"loss\": 0.0003380531561560929, \"time-step\": 2744}, {\"accuracy\": 1.0, \"loss\": 0.0003309768799226731, \"time-step\": 2745}, {\"accuracy\": 1.0, \"loss\": 0.000337811594363302, \"time-step\": 2746}, {\"accuracy\": 1.0, \"loss\": 0.00033074014936573803, \"time-step\": 2747}, {\"accuracy\": 1.0, \"loss\": 0.0003375657834112644, \"time-step\": 2748}, {\"accuracy\": 1.0, \"loss\": 0.00033049783087335527, \"time-step\": 2749}, {\"accuracy\": 1.0, \"loss\": 0.00033731895382516086, \"time-step\": 2750}, {\"accuracy\": 1.0, \"loss\": 0.0003302532131783664, \"time-step\": 2751}, {\"accuracy\": 1.0, \"loss\": 0.00033707887632772326, \"time-step\": 2752}, {\"accuracy\": 1.0, \"loss\": 0.00033002448617480695, \"time-step\": 2753}, {\"accuracy\": 1.0, \"loss\": 0.0003368395264260471, \"time-step\": 2754}, {\"accuracy\": 1.0, \"loss\": 0.0003297908406239003, \"time-step\": 2755}, {\"accuracy\": 1.0, \"loss\": 0.00033660081680864096, \"time-step\": 2756}, {\"accuracy\": 1.0, \"loss\": 0.0003295537317171693, \"time-step\": 2757}, {\"accuracy\": 1.0, \"loss\": 0.0003363580326549709, \"time-step\": 2758}, {\"accuracy\": 1.0, \"loss\": 0.0003293087938800454, \"time-step\": 2759}, {\"accuracy\": 1.0, \"loss\": 0.000336113793309778, \"time-step\": 2760}, {\"accuracy\": 1.0, \"loss\": 0.00032907637069001794, \"time-step\": 2761}, {\"accuracy\": 1.0, \"loss\": 0.0003358760732226074, \"time-step\": 2762}, {\"accuracy\": 1.0, \"loss\": 0.0003288402804173529, \"time-step\": 2763}, {\"accuracy\": 1.0, \"loss\": 0.00033563494798727334, \"time-step\": 2764}, {\"accuracy\": 1.0, \"loss\": 0.0003286067221779376, \"time-step\": 2765}, {\"accuracy\": 1.0, \"loss\": 0.00033540104050189257, \"time-step\": 2766}, {\"accuracy\": 1.0, \"loss\": 0.0003283772384747863, \"time-step\": 2767}, {\"accuracy\": 1.0, \"loss\": 0.00033516663825139403, \"time-step\": 2768}, {\"accuracy\": 1.0, \"loss\": 0.0003281421377323568, \"time-step\": 2769}, {\"accuracy\": 1.0, \"loss\": 0.0003349328471813351, \"time-step\": 2770}, {\"accuracy\": 1.0, \"loss\": 0.00032791454577818513, \"time-step\": 2771}, {\"accuracy\": 1.0, \"loss\": 0.00033469664049334824, \"time-step\": 2772}, {\"accuracy\": 1.0, \"loss\": 0.00032769088284112513, \"time-step\": 2773}, {\"accuracy\": 1.0, \"loss\": 0.0003344723954796791, \"time-step\": 2774}, {\"accuracy\": 1.0, \"loss\": 0.00032746331999078393, \"time-step\": 2775}, {\"accuracy\": 1.0, \"loss\": 0.0003342358395457268, \"time-step\": 2776}, {\"accuracy\": 1.0, \"loss\": 0.00032723339973017573, \"time-step\": 2777}, {\"accuracy\": 1.0, \"loss\": 0.00033400009851902723, \"time-step\": 2778}, {\"accuracy\": 1.0, \"loss\": 0.00032698383438400924, \"time-step\": 2779}, {\"accuracy\": 1.0, \"loss\": 0.00033374634222127497, \"time-step\": 2780}, {\"accuracy\": 1.0, \"loss\": 0.0003267441061325371, \"time-step\": 2781}, {\"accuracy\": 1.0, \"loss\": 0.00033350609010085464, \"time-step\": 2782}, {\"accuracy\": 1.0, \"loss\": 0.0003265103150624782, \"time-step\": 2783}, {\"accuracy\": 1.0, \"loss\": 0.00033327267738059163, \"time-step\": 2784}, {\"accuracy\": 1.0, \"loss\": 0.00032628924236632884, \"time-step\": 2785}, {\"accuracy\": 1.0, \"loss\": 0.0003330457548145205, \"time-step\": 2786}, {\"accuracy\": 1.0, \"loss\": 0.00032605029991827905, \"time-step\": 2787}, {\"accuracy\": 1.0, \"loss\": 0.00033279458875767887, \"time-step\": 2788}, {\"accuracy\": 1.0, \"loss\": 0.0003258090582676232, \"time-step\": 2789}, {\"accuracy\": 1.0, \"loss\": 0.00033255040762014687, \"time-step\": 2790}, {\"accuracy\": 1.0, \"loss\": 0.00032557579106651247, \"time-step\": 2791}, {\"accuracy\": 1.0, \"loss\": 0.00033232569694519043, \"time-step\": 2792}, {\"accuracy\": 1.0, \"loss\": 0.0003253552131354809, \"time-step\": 2793}, {\"accuracy\": 1.0, \"loss\": 0.0003320992400404066, \"time-step\": 2794}, {\"accuracy\": 1.0, \"loss\": 0.00032513600308448076, \"time-step\": 2795}, {\"accuracy\": 1.0, \"loss\": 0.00033187438384629786, \"time-step\": 2796}, {\"accuracy\": 1.0, \"loss\": 0.0003249129222240299, \"time-step\": 2797}, {\"accuracy\": 1.0, \"loss\": 0.0003316433576401323, \"time-step\": 2798}, {\"accuracy\": 1.0, \"loss\": 0.00032467671553604305, \"time-step\": 2799}, {\"accuracy\": 1.0, \"loss\": 0.00033140263985842466, \"time-step\": 2800}, {\"accuracy\": 1.0, \"loss\": 0.00032444100361317396, \"time-step\": 2801}, {\"accuracy\": 1.0, \"loss\": 0.00033116841223090887, \"time-step\": 2802}, {\"accuracy\": 1.0, \"loss\": 0.00032422030926682055, \"time-step\": 2803}, {\"accuracy\": 1.0, \"loss\": 0.0003309399471618235, \"time-step\": 2804}, {\"accuracy\": 1.0, \"loss\": 0.00032399455085396767, \"time-step\": 2805}, {\"accuracy\": 1.0, \"loss\": 0.00033071491634473205, \"time-step\": 2806}, {\"accuracy\": 1.0, \"loss\": 0.0003237690543755889, \"time-step\": 2807}, {\"accuracy\": 1.0, \"loss\": 0.00033047545002773404, \"time-step\": 2808}, {\"accuracy\": 1.0, \"loss\": 0.0003235278418287635, \"time-step\": 2809}, {\"accuracy\": 1.0, \"loss\": 0.0003302359546069056, \"time-step\": 2810}, {\"accuracy\": 1.0, \"loss\": 0.0003233003953937441, \"time-step\": 2811}, {\"accuracy\": 1.0, \"loss\": 0.0003300061507616192, \"time-step\": 2812}, {\"accuracy\": 1.0, \"loss\": 0.0003230763541068882, \"time-step\": 2813}, {\"accuracy\": 1.0, \"loss\": 0.0003297819057479501, \"time-step\": 2814}, {\"accuracy\": 1.0, \"loss\": 0.0003228569694329053, \"time-step\": 2815}, {\"accuracy\": 1.0, \"loss\": 0.0003295507049188018, \"time-step\": 2816}, {\"accuracy\": 1.0, \"loss\": 0.0003226231783628464, \"time-step\": 2817}, {\"accuracy\": 1.0, \"loss\": 0.0003293205227237195, \"time-step\": 2818}, {\"accuracy\": 1.0, \"loss\": 0.00032240949803963304, \"time-step\": 2819}, {\"accuracy\": 1.0, \"loss\": 0.00032910151639953256, \"time-step\": 2820}, {\"accuracy\": 1.0, \"loss\": 0.00032218877458944917, \"time-step\": 2821}, {\"accuracy\": 1.0, \"loss\": 0.0003288796287961304, \"time-step\": 2822}, {\"accuracy\": 1.0, \"loss\": 0.0003219718055333942, \"time-step\": 2823}, {\"accuracy\": 1.0, \"loss\": 0.00032866321271285415, \"time-step\": 2824}, {\"accuracy\": 1.0, \"loss\": 0.0003217603953089565, \"time-step\": 2825}, {\"accuracy\": 1.0, \"loss\": 0.0003284435370005667, \"time-step\": 2826}, {\"accuracy\": 1.0, \"loss\": 0.0003215378965251148, \"time-step\": 2827}, {\"accuracy\": 1.0, \"loss\": 0.0003282069810666144, \"time-step\": 2828}, {\"accuracy\": 1.0, \"loss\": 0.0003213070740457624, \"time-step\": 2829}, {\"accuracy\": 1.0, \"loss\": 0.00032798657775856555, \"time-step\": 2830}, {\"accuracy\": 1.0, \"loss\": 0.0003210902214050293, \"time-step\": 2831}, {\"accuracy\": 1.0, \"loss\": 0.0003277537180110812, \"time-step\": 2832}, {\"accuracy\": 1.0, \"loss\": 0.0003208619891665876, \"time-step\": 2833}, {\"accuracy\": 1.0, \"loss\": 0.0003275240887887776, \"time-step\": 2834}, {\"accuracy\": 1.0, \"loss\": 0.00032063599792309105, \"time-step\": 2835}, {\"accuracy\": 1.0, \"loss\": 0.0003272953908890486, \"time-step\": 2836}, {\"accuracy\": 1.0, \"loss\": 0.0003204189706593752, \"time-step\": 2837}, {\"accuracy\": 1.0, \"loss\": 0.0003270785673521459, \"time-step\": 2838}, {\"accuracy\": 1.0, \"loss\": 0.0003201956278644502, \"time-step\": 2839}, {\"accuracy\": 1.0, \"loss\": 0.00032684224424883723, \"time-step\": 2840}, {\"accuracy\": 1.0, \"loss\": 0.0003199704806320369, \"time-step\": 2841}, {\"accuracy\": 1.0, \"loss\": 0.00032662335433997214, \"time-step\": 2842}, {\"accuracy\": 1.0, \"loss\": 0.0003197567129973322, \"time-step\": 2843}, {\"accuracy\": 1.0, \"loss\": 0.0003263982362113893, \"time-step\": 2844}, {\"accuracy\": 1.0, \"loss\": 0.00031952722929418087, \"time-step\": 2845}, {\"accuracy\": 1.0, \"loss\": 0.0003261665115132928, \"time-step\": 2846}, {\"accuracy\": 1.0, \"loss\": 0.0003193060401827097, \"time-step\": 2847}, {\"accuracy\": 1.0, \"loss\": 0.0003259487566538155, \"time-step\": 2848}, {\"accuracy\": 1.0, \"loss\": 0.0003190921852365136, \"time-step\": 2849}, {\"accuracy\": 1.0, \"loss\": 0.00032573071075603366, \"time-step\": 2850}, {\"accuracy\": 1.0, \"loss\": 0.00031888176454231143, \"time-step\": 2851}, {\"accuracy\": 1.0, \"loss\": 0.0003255091141909361, \"time-step\": 2852}, {\"accuracy\": 1.0, \"loss\": 0.0003186606918461621, \"time-step\": 2853}, {\"accuracy\": 1.0, \"loss\": 0.00032528661540709436, \"time-step\": 2854}, {\"accuracy\": 1.0, \"loss\": 0.00031844136537984014, \"time-step\": 2855}, {\"accuracy\": 1.0, \"loss\": 0.0003250699955970049, \"time-step\": 2856}, {\"accuracy\": 1.0, \"loss\": 0.00031822919845581055, \"time-step\": 2857}, {\"accuracy\": 1.0, \"loss\": 0.00032486004056409, \"time-step\": 2858}, {\"accuracy\": 1.0, \"loss\": 0.00031802713056094944, \"time-step\": 2859}, {\"accuracy\": 1.0, \"loss\": 0.0003246406267862767, \"time-step\": 2860}, {\"accuracy\": 1.0, \"loss\": 0.0003178074839524925, \"time-step\": 2861}, {\"accuracy\": 1.0, \"loss\": 0.0003244207182433456, \"time-step\": 2862}, {\"accuracy\": 1.0, \"loss\": 0.0003175918245688081, \"time-step\": 2863}, {\"accuracy\": 1.0, \"loss\": 0.0003242000821046531, \"time-step\": 2864}, {\"accuracy\": 1.0, \"loss\": 0.0003173733130097389, \"time-step\": 2865}, {\"accuracy\": 1.0, \"loss\": 0.0003239851212128997, \"time-step\": 2866}, {\"accuracy\": 1.0, \"loss\": 0.00031716455123387277, \"time-step\": 2867}, {\"accuracy\": 1.0, \"loss\": 0.00032376698800362647, \"time-step\": 2868}, {\"accuracy\": 1.0, \"loss\": 0.0003169431001879275, \"time-step\": 2869}, {\"accuracy\": 1.0, \"loss\": 0.0003235398617107421, \"time-step\": 2870}, {\"accuracy\": 1.0, \"loss\": 0.0003167228715028614, \"time-step\": 2871}, {\"accuracy\": 1.0, \"loss\": 0.0003233202442061156, \"time-step\": 2872}, {\"accuracy\": 1.0, \"loss\": 0.0003165144589729607, \"time-step\": 2873}, {\"accuracy\": 1.0, \"loss\": 0.00032311066752299666, \"time-step\": 2874}, {\"accuracy\": 1.0, \"loss\": 0.0003163080255035311, \"time-step\": 2875}, {\"accuracy\": 1.0, \"loss\": 0.00032289879163727164, \"time-step\": 2876}, {\"accuracy\": 1.0, \"loss\": 0.00031609932193532586, \"time-step\": 2877}, {\"accuracy\": 1.0, \"loss\": 0.00032268461654894054, \"time-step\": 2878}, {\"accuracy\": 1.0, \"loss\": 0.00031588826095685363, \"time-step\": 2879}, {\"accuracy\": 1.0, \"loss\": 0.0003224771353416145, \"time-step\": 2880}, {\"accuracy\": 1.0, \"loss\": 0.00031569701968692243, \"time-step\": 2881}, {\"accuracy\": 1.0, \"loss\": 0.00032228371128439903, \"time-step\": 2882}, {\"accuracy\": 1.0, \"loss\": 0.00031548968399874866, \"time-step\": 2883}, {\"accuracy\": 1.0, \"loss\": 0.00032206150353886187, \"time-step\": 2884}, {\"accuracy\": 1.0, \"loss\": 0.0003152716381009668, \"time-step\": 2885}, {\"accuracy\": 1.0, \"loss\": 0.0003218412457499653, \"time-step\": 2886}, {\"accuracy\": 1.0, \"loss\": 0.00031504881917499006, \"time-step\": 2887}, {\"accuracy\": 1.0, \"loss\": 0.0003216161858290434, \"time-step\": 2888}, {\"accuracy\": 1.0, \"loss\": 0.0003148392424918711, \"time-step\": 2889}, {\"accuracy\": 1.0, \"loss\": 0.00032140404800884426, \"time-step\": 2890}, {\"accuracy\": 1.0, \"loss\": 0.00031463016057386994, \"time-step\": 2891}, {\"accuracy\": 1.0, \"loss\": 0.00032118329545482993, \"time-step\": 2892}, {\"accuracy\": 1.0, \"loss\": 0.00031441188184544444, \"time-step\": 2893}, {\"accuracy\": 1.0, \"loss\": 0.0003209710121154785, \"time-step\": 2894}, {\"accuracy\": 1.0, \"loss\": 0.0003141968627460301, \"time-step\": 2895}, {\"accuracy\": 1.0, \"loss\": 0.00032074583577923477, \"time-step\": 2896}, {\"accuracy\": 1.0, \"loss\": 0.0003139778273180127, \"time-step\": 2897}, {\"accuracy\": 1.0, \"loss\": 0.00032052770256996155, \"time-step\": 2898}, {\"accuracy\": 1.0, \"loss\": 0.0003137717430945486, \"time-step\": 2899}, {\"accuracy\": 1.0, \"loss\": 0.0003203192609362304, \"time-step\": 2900}, {\"accuracy\": 1.0, \"loss\": 0.0003135672304779291, \"time-step\": 2901}, {\"accuracy\": 1.0, \"loss\": 0.0003201115468982607, \"time-step\": 2902}, {\"accuracy\": 1.0, \"loss\": 0.000313358788844198, \"time-step\": 2903}, {\"accuracy\": 1.0, \"loss\": 0.00031989702256396413, \"time-step\": 2904}, {\"accuracy\": 1.0, \"loss\": 0.00031315264641307294, \"time-step\": 2905}, {\"accuracy\": 1.0, \"loss\": 0.00031968214898370206, \"time-step\": 2906}, {\"accuracy\": 1.0, \"loss\": 0.00031293334905058146, \"time-step\": 2907}, {\"accuracy\": 1.0, \"loss\": 0.00031946523813530803, \"time-step\": 2908}, {\"accuracy\": 1.0, \"loss\": 0.0003127359668724239, \"time-step\": 2909}, {\"accuracy\": 1.0, \"loss\": 0.0003192732692696154, \"time-step\": 2910}, {\"accuracy\": 1.0, \"loss\": 0.0003125403309240937, \"time-step\": 2911}, {\"accuracy\": 1.0, \"loss\": 0.0003190627321600914, \"time-step\": 2912}, {\"accuracy\": 1.0, \"loss\": 0.0003123321512248367, \"time-step\": 2913}, {\"accuracy\": 1.0, \"loss\": 0.00031884913914836943, \"time-step\": 2914}, {\"accuracy\": 1.0, \"loss\": 0.00031211553141474724, \"time-step\": 2915}, {\"accuracy\": 1.0, \"loss\": 0.00031863251933827996, \"time-step\": 2916}, {\"accuracy\": 1.0, \"loss\": 0.0003119041211903095, \"time-step\": 2917}, {\"accuracy\": 1.0, \"loss\": 0.00031840894371271133, \"time-step\": 2918}, {\"accuracy\": 1.0, \"loss\": 0.00031168447458185256, \"time-step\": 2919}, {\"accuracy\": 1.0, \"loss\": 0.00031820483854971826, \"time-step\": 2920}, {\"accuracy\": 1.0, \"loss\": 0.00031149451388046145, \"time-step\": 2921}, {\"accuracy\": 1.0, \"loss\": 0.0003179999184794724, \"time-step\": 2922}, {\"accuracy\": 1.0, \"loss\": 0.0003112877020612359, \"time-step\": 2923}, {\"accuracy\": 1.0, \"loss\": 0.0003177897888235748, \"time-step\": 2924}, {\"accuracy\": 1.0, \"loss\": 0.00031108560506254435, \"time-step\": 2925}, {\"accuracy\": 1.0, \"loss\": 0.0003175921447109431, \"time-step\": 2926}, {\"accuracy\": 1.0, \"loss\": 0.0003108925302512944, \"time-step\": 2927}, {\"accuracy\": 1.0, \"loss\": 0.0003173900186084211, \"time-step\": 2928}, {\"accuracy\": 1.0, \"loss\": 0.0003106905205640942, \"time-step\": 2929}, {\"accuracy\": 1.0, \"loss\": 0.0003171861753799021, \"time-step\": 2930}, {\"accuracy\": 1.0, \"loss\": 0.00031049217795953155, \"time-step\": 2931}, {\"accuracy\": 1.0, \"loss\": 0.00031697668600827456, \"time-step\": 2932}, {\"accuracy\": 1.0, \"loss\": 0.00031028006924316287, \"time-step\": 2933}, {\"accuracy\": 1.0, \"loss\": 0.000316763500450179, \"time-step\": 2934}, {\"accuracy\": 1.0, \"loss\": 0.0003100742760580033, \"time-step\": 2935}, {\"accuracy\": 1.0, \"loss\": 0.0003165602684020996, \"time-step\": 2936}, {\"accuracy\": 1.0, \"loss\": 0.00030987904756329954, \"time-step\": 2937}, {\"accuracy\": 1.0, \"loss\": 0.00031635802588425577, \"time-step\": 2938}, {\"accuracy\": 1.0, \"loss\": 0.00030967057682573795, \"time-step\": 2939}, {\"accuracy\": 1.0, \"loss\": 0.0003161472559440881, \"time-step\": 2940}, {\"accuracy\": 1.0, \"loss\": 0.0003094700223300606, \"time-step\": 2941}, {\"accuracy\": 1.0, \"loss\": 0.00031594547908753157, \"time-step\": 2942}, {\"accuracy\": 1.0, \"loss\": 0.0003092835540883243, \"time-step\": 2943}, {\"accuracy\": 1.0, \"loss\": 0.0003157605533488095, \"time-step\": 2944}, {\"accuracy\": 1.0, \"loss\": 0.0003090951358899474, \"time-step\": 2945}, {\"accuracy\": 1.0, \"loss\": 0.0003155602025799453, \"time-step\": 2946}, {\"accuracy\": 1.0, \"loss\": 0.00030889498884789646, \"time-step\": 2947}, {\"accuracy\": 1.0, \"loss\": 0.0003153626748826355, \"time-step\": 2948}, {\"accuracy\": 1.0, \"loss\": 0.0003087010991293937, \"time-step\": 2949}, {\"accuracy\": 1.0, \"loss\": 0.00031516526360064745, \"time-step\": 2950}, {\"accuracy\": 1.0, \"loss\": 0.0003085098578594625, \"time-step\": 2951}, {\"accuracy\": 1.0, \"loss\": 0.0003149651747662574, \"time-step\": 2952}, {\"accuracy\": 1.0, \"loss\": 0.0003083058400079608, \"time-step\": 2953}, {\"accuracy\": 1.0, \"loss\": 0.00031474974821321666, \"time-step\": 2954}, {\"accuracy\": 1.0, \"loss\": 0.00030809780582785606, \"time-step\": 2955}, {\"accuracy\": 1.0, \"loss\": 0.0003145443624816835, \"time-step\": 2956}, {\"accuracy\": 1.0, \"loss\": 0.00030789372976869345, \"time-step\": 2957}, {\"accuracy\": 1.0, \"loss\": 0.00031434156699106097, \"time-step\": 2958}, {\"accuracy\": 1.0, \"loss\": 0.00030770309967920184, \"time-step\": 2959}, {\"accuracy\": 1.0, \"loss\": 0.00031414025579579175, \"time-step\": 2960}, {\"accuracy\": 1.0, \"loss\": 0.0003074901469517499, \"time-step\": 2961}, {\"accuracy\": 1.0, \"loss\": 0.0003139286709483713, \"time-step\": 2962}, {\"accuracy\": 1.0, \"loss\": 0.00030730036087334156, \"time-step\": 2963}, {\"accuracy\": 1.0, \"loss\": 0.00031373187084682286, \"time-step\": 2964}, {\"accuracy\": 1.0, \"loss\": 0.0003071021055802703, \"time-step\": 2965}, {\"accuracy\": 1.0, \"loss\": 0.0003135405422654003, \"time-step\": 2966}, {\"accuracy\": 1.0, \"loss\": 0.0003069150843657553, \"time-step\": 2967}, {\"accuracy\": 1.0, \"loss\": 0.00031334481900557876, \"time-step\": 2968}, {\"accuracy\": 1.0, \"loss\": 0.00030671950662508607, \"time-step\": 2969}, {\"accuracy\": 1.0, \"loss\": 0.00031313864747062325, \"time-step\": 2970}, {\"accuracy\": 1.0, \"loss\": 0.00030651772976852953, \"time-step\": 2971}, {\"accuracy\": 1.0, \"loss\": 0.00031293422216549516, \"time-step\": 2972}, {\"accuracy\": 1.0, \"loss\": 0.00030631470144726336, \"time-step\": 2973}, {\"accuracy\": 1.0, \"loss\": 0.00031274149660021067, \"time-step\": 2974}, {\"accuracy\": 1.0, \"loss\": 0.0003061293391510844, \"time-step\": 2975}, {\"accuracy\": 1.0, \"loss\": 0.0003125387884210795, \"time-step\": 2976}, {\"accuracy\": 1.0, \"loss\": 0.00030593015253543854, \"time-step\": 2977}, {\"accuracy\": 1.0, \"loss\": 0.00031234684865921736, \"time-step\": 2978}, {\"accuracy\": 1.0, \"loss\": 0.0003057390567846596, \"time-step\": 2979}, {\"accuracy\": 1.0, \"loss\": 0.00031214661430567503, \"time-step\": 2980}, {\"accuracy\": 1.0, \"loss\": 0.00030554475961253047, \"time-step\": 2981}, {\"accuracy\": 1.0, \"loss\": 0.0003119531029369682, \"time-step\": 2982}, {\"accuracy\": 1.0, \"loss\": 0.0003053591644857079, \"time-step\": 2983}, {\"accuracy\": 1.0, \"loss\": 0.0003117602609563619, \"time-step\": 2984}, {\"accuracy\": 1.0, \"loss\": 0.00030516114202328026, \"time-step\": 2985}, {\"accuracy\": 1.0, \"loss\": 0.000311564770527184, \"time-step\": 2986}, {\"accuracy\": 1.0, \"loss\": 0.000304968940326944, \"time-step\": 2987}, {\"accuracy\": 1.0, \"loss\": 0.0003113600250799209, \"time-step\": 2988}, {\"accuracy\": 1.0, \"loss\": 0.0003047721111215651, \"time-step\": 2989}, {\"accuracy\": 1.0, \"loss\": 0.0003111652913503349, \"time-step\": 2990}, {\"accuracy\": 1.0, \"loss\": 0.0003045781049877405, \"time-step\": 2991}, {\"accuracy\": 1.0, \"loss\": 0.00031097058672457933, \"time-step\": 2992}, {\"accuracy\": 1.0, \"loss\": 0.00030439786496572196, \"time-step\": 2993}, {\"accuracy\": 1.0, \"loss\": 0.00031078443862497807, \"time-step\": 2994}, {\"accuracy\": 1.0, \"loss\": 0.00030420886469073594, \"time-step\": 2995}, {\"accuracy\": 1.0, \"loss\": 0.00031059017055667937, \"time-step\": 2996}, {\"accuracy\": 1.0, \"loss\": 0.00030401168623939157, \"time-step\": 2997}, {\"accuracy\": 1.0, \"loss\": 0.00031038341694511473, \"time-step\": 2998}, {\"accuracy\": 1.0, \"loss\": 0.0003038091235794127, \"time-step\": 2999}, {\"accuracy\": 1.0, \"loss\": 0.00031018813024275005, \"time-step\": 3000}, {\"accuracy\": 1.0, \"loss\": 0.00030362512916326523, \"time-step\": 3001}, {\"accuracy\": 1.0, \"loss\": 0.0003099956375081092, \"time-step\": 3002}, {\"accuracy\": 1.0, \"loss\": 0.0003034390974789858, \"time-step\": 3003}, {\"accuracy\": 1.0, \"loss\": 0.0003098086453974247, \"time-step\": 3004}, {\"accuracy\": 1.0, \"loss\": 0.00030325210536830127, \"time-step\": 3005}, {\"accuracy\": 1.0, \"loss\": 0.00030961749143898487, \"time-step\": 3006}, {\"accuracy\": 1.0, \"loss\": 0.00030306409462355077, \"time-step\": 3007}, {\"accuracy\": 1.0, \"loss\": 0.0003094285784754902, \"time-step\": 3008}, {\"accuracy\": 1.0, \"loss\": 0.00030287832487374544, \"time-step\": 3009}, {\"accuracy\": 1.0, \"loss\": 0.000309233320876956, \"time-step\": 3010}, {\"accuracy\": 1.0, \"loss\": 0.00030267401598393917, \"time-step\": 3011}, {\"accuracy\": 1.0, \"loss\": 0.0003090254031121731, \"time-step\": 3012}, {\"accuracy\": 1.0, \"loss\": 0.00030248513212427497, \"time-step\": 3013}, {\"accuracy\": 1.0, \"loss\": 0.00030883989529684186, \"time-step\": 3014}, {\"accuracy\": 1.0, \"loss\": 0.0003022997407242656, \"time-step\": 3015}, {\"accuracy\": 1.0, \"loss\": 0.0003086520009674132, \"time-step\": 3016}, {\"accuracy\": 1.0, \"loss\": 0.00030211813282221556, \"time-step\": 3017}, {\"accuracy\": 1.0, \"loss\": 0.00030846602749079466, \"time-step\": 3018}, {\"accuracy\": 1.0, \"loss\": 0.00030192601843737066, \"time-step\": 3019}, {\"accuracy\": 1.0, \"loss\": 0.00030826363945379853, \"time-step\": 3020}, {\"accuracy\": 1.0, \"loss\": 0.0003017330600414425, \"time-step\": 3021}, {\"accuracy\": 1.0, \"loss\": 0.0003080695460084826, \"time-step\": 3022}, {\"accuracy\": 1.0, \"loss\": 0.0003015464171767235, \"time-step\": 3023}, {\"accuracy\": 1.0, \"loss\": 0.0003078787703998387, \"time-step\": 3024}, {\"accuracy\": 1.0, \"loss\": 0.0003013498499058187, \"time-step\": 3025}, {\"accuracy\": 1.0, \"loss\": 0.00030768202850595117, \"time-step\": 3026}, {\"accuracy\": 1.0, \"loss\": 0.00030116699053905904, \"time-step\": 3027}, {\"accuracy\": 1.0, \"loss\": 0.00030750353471376, \"time-step\": 3028}, {\"accuracy\": 1.0, \"loss\": 0.00030099041759967804, \"time-step\": 3029}, {\"accuracy\": 1.0, \"loss\": 0.00030731502920389175, \"time-step\": 3030}, {\"accuracy\": 1.0, \"loss\": 0.00030080153374001384, \"time-step\": 3031}, {\"accuracy\": 1.0, \"loss\": 0.0003071338578592986, \"time-step\": 3032}, {\"accuracy\": 1.0, \"loss\": 0.0003006233600899577, \"time-step\": 3033}, {\"accuracy\": 1.0, \"loss\": 0.0003069468366447836, \"time-step\": 3034}, {\"accuracy\": 1.0, \"loss\": 0.00030044279992580414, \"time-step\": 3035}, {\"accuracy\": 1.0, \"loss\": 0.00030676196911372244, \"time-step\": 3036}, {\"accuracy\": 1.0, \"loss\": 0.00030026613967493176, \"time-step\": 3037}, {\"accuracy\": 1.0, \"loss\": 0.0003065805067308247, \"time-step\": 3038}, {\"accuracy\": 1.0, \"loss\": 0.0003000847063958645, \"time-step\": 3039}, {\"accuracy\": 1.0, \"loss\": 0.00030639200122095644, \"time-step\": 3040}, {\"accuracy\": 1.0, \"loss\": 0.00029989631730131805, \"time-step\": 3041}, {\"accuracy\": 1.0, \"loss\": 0.00030620608595199883, \"time-step\": 3042}, {\"accuracy\": 1.0, \"loss\": 0.000299718085443601, \"time-step\": 3043}, {\"accuracy\": 1.0, \"loss\": 0.00030601726029999554, \"time-step\": 3044}, {\"accuracy\": 1.0, \"loss\": 0.0002995242248289287, \"time-step\": 3045}, {\"accuracy\": 1.0, \"loss\": 0.0003058168513234705, \"time-step\": 3046}, {\"accuracy\": 1.0, \"loss\": 0.0002993276866618544, \"time-step\": 3047}, {\"accuracy\": 1.0, \"loss\": 0.00030562179745174944, \"time-step\": 3048}, {\"accuracy\": 1.0, \"loss\": 0.00029913982143625617, \"time-step\": 3049}, {\"accuracy\": 1.0, \"loss\": 0.0003054377157241106, \"time-step\": 3050}, {\"accuracy\": 1.0, \"loss\": 0.0002989693311974406, \"time-step\": 3051}, {\"accuracy\": 1.0, \"loss\": 0.0003052696993108839, \"time-step\": 3052}, {\"accuracy\": 1.0, \"loss\": 0.0002987981715705246, \"time-step\": 3053}, {\"accuracy\": 1.0, \"loss\": 0.0003050839004572481, \"time-step\": 3054}, {\"accuracy\": 1.0, \"loss\": 0.0002986164763569832, \"time-step\": 3055}, {\"accuracy\": 1.0, \"loss\": 0.0003049008082598448, \"time-step\": 3056}, {\"accuracy\": 1.0, \"loss\": 0.0002984278544317931, \"time-step\": 3057}, {\"accuracy\": 1.0, \"loss\": 0.00030471186619251966, \"time-step\": 3058}, {\"accuracy\": 1.0, \"loss\": 0.00029825602541677654, \"time-step\": 3059}, {\"accuracy\": 1.0, \"loss\": 0.0003045324992854148, \"time-step\": 3060}, {\"accuracy\": 1.0, \"loss\": 0.0002980672288686037, \"time-step\": 3061}, {\"accuracy\": 1.0, \"loss\": 0.00030434061773121357, \"time-step\": 3062}, {\"accuracy\": 1.0, \"loss\": 0.000297894817776978, \"time-step\": 3063}, {\"accuracy\": 1.0, \"loss\": 0.0003041706222575158, \"time-step\": 3064}, {\"accuracy\": 1.0, \"loss\": 0.0002977148396894336, \"time-step\": 3065}, {\"accuracy\": 1.0, \"loss\": 0.00030398386297747493, \"time-step\": 3066}, {\"accuracy\": 1.0, \"loss\": 0.00029753486160188913, \"time-step\": 3067}, {\"accuracy\": 1.0, \"loss\": 0.00030379684176295996, \"time-step\": 3068}, {\"accuracy\": 1.0, \"loss\": 0.0002973423688672483, \"time-step\": 3069}, {\"accuracy\": 1.0, \"loss\": 0.0003036059788428247, \"time-step\": 3070}, {\"accuracy\": 1.0, \"loss\": 0.000297163991490379, \"time-step\": 3071}, {\"accuracy\": 1.0, \"loss\": 0.0003034295223187655, \"time-step\": 3072}, {\"accuracy\": 1.0, \"loss\": 0.00029698468279093504, \"time-step\": 3073}, {\"accuracy\": 1.0, \"loss\": 0.00030323234386742115, \"time-step\": 3074}, {\"accuracy\": 1.0, \"loss\": 0.0002968006592709571, \"time-step\": 3075}, {\"accuracy\": 1.0, \"loss\": 0.00030305093969218433, \"time-step\": 3076}, {\"accuracy\": 1.0, \"loss\": 0.0002966181782539934, \"time-step\": 3077}, {\"accuracy\": 1.0, \"loss\": 0.0003028700884897262, \"time-step\": 3078}, {\"accuracy\": 1.0, \"loss\": 0.00029643316520377994, \"time-step\": 3079}, {\"accuracy\": 1.0, \"loss\": 0.0003026789636351168, \"time-step\": 3080}, {\"accuracy\": 1.0, \"loss\": 0.00029625429306179285, \"time-step\": 3081}, {\"accuracy\": 1.0, \"loss\": 0.00030250055715441704, \"time-step\": 3082}, {\"accuracy\": 1.0, \"loss\": 0.00029608572367578745, \"time-step\": 3083}, {\"accuracy\": 1.0, \"loss\": 0.00030232963035814464, \"time-step\": 3084}, {\"accuracy\": 1.0, \"loss\": 0.000295907782856375, \"time-step\": 3085}, {\"accuracy\": 1.0, \"loss\": 0.0003021450829692185, \"time-step\": 3086}, {\"accuracy\": 1.0, \"loss\": 0.0002957332180812955, \"time-step\": 3087}, {\"accuracy\": 1.0, \"loss\": 0.0003019685100298375, \"time-step\": 3088}, {\"accuracy\": 1.0, \"loss\": 0.00029555370565503836, \"time-step\": 3089}, {\"accuracy\": 1.0, \"loss\": 0.00030178940505720675, \"time-step\": 3090}, {\"accuracy\": 1.0, \"loss\": 0.0002953906077891588, \"time-step\": 3091}, {\"accuracy\": 1.0, \"loss\": 0.00030161472386680543, \"time-step\": 3092}, {\"accuracy\": 1.0, \"loss\": 0.00029520364478230476, \"time-step\": 3093}, {\"accuracy\": 1.0, \"loss\": 0.0003014299145434052, \"time-step\": 3094}, {\"accuracy\": 1.0, \"loss\": 0.00029503466794267297, \"time-step\": 3095}, {\"accuracy\": 1.0, \"loss\": 0.00030125316698104143, \"time-step\": 3096}, {\"accuracy\": 1.0, \"loss\": 0.0002948510809801519, \"time-step\": 3097}, {\"accuracy\": 1.0, \"loss\": 0.0003010786313097924, \"time-step\": 3098}, {\"accuracy\": 1.0, \"loss\": 0.0002946944732684642, \"time-step\": 3099}, {\"accuracy\": 1.0, \"loss\": 0.00030091102235019207, \"time-step\": 3100}, {\"accuracy\": 1.0, \"loss\": 0.0002945179003290832, \"time-step\": 3101}, {\"accuracy\": 1.0, \"loss\": 0.00030072659137658775, \"time-step\": 3102}, {\"accuracy\": 1.0, \"loss\": 0.000294331053737551, \"time-step\": 3103}, {\"accuracy\": 1.0, \"loss\": 0.0003005364560522139, \"time-step\": 3104}, {\"accuracy\": 1.0, \"loss\": 0.000294155819574371, \"time-step\": 3105}, {\"accuracy\": 1.0, \"loss\": 0.0003003604360856116, \"time-step\": 3106}, {\"accuracy\": 1.0, \"loss\": 0.00029397665639407933, \"time-step\": 3107}, {\"accuracy\": 1.0, \"loss\": 0.0003001725999638438, \"time-step\": 3108}, {\"accuracy\": 1.0, \"loss\": 0.00029379368061199784, \"time-step\": 3109}, {\"accuracy\": 1.0, \"loss\": 0.0002999906719196588, \"time-step\": 3110}, {\"accuracy\": 1.0, \"loss\": 0.00029361792257986963, \"time-step\": 3111}, {\"accuracy\": 1.0, \"loss\": 0.00029981162515468895, \"time-step\": 3112}, {\"accuracy\": 1.0, \"loss\": 0.0002934419608209282, \"time-step\": 3113}, {\"accuracy\": 1.0, \"loss\": 0.00029963403358124197, \"time-step\": 3114}, {\"accuracy\": 1.0, \"loss\": 0.0002932631177827716, \"time-step\": 3115}, {\"accuracy\": 1.0, \"loss\": 0.0002994452661368996, \"time-step\": 3116}, {\"accuracy\": 1.0, \"loss\": 0.00029308104421943426, \"time-step\": 3117}, {\"accuracy\": 1.0, \"loss\": 0.0002992669469676912, \"time-step\": 3118}, {\"accuracy\": 1.0, \"loss\": 0.00029290636302903295, \"time-step\": 3119}, {\"accuracy\": 1.0, \"loss\": 0.000299098581308499, \"time-step\": 3120}, {\"accuracy\": 1.0, \"loss\": 0.0002927514142356813, \"time-step\": 3121}, {\"accuracy\": 1.0, \"loss\": 0.00029894179897382855, \"time-step\": 3122}, {\"accuracy\": 1.0, \"loss\": 0.00029259774601086974, \"time-step\": 3123}, {\"accuracy\": 1.0, \"loss\": 0.00029877983615733683, \"time-step\": 3124}, {\"accuracy\": 1.0, \"loss\": 0.0002924319705925882, \"time-step\": 3125}, {\"accuracy\": 1.0, \"loss\": 0.000298605824355036, \"time-step\": 3126}, {\"accuracy\": 1.0, \"loss\": 0.0002922530402429402, \"time-step\": 3127}, {\"accuracy\": 1.0, \"loss\": 0.00029842182993888855, \"time-step\": 3128}, {\"accuracy\": 1.0, \"loss\": 0.00029207931947894394, \"time-step\": 3129}, {\"accuracy\": 1.0, \"loss\": 0.00029825163073837757, \"time-step\": 3130}, {\"accuracy\": 1.0, \"loss\": 0.00029190274653956294, \"time-step\": 3131}, {\"accuracy\": 1.0, \"loss\": 0.00029806658858433366, \"time-step\": 3132}, {\"accuracy\": 1.0, \"loss\": 0.00029173598159104586, \"time-step\": 3133}, {\"accuracy\": 1.0, \"loss\": 0.0002979006676468998, \"time-step\": 3134}, {\"accuracy\": 1.0, \"loss\": 0.00029156560776755214, \"time-step\": 3135}, {\"accuracy\": 1.0, \"loss\": 0.00029773206915706396, \"time-step\": 3136}, {\"accuracy\": 1.0, \"loss\": 0.00029140751576051116, \"time-step\": 3137}, {\"accuracy\": 1.0, \"loss\": 0.0002975693787448108, \"time-step\": 3138}, {\"accuracy\": 1.0, \"loss\": 0.00029124750290066004, \"time-step\": 3139}, {\"accuracy\": 1.0, \"loss\": 0.0002974008093588054, \"time-step\": 3140}, {\"accuracy\": 1.0, \"loss\": 0.0002910762559622526, \"time-step\": 3141}, {\"accuracy\": 1.0, \"loss\": 0.00029722379986196756, \"time-step\": 3142}, {\"accuracy\": 1.0, \"loss\": 0.00029089473537169397, \"time-step\": 3143}, {\"accuracy\": 1.0, \"loss\": 0.000297042541205883, \"time-step\": 3144}, {\"accuracy\": 1.0, \"loss\": 0.00029072194593027234, \"time-step\": 3145}, {\"accuracy\": 1.0, \"loss\": 0.0002968631742987782, \"time-step\": 3146}, {\"accuracy\": 1.0, \"loss\": 0.0002905552682932466, \"time-step\": 3147}, {\"accuracy\": 1.0, \"loss\": 0.00029669678770005703, \"time-step\": 3148}, {\"accuracy\": 1.0, \"loss\": 0.0002903811982832849, \"time-step\": 3149}, {\"accuracy\": 1.0, \"loss\": 0.0002965229796245694, \"time-step\": 3150}, {\"accuracy\": 1.0, \"loss\": 0.00029021885711699724, \"time-step\": 3151}, {\"accuracy\": 1.0, \"loss\": 0.0002963529550470412, \"time-step\": 3152}, {\"accuracy\": 1.0, \"loss\": 0.00029005121905356646, \"time-step\": 3153}, {\"accuracy\": 1.0, \"loss\": 0.00029618333792313933, \"time-step\": 3154}, {\"accuracy\": 1.0, \"loss\": 0.0002898805250879377, \"time-step\": 3155}, {\"accuracy\": 1.0, \"loss\": 0.0002960056299343705, \"time-step\": 3156}, {\"accuracy\": 1.0, \"loss\": 0.0002897088706959039, \"time-step\": 3157}, {\"accuracy\": 1.0, \"loss\": 0.0002958324912469834, \"time-step\": 3158}, {\"accuracy\": 1.0, \"loss\": 0.0002895402431022376, \"time-step\": 3159}, {\"accuracy\": 1.0, \"loss\": 0.00029566645389422774, \"time-step\": 3160}, {\"accuracy\": 1.0, \"loss\": 0.0002893711207434535, \"time-step\": 3161}, {\"accuracy\": 1.0, \"loss\": 0.0002954901719931513, \"time-step\": 3162}, {\"accuracy\": 1.0, \"loss\": 0.0002892012125812471, \"time-step\": 3163}, {\"accuracy\": 1.0, \"loss\": 0.0002953232324216515, \"time-step\": 3164}, {\"accuracy\": 1.0, \"loss\": 0.0002890431205742061, \"time-step\": 3165}, {\"accuracy\": 1.0, \"loss\": 0.00029515341157093644, \"time-step\": 3166}, {\"accuracy\": 1.0, \"loss\": 0.0002888656163122505, \"time-step\": 3167}, {\"accuracy\": 1.0, \"loss\": 0.00029497587820515037, \"time-step\": 3168}, {\"accuracy\": 1.0, \"loss\": 0.000288698443910107, \"time-step\": 3169}, {\"accuracy\": 1.0, \"loss\": 0.00029481155797839165, \"time-step\": 3170}, {\"accuracy\": 1.0, \"loss\": 0.00028854087577201426, \"time-step\": 3171}, {\"accuracy\": 1.0, \"loss\": 0.0002946483436971903, \"time-step\": 3172}, {\"accuracy\": 1.0, \"loss\": 0.00028837352874688804, \"time-step\": 3173}, {\"accuracy\": 1.0, \"loss\": 0.00029447299311868846, \"time-step\": 3174}, {\"accuracy\": 1.0, \"loss\": 0.00028819931321777403, \"time-step\": 3175}, {\"accuracy\": 1.0, \"loss\": 0.00029429246205836535, \"time-step\": 3176}, {\"accuracy\": 1.0, \"loss\": 0.0002880249812733382, \"time-step\": 3177}, {\"accuracy\": 1.0, \"loss\": 0.00029411548166535795, \"time-step\": 3178}, {\"accuracy\": 1.0, \"loss\": 0.0002878604573197663, \"time-step\": 3179}, {\"accuracy\": 1.0, \"loss\": 0.000293964782031253, \"time-step\": 3180}, {\"accuracy\": 1.0, \"loss\": 0.00028770981589332223, \"time-step\": 3181}, {\"accuracy\": 1.0, \"loss\": 0.00029380148043856025, \"time-step\": 3182}, {\"accuracy\": 1.0, \"loss\": 0.000287543807644397, \"time-step\": 3183}, {\"accuracy\": 1.0, \"loss\": 0.00029363419162109494, \"time-step\": 3184}, {\"accuracy\": 1.0, \"loss\": 0.00028738612309098244, \"time-step\": 3185}, {\"accuracy\": 1.0, \"loss\": 0.0002934721123892814, \"time-step\": 3186}, {\"accuracy\": 1.0, \"loss\": 0.0002872226759791374, \"time-step\": 3187}, {\"accuracy\": 1.0, \"loss\": 0.00029330392135307193, \"time-step\": 3188}, {\"accuracy\": 1.0, \"loss\": 0.00028705818112939596, \"time-step\": 3189}, {\"accuracy\": 1.0, \"loss\": 0.00029314050334505737, \"time-step\": 3190}, {\"accuracy\": 1.0, \"loss\": 0.00028690442559309304, \"time-step\": 3191}, {\"accuracy\": 1.0, \"loss\": 0.0002929768816102296, \"time-step\": 3192}, {\"accuracy\": 1.0, \"loss\": 0.0002867317816708237, \"time-step\": 3193}, {\"accuracy\": 1.0, \"loss\": 0.00029279853333719075, \"time-step\": 3194}, {\"accuracy\": 1.0, \"loss\": 0.0002865642309188843, \"time-step\": 3195}, {\"accuracy\": 1.0, \"loss\": 0.0002926351153291762, \"time-step\": 3196}, {\"accuracy\": 1.0, \"loss\": 0.000286396243609488, \"time-step\": 3197}, {\"accuracy\": 1.0, \"loss\": 0.0002924611617345363, \"time-step\": 3198}, {\"accuracy\": 1.0, \"loss\": 0.0002862295659724623, \"time-step\": 3199}, {\"accuracy\": 1.0, \"loss\": 0.0002922870626207441, \"time-step\": 3200}, {\"accuracy\": 1.0, \"loss\": 0.0002860638778656721, \"time-step\": 3201}, {\"accuracy\": 1.0, \"loss\": 0.00029212539084255695, \"time-step\": 3202}, {\"accuracy\": 1.0, \"loss\": 0.0002859121304936707, \"time-step\": 3203}, {\"accuracy\": 1.0, \"loss\": 0.0002919704420492053, \"time-step\": 3204}, {\"accuracy\": 1.0, \"loss\": 0.00028575106989592314, \"time-step\": 3205}, {\"accuracy\": 1.0, \"loss\": 0.000291805830784142, \"time-step\": 3206}, {\"accuracy\": 1.0, \"loss\": 0.00028559091151691973, \"time-step\": 3207}, {\"accuracy\": 1.0, \"loss\": 0.0002916401717811823, \"time-step\": 3208}, {\"accuracy\": 1.0, \"loss\": 0.0002854217018466443, \"time-step\": 3209}, {\"accuracy\": 1.0, \"loss\": 0.00029146834276616573, \"time-step\": 3210}, {\"accuracy\": 1.0, \"loss\": 0.0002852600300684571, \"time-step\": 3211}, {\"accuracy\": 1.0, \"loss\": 0.0002913070493377745, \"time-step\": 3212}, {\"accuracy\": 1.0, \"loss\": 0.0002851016179192811, \"time-step\": 3213}, {\"accuracy\": 1.0, \"loss\": 0.00029114342760294676, \"time-step\": 3214}, {\"accuracy\": 1.0, \"loss\": 0.0002849467273335904, \"time-step\": 3215}, {\"accuracy\": 1.0, \"loss\": 0.0002909915638156235, \"time-step\": 3216}, {\"accuracy\": 1.0, \"loss\": 0.0002847941650543362, \"time-step\": 3217}, {\"accuracy\": 1.0, \"loss\": 0.0002908242167904973, \"time-step\": 3218}, {\"accuracy\": 1.0, \"loss\": 0.00028462347108870745, \"time-step\": 3219}, {\"accuracy\": 1.0, \"loss\": 0.00029066024580970407, \"time-step\": 3220}, {\"accuracy\": 1.0, \"loss\": 0.00028446910437196493, \"time-step\": 3221}, {\"accuracy\": 1.0, \"loss\": 0.0002905036089941859, \"time-step\": 3222}, {\"accuracy\": 1.0, \"loss\": 0.0002843223337549716, \"time-step\": 3223}, {\"accuracy\": 1.0, \"loss\": 0.0002903511340264231, \"time-step\": 3224}, {\"accuracy\": 1.0, \"loss\": 0.0002841648238245398, \"time-step\": 3225}, {\"accuracy\": 1.0, \"loss\": 0.00029019691282883286, \"time-step\": 3226}, {\"accuracy\": 1.0, \"loss\": 0.00028401604504324496, \"time-step\": 3227}, {\"accuracy\": 1.0, \"loss\": 0.00029003917006775737, \"time-step\": 3228}, {\"accuracy\": 1.0, \"loss\": 0.00028385434416122735, \"time-step\": 3229}, {\"accuracy\": 1.0, \"loss\": 0.0002898760430980474, \"time-step\": 3230}, {\"accuracy\": 1.0, \"loss\": 0.0002837014617398381, \"time-step\": 3231}, {\"accuracy\": 1.0, \"loss\": 0.0002897204540204257, \"time-step\": 3232}, {\"accuracy\": 1.0, \"loss\": 0.00028354115784168243, \"time-step\": 3233}, {\"accuracy\": 1.0, \"loss\": 0.000289558811346069, \"time-step\": 3234}, {\"accuracy\": 1.0, \"loss\": 0.0002833932521753013, \"time-step\": 3235}, {\"accuracy\": 1.0, \"loss\": 0.0002893991186283529, \"time-step\": 3236}, {\"accuracy\": 1.0, \"loss\": 0.0002832270401995629, \"time-step\": 3237}, {\"accuracy\": 1.0, \"loss\": 0.0002892414922825992, \"time-step\": 3238}, {\"accuracy\": 1.0, \"loss\": 0.0002830769226420671, \"time-step\": 3239}, {\"accuracy\": 1.0, \"loss\": 0.0002890853793360293, \"time-step\": 3240}, {\"accuracy\": 1.0, \"loss\": 0.0002829180739354342, \"time-step\": 3241}, {\"accuracy\": 1.0, \"loss\": 0.00028892100090160966, \"time-step\": 3242}, {\"accuracy\": 1.0, \"loss\": 0.00028275614022277296, \"time-step\": 3243}, {\"accuracy\": 1.0, \"loss\": 0.00028874591225758195, \"time-step\": 3244}, {\"accuracy\": 1.0, \"loss\": 0.00028257715166546404, \"time-step\": 3245}, {\"accuracy\": 1.0, \"loss\": 0.00028857015422545373, \"time-step\": 3246}, {\"accuracy\": 1.0, \"loss\": 0.00028241766267456114, \"time-step\": 3247}, {\"accuracy\": 1.0, \"loss\": 0.00028841590392403305, \"time-step\": 3248}, {\"accuracy\": 1.0, \"loss\": 0.00028227060101926327, \"time-step\": 3249}, {\"accuracy\": 1.0, \"loss\": 0.00028825615299865603, \"time-step\": 3250}, {\"accuracy\": 1.0, \"loss\": 0.00028211198514327407, \"time-step\": 3251}, {\"accuracy\": 1.0, \"loss\": 0.00028809564537368715, \"time-step\": 3252}, {\"accuracy\": 1.0, \"loss\": 0.00028194693732075393, \"time-step\": 3253}, {\"accuracy\": 1.0, \"loss\": 0.0002879388921428472, \"time-step\": 3254}, {\"accuracy\": 1.0, \"loss\": 0.00028179478249512613, \"time-step\": 3255}, {\"accuracy\": 1.0, \"loss\": 0.00028777457191608846, \"time-step\": 3256}, {\"accuracy\": 1.0, \"loss\": 0.00028163535171188414, \"time-step\": 3257}, {\"accuracy\": 1.0, \"loss\": 0.00028761138673871756, \"time-step\": 3258}, {\"accuracy\": 1.0, \"loss\": 0.00028147586272098124, \"time-step\": 3259}, {\"accuracy\": 1.0, \"loss\": 0.0002874477067962289, \"time-step\": 3260}, {\"accuracy\": 1.0, \"loss\": 0.00028131704311817884, \"time-step\": 3261}, {\"accuracy\": 1.0, \"loss\": 0.000287295610178262, \"time-step\": 3262}, {\"accuracy\": 1.0, \"loss\": 0.0002811715821735561, \"time-step\": 3263}, {\"accuracy\": 1.0, \"loss\": 0.000287146947812289, \"time-step\": 3264}, {\"accuracy\": 1.0, \"loss\": 0.00028102652868255973, \"time-step\": 3265}, {\"accuracy\": 1.0, \"loss\": 0.0002869930467568338, \"time-step\": 3266}, {\"accuracy\": 1.0, \"loss\": 0.0002808683493640274, \"time-step\": 3267}, {\"accuracy\": 1.0, \"loss\": 0.00028683579876087606, \"time-step\": 3268}, {\"accuracy\": 1.0, \"loss\": 0.00028072248096577823, \"time-step\": 3269}, {\"accuracy\": 1.0, \"loss\": 0.0002866979921236634, \"time-step\": 3270}, {\"accuracy\": 1.0, \"loss\": 0.0002805890399031341, \"time-step\": 3271}, {\"accuracy\": 1.0, \"loss\": 0.0002865564892999828, \"time-step\": 3272}, {\"accuracy\": 1.0, \"loss\": 0.0002804401738103479, \"time-step\": 3273}, {\"accuracy\": 1.0, \"loss\": 0.00028639374068006873, \"time-step\": 3274}, {\"accuracy\": 1.0, \"loss\": 0.0002802863600663841, \"time-step\": 3275}, {\"accuracy\": 1.0, \"loss\": 0.0002862393157556653, \"time-step\": 3276}, {\"accuracy\": 1.0, \"loss\": 0.0002801264636218548, \"time-step\": 3277}, {\"accuracy\": 1.0, \"loss\": 0.00028607709100469947, \"time-step\": 3278}, {\"accuracy\": 1.0, \"loss\": 0.00027996310382150114, \"time-step\": 3279}, {\"accuracy\": 1.0, \"loss\": 0.00028590907459147274, \"time-step\": 3280}, {\"accuracy\": 1.0, \"loss\": 0.000279811181826517, \"time-step\": 3281}, {\"accuracy\": 1.0, \"loss\": 0.0002857603249140084, \"time-step\": 3282}, {\"accuracy\": 1.0, \"loss\": 0.0002796624321490526, \"time-step\": 3283}, {\"accuracy\": 1.0, \"loss\": 0.0002856040373444557, \"time-step\": 3284}, {\"accuracy\": 1.0, \"loss\": 0.0002795050968416035, \"time-step\": 3285}, {\"accuracy\": 1.0, \"loss\": 0.0002854463818948716, \"time-step\": 3286}, {\"accuracy\": 1.0, \"loss\": 0.00027935593971051276, \"time-step\": 3287}, {\"accuracy\": 1.0, \"loss\": 0.0002852992620319128, \"time-step\": 3288}, {\"accuracy\": 1.0, \"loss\": 0.00027921132277697325, \"time-step\": 3289}, {\"accuracy\": 1.0, \"loss\": 0.00028513968572951853, \"time-step\": 3290}, {\"accuracy\": 1.0, \"loss\": 0.00027904592570848763, \"time-step\": 3291}, {\"accuracy\": 1.0, \"loss\": 0.00028497990570031106, \"time-step\": 3292}, {\"accuracy\": 1.0, \"loss\": 0.00027890168712474406, \"time-step\": 3293}, {\"accuracy\": 1.0, \"loss\": 0.0002848318254109472, \"time-step\": 3294}, {\"accuracy\": 1.0, \"loss\": 0.0002787484263535589, \"time-step\": 3295}, {\"accuracy\": 1.0, \"loss\": 0.00028467492666095495, \"time-step\": 3296}, {\"accuracy\": 1.0, \"loss\": 0.0002785993565339595, \"time-step\": 3297}, {\"accuracy\": 1.0, \"loss\": 0.0002845301933120936, \"time-step\": 3298}, {\"accuracy\": 1.0, \"loss\": 0.0002784524695016444, \"time-step\": 3299}, {\"accuracy\": 1.0, \"loss\": 0.00028436677530407906, \"time-step\": 3300}, {\"accuracy\": 1.0, \"loss\": 0.0002782851515803486, \"time-step\": 3301}, {\"accuracy\": 1.0, \"loss\": 0.0002841963432729244, \"time-step\": 3302}, {\"accuracy\": 1.0, \"loss\": 0.00027812557527795434, \"time-step\": 3303}, {\"accuracy\": 1.0, \"loss\": 0.0002840422675944865, \"time-step\": 3304}, {\"accuracy\": 1.0, \"loss\": 0.0002779829374048859, \"time-step\": 3305}, {\"accuracy\": 1.0, \"loss\": 0.00028389241197146475, \"time-step\": 3306}, {\"accuracy\": 1.0, \"loss\": 0.00027783552650362253, \"time-step\": 3307}, {\"accuracy\": 1.0, \"loss\": 0.00028374732937663794, \"time-step\": 3308}, {\"accuracy\": 1.0, \"loss\": 0.00027769088046625257, \"time-step\": 3309}, {\"accuracy\": 1.0, \"loss\": 0.0002836069033946842, \"time-step\": 3310}, {\"accuracy\": 1.0, \"loss\": 0.0002775535103864968, \"time-step\": 3311}, {\"accuracy\": 1.0, \"loss\": 0.0002834539918694645, \"time-step\": 3312}, {\"accuracy\": 1.0, \"loss\": 0.0002773940213955939, \"time-step\": 3313}, {\"accuracy\": 1.0, \"loss\": 0.0002832921454682946, \"time-step\": 3314}, {\"accuracy\": 1.0, \"loss\": 0.0002772479201667011, \"time-step\": 3315}, {\"accuracy\": 1.0, \"loss\": 0.00028314138762652874, \"time-step\": 3316}, {\"accuracy\": 1.0, \"loss\": 0.0002770870632957667, \"time-step\": 3317}, {\"accuracy\": 1.0, \"loss\": 0.0002829819277394563, \"time-step\": 3318}, {\"accuracy\": 1.0, \"loss\": 0.0002769477723632008, \"time-step\": 3319}, {\"accuracy\": 1.0, \"loss\": 0.00028284837026149035, \"time-step\": 3320}, {\"accuracy\": 1.0, \"loss\": 0.0002768060949165374, \"time-step\": 3321}, {\"accuracy\": 1.0, \"loss\": 0.00028269016183912754, \"time-step\": 3322}, {\"accuracy\": 1.0, \"loss\": 0.00027664427761919796, \"time-step\": 3323}, {\"accuracy\": 1.0, \"loss\": 0.0002825244446285069, \"time-step\": 3324}, {\"accuracy\": 1.0, \"loss\": 0.0002764876699075103, \"time-step\": 3325}, {\"accuracy\": 1.0, \"loss\": 0.00028237729566171765, \"time-step\": 3326}, {\"accuracy\": 1.0, \"loss\": 0.0002763446536846459, \"time-step\": 3327}, {\"accuracy\": 1.0, \"loss\": 0.0002822197857312858, \"time-step\": 3328}, {\"accuracy\": 1.0, \"loss\": 0.0002761946525424719, \"time-step\": 3329}, {\"accuracy\": 1.0, \"loss\": 0.00028208130970597267, \"time-step\": 3330}, {\"accuracy\": 1.0, \"loss\": 0.00027606383082456887, \"time-step\": 3331}, {\"accuracy\": 1.0, \"loss\": 0.0002819462097249925, \"time-step\": 3332}, {\"accuracy\": 1.0, \"loss\": 0.0002759266644716263, \"time-step\": 3333}, {\"accuracy\": 1.0, \"loss\": 0.0002818030188791454, \"time-step\": 3334}, {\"accuracy\": 1.0, \"loss\": 0.0002757814363576472, \"time-step\": 3335}, {\"accuracy\": 1.0, \"loss\": 0.0002816621563397348, \"time-step\": 3336}, {\"accuracy\": 1.0, \"loss\": 0.0002756446192506701, \"time-step\": 3337}, {\"accuracy\": 1.0, \"loss\": 0.0002815062180161476, \"time-step\": 3338}, {\"accuracy\": 1.0, \"loss\": 0.00027548716752789915, \"time-step\": 3339}, {\"accuracy\": 1.0, \"loss\": 0.0002813552273437381, \"time-step\": 3340}, {\"accuracy\": 1.0, \"loss\": 0.00027533603133633733, \"time-step\": 3341}, {\"accuracy\": 1.0, \"loss\": 0.0002811947779264301, \"time-step\": 3342}, {\"accuracy\": 1.0, \"loss\": 0.000275186903309077, \"time-step\": 3343}, {\"accuracy\": 1.0, \"loss\": 0.00028104413649998605, \"time-step\": 3344}, {\"accuracy\": 1.0, \"loss\": 0.00027503061573952436, \"time-step\": 3345}, {\"accuracy\": 1.0, \"loss\": 0.0002808843564707786, \"time-step\": 3346}, {\"accuracy\": 1.0, \"loss\": 0.00027487840270623565, \"time-step\": 3347}, {\"accuracy\": 1.0, \"loss\": 0.0002807327837217599, \"time-step\": 3348}, {\"accuracy\": 1.0, \"loss\": 0.0002747338730841875, \"time-step\": 3349}, {\"accuracy\": 1.0, \"loss\": 0.0002805817639455199, \"time-step\": 3350}, {\"accuracy\": 1.0, \"loss\": 0.0002745803212746978, \"time-step\": 3351}, {\"accuracy\": 1.0, \"loss\": 0.00028042448684573174, \"time-step\": 3352}, {\"accuracy\": 1.0, \"loss\": 0.0002744297671597451, \"time-step\": 3353}, {\"accuracy\": 1.0, \"loss\": 0.0002802757080644369, \"time-step\": 3354}, {\"accuracy\": 1.0, \"loss\": 0.0002742912038229406, \"time-step\": 3355}, {\"accuracy\": 1.0, \"loss\": 0.00028013859991915524, \"time-step\": 3356}, {\"accuracy\": 1.0, \"loss\": 0.00027415179647505283, \"time-step\": 3357}, {\"accuracy\": 1.0, \"loss\": 0.00027998635778203607, \"time-step\": 3358}, {\"accuracy\": 1.0, \"loss\": 0.00027400313410907984, \"time-step\": 3359}, {\"accuracy\": 1.0, \"loss\": 0.0002798434579744935, \"time-step\": 3360}, {\"accuracy\": 1.0, \"loss\": 0.0002738647162914276, \"time-step\": 3361}, {\"accuracy\": 1.0, \"loss\": 0.0002797036722768098, \"time-step\": 3362}, {\"accuracy\": 1.0, \"loss\": 0.00027372961631044745, \"time-step\": 3363}, {\"accuracy\": 1.0, \"loss\": 0.000279568659607321, \"time-step\": 3364}, {\"accuracy\": 1.0, \"loss\": 0.00027358951047062874, \"time-step\": 3365}, {\"accuracy\": 1.0, \"loss\": 0.00027942191809415817, \"time-step\": 3366}, {\"accuracy\": 1.0, \"loss\": 0.0002734505687840283, \"time-step\": 3367}, {\"accuracy\": 1.0, \"loss\": 0.00027927395422011614, \"time-step\": 3368}, {\"accuracy\": 1.0, \"loss\": 0.00027330368175171316, \"time-step\": 3369}, {\"accuracy\": 1.0, \"loss\": 0.00027913355734199286, \"time-step\": 3370}, {\"accuracy\": 1.0, \"loss\": 0.0002731686399783939, \"time-step\": 3371}, {\"accuracy\": 1.0, \"loss\": 0.0002789929567370564, \"time-step\": 3372}, {\"accuracy\": 1.0, \"loss\": 0.00027303697424940765, \"time-step\": 3373}, {\"accuracy\": 1.0, \"loss\": 0.0002788557321764529, \"time-step\": 3374}, {\"accuracy\": 1.0, \"loss\": 0.00027288426645100117, \"time-step\": 3375}, {\"accuracy\": 1.0, \"loss\": 0.00027869452605955303, \"time-step\": 3376}, {\"accuracy\": 1.0, \"loss\": 0.00027273650630377233, \"time-step\": 3377}, {\"accuracy\": 1.0, \"loss\": 0.00027855182997882366, \"time-step\": 3378}, {\"accuracy\": 1.0, \"loss\": 0.0002725927624851465, \"time-step\": 3379}, {\"accuracy\": 1.0, \"loss\": 0.0002784110838547349, \"time-step\": 3380}, {\"accuracy\": 1.0, \"loss\": 0.0002724586520344019, \"time-step\": 3381}, {\"accuracy\": 1.0, \"loss\": 0.0002782691444735974, \"time-step\": 3382}, {\"accuracy\": 1.0, \"loss\": 0.00027231191052123904, \"time-step\": 3383}, {\"accuracy\": 1.0, \"loss\": 0.00027810793835669756, \"time-step\": 3384}, {\"accuracy\": 1.0, \"loss\": 0.00027216318994760513, \"time-step\": 3385}, {\"accuracy\": 1.0, \"loss\": 0.0002779684728011489, \"time-step\": 3386}, {\"accuracy\": 1.0, \"loss\": 0.00027202264755032957, \"time-step\": 3387}, {\"accuracy\": 1.0, \"loss\": 0.0002778170455712825, \"time-step\": 3388}, {\"accuracy\": 1.0, \"loss\": 0.0002718705218285322, \"time-step\": 3389}, {\"accuracy\": 1.0, \"loss\": 0.0002776652399916202, \"time-step\": 3390}, {\"accuracy\": 1.0, \"loss\": 0.0002717222960200161, \"time-step\": 3391}, {\"accuracy\": 1.0, \"loss\": 0.00027751721790991724, \"time-step\": 3392}, {\"accuracy\": 1.0, \"loss\": 0.0002715880691539496, \"time-step\": 3393}, {\"accuracy\": 1.0, \"loss\": 0.00027738092467188835, \"time-step\": 3394}, {\"accuracy\": 1.0, \"loss\": 0.0002714527945499867, \"time-step\": 3395}, {\"accuracy\": 1.0, \"loss\": 0.0002772409934550524, \"time-step\": 3396}, {\"accuracy\": 1.0, \"loss\": 0.0002713180147111416, \"time-step\": 3397}, {\"accuracy\": 1.0, \"loss\": 0.0002771110739558935, \"time-step\": 3398}, {\"accuracy\": 1.0, \"loss\": 0.00027118169236928225, \"time-step\": 3399}, {\"accuracy\": 1.0, \"loss\": 0.0002769709099084139, \"time-step\": 3400}, {\"accuracy\": 1.0, \"loss\": 0.00027104810578748584, \"time-step\": 3401}, {\"accuracy\": 1.0, \"loss\": 0.0002768229169305414, \"time-step\": 3402}, {\"accuracy\": 1.0, \"loss\": 0.00027089478680863976, \"time-step\": 3403}, {\"accuracy\": 1.0, \"loss\": 0.000276669830782339, \"time-step\": 3404}, {\"accuracy\": 1.0, \"loss\": 0.0002707469102460891, \"time-step\": 3405}, {\"accuracy\": 1.0, \"loss\": 0.0002765224198810756, \"time-step\": 3406}, {\"accuracy\": 1.0, \"loss\": 0.0002706115192268044, \"time-step\": 3407}, {\"accuracy\": 1.0, \"loss\": 0.00027638746541924775, \"time-step\": 3408}, {\"accuracy\": 1.0, \"loss\": 0.00027047135517932475, \"time-step\": 3409}, {\"accuracy\": 1.0, \"loss\": 0.0002762428775895387, \"time-step\": 3410}, {\"accuracy\": 1.0, \"loss\": 0.00027033622609451413, \"time-step\": 3411}, {\"accuracy\": 1.0, \"loss\": 0.00027610408142209053, \"time-step\": 3412}, {\"accuracy\": 1.0, \"loss\": 0.0002701992925722152, \"time-step\": 3413}, {\"accuracy\": 1.0, \"loss\": 0.0002759612980298698, \"time-step\": 3414}, {\"accuracy\": 1.0, \"loss\": 0.0002700484765227884, \"time-step\": 3415}, {\"accuracy\": 1.0, \"loss\": 0.00027580460300669074, \"time-step\": 3416}, {\"accuracy\": 1.0, \"loss\": 0.00026990764308720827, \"time-step\": 3417}, {\"accuracy\": 1.0, \"loss\": 0.0002756688045337796, \"time-step\": 3418}, {\"accuracy\": 1.0, \"loss\": 0.0002697732124943286, \"time-step\": 3419}, {\"accuracy\": 1.0, \"loss\": 0.00027552773826755583, \"time-step\": 3420}, {\"accuracy\": 1.0, \"loss\": 0.0002696271112654358, \"time-step\": 3421}, {\"accuracy\": 1.0, \"loss\": 0.00027538411086425185, \"time-step\": 3422}, {\"accuracy\": 1.0, \"loss\": 0.0002694967552088201, \"time-step\": 3423}, {\"accuracy\": 1.0, \"loss\": 0.0002752464497461915, \"time-step\": 3424}, {\"accuracy\": 1.0, \"loss\": 0.00026935242931358516, \"time-step\": 3425}, {\"accuracy\": 1.0, \"loss\": 0.0002751056745182723, \"time-step\": 3426}, {\"accuracy\": 1.0, \"loss\": 0.0002692162524908781, \"time-step\": 3427}, {\"accuracy\": 1.0, \"loss\": 0.00027496032998897135, \"time-step\": 3428}, {\"accuracy\": 1.0, \"loss\": 0.0002690758847165853, \"time-step\": 3429}, {\"accuracy\": 1.0, \"loss\": 0.00027482162113301456, \"time-step\": 3430}, {\"accuracy\": 1.0, \"loss\": 0.0002689445100259036, \"time-step\": 3431}, {\"accuracy\": 1.0, \"loss\": 0.00027468526968732476, \"time-step\": 3432}, {\"accuracy\": 1.0, \"loss\": 0.00026880684890784323, \"time-step\": 3433}, {\"accuracy\": 1.0, \"loss\": 0.0002745513920672238, \"time-step\": 3434}, {\"accuracy\": 1.0, \"loss\": 0.00026868091663345695, \"time-step\": 3435}, {\"accuracy\": 1.0, \"loss\": 0.00027442778809927404, \"time-step\": 3436}, {\"accuracy\": 1.0, \"loss\": 0.00026855472242459655, \"time-step\": 3437}, {\"accuracy\": 1.0, \"loss\": 0.0002742844517342746, \"time-step\": 3438}, {\"accuracy\": 1.0, \"loss\": 0.00026841097860597074, \"time-step\": 3439}, {\"accuracy\": 1.0, \"loss\": 0.00027414195938035846, \"time-step\": 3440}, {\"accuracy\": 1.0, \"loss\": 0.00026826938847079873, \"time-step\": 3441}, {\"accuracy\": 1.0, \"loss\": 0.00027399964164942503, \"time-step\": 3442}, {\"accuracy\": 1.0, \"loss\": 0.00026813949807547033, \"time-step\": 3443}, {\"accuracy\": 1.0, \"loss\": 0.0002738693729043007, \"time-step\": 3444}, {\"accuracy\": 1.0, \"loss\": 0.0002680069883354008, \"time-step\": 3445}, {\"accuracy\": 1.0, \"loss\": 0.00027372714248485863, \"time-step\": 3446}, {\"accuracy\": 1.0, \"loss\": 0.00026786886155605316, \"time-step\": 3447}, {\"accuracy\": 1.0, \"loss\": 0.0002736009773798287, \"time-step\": 3448}, {\"accuracy\": 1.0, \"loss\": 0.0002677490410860628, \"time-step\": 3449}, {\"accuracy\": 1.0, \"loss\": 0.00027346969000063837, \"time-step\": 3450}, {\"accuracy\": 1.0, \"loss\": 0.00026762555353343487, \"time-step\": 3451}, {\"accuracy\": 1.0, \"loss\": 0.0002733446308411658, \"time-step\": 3452}, {\"accuracy\": 1.0, \"loss\": 0.0002674978750292212, \"time-step\": 3453}, {\"accuracy\": 1.0, \"loss\": 0.0002732163993641734, \"time-step\": 3454}, {\"accuracy\": 1.0, \"loss\": 0.0002673605049494654, \"time-step\": 3455}, {\"accuracy\": 1.0, \"loss\": 0.00027306389529258013, \"time-step\": 3456}, {\"accuracy\": 1.0, \"loss\": 0.00026720884488895535, \"time-step\": 3457}, {\"accuracy\": 1.0, \"loss\": 0.0002729116822592914, \"time-step\": 3458}, {\"accuracy\": 1.0, \"loss\": 0.0002670676039997488, \"time-step\": 3459}, {\"accuracy\": 1.0, \"loss\": 0.0002727727987803519, \"time-step\": 3460}, {\"accuracy\": 1.0, \"loss\": 0.0002669327659532428, \"time-step\": 3461}, {\"accuracy\": 1.0, \"loss\": 0.00027263243100605905, \"time-step\": 3462}, {\"accuracy\": 1.0, \"loss\": 0.0002667949302121997, \"time-step\": 3463}, {\"accuracy\": 1.0, \"loss\": 0.00027249741833657026, \"time-step\": 3464}, {\"accuracy\": 1.0, \"loss\": 0.00026666195481084287, \"time-step\": 3465}, {\"accuracy\": 1.0, \"loss\": 0.00027236275491304696, \"time-step\": 3466}, {\"accuracy\": 1.0, \"loss\": 0.0002665278734639287, \"time-step\": 3467}, {\"accuracy\": 1.0, \"loss\": 0.0002722267236094922, \"time-step\": 3468}, {\"accuracy\": 1.0, \"loss\": 0.00026639539282768965, \"time-step\": 3469}, {\"accuracy\": 1.0, \"loss\": 0.00027208300889469683, \"time-step\": 3470}, {\"accuracy\": 1.0, \"loss\": 0.0002662556944414973, \"time-step\": 3471}, {\"accuracy\": 1.0, \"loss\": 0.000271942641120404, \"time-step\": 3472}, {\"accuracy\": 1.0, \"loss\": 0.0002661139005795121, \"time-step\": 3473}, {\"accuracy\": 1.0, \"loss\": 0.0002717951429076493, \"time-step\": 3474}, {\"accuracy\": 1.0, \"loss\": 0.0002659789752215147, \"time-step\": 3475}, {\"accuracy\": 1.0, \"loss\": 0.000271663477178663, \"time-step\": 3476}, {\"accuracy\": 1.0, \"loss\": 0.00026585007435642183, \"time-step\": 3477}, {\"accuracy\": 1.0, \"loss\": 0.00027153862174600363, \"time-step\": 3478}, {\"accuracy\": 1.0, \"loss\": 0.0002657284203451127, \"time-step\": 3479}, {\"accuracy\": 1.0, \"loss\": 0.00027141490136273205, \"time-step\": 3480}, {\"accuracy\": 1.0, \"loss\": 0.00026559681282378733, \"time-step\": 3481}, {\"accuracy\": 1.0, \"loss\": 0.00027127511566504836, \"time-step\": 3482}, {\"accuracy\": 1.0, \"loss\": 0.00026546497247181833, \"time-step\": 3483}, {\"accuracy\": 1.0, \"loss\": 0.0002711357665248215, \"time-step\": 3484}, {\"accuracy\": 1.0, \"loss\": 0.0002653276897035539, \"time-step\": 3485}, {\"accuracy\": 1.0, \"loss\": 0.0002710049448069185, \"time-step\": 3486}, {\"accuracy\": 1.0, \"loss\": 0.00026520193205215037, \"time-step\": 3487}, {\"accuracy\": 1.0, \"loss\": 0.00027087377384305, \"time-step\": 3488}, {\"accuracy\": 1.0, \"loss\": 0.00026506627909839153, \"time-step\": 3489}, {\"accuracy\": 1.0, \"loss\": 0.0002707260719034821, \"time-step\": 3490}, {\"accuracy\": 1.0, \"loss\": 0.00026492367032915354, \"time-step\": 3491}, {\"accuracy\": 1.0, \"loss\": 0.0002705852675717324, \"time-step\": 3492}, {\"accuracy\": 1.0, \"loss\": 0.00026478382642380893, \"time-step\": 3493}, {\"accuracy\": 1.0, \"loss\": 0.00027044760645367205, \"time-step\": 3494}, {\"accuracy\": 1.0, \"loss\": 0.00026466199778951705, \"time-step\": 3495}, {\"accuracy\": 1.0, \"loss\": 0.00027033654623664916, \"time-step\": 3496}, {\"accuracy\": 1.0, \"loss\": 0.0002645492786541581, \"time-step\": 3497}, {\"accuracy\": 1.0, \"loss\": 0.00027021230198442936, \"time-step\": 3498}, {\"accuracy\": 1.0, \"loss\": 0.0002644180494826287, \"time-step\": 3499}, {\"accuracy\": 1.0, \"loss\": 0.00027007318567484617, \"time-step\": 3500}, {\"accuracy\": 1.0, \"loss\": 0.00026429188437759876, \"time-step\": 3501}, {\"accuracy\": 1.0, \"loss\": 0.00026994975632987916, \"time-step\": 3502}, {\"accuracy\": 1.0, \"loss\": 0.00026417121989652514, \"time-step\": 3503}, {\"accuracy\": 1.0, \"loss\": 0.00026981806149706244, \"time-step\": 3504}, {\"accuracy\": 1.0, \"loss\": 0.00026403070660308003, \"time-step\": 3505}, {\"accuracy\": 1.0, \"loss\": 0.00026967565645463765, \"time-step\": 3506}, {\"accuracy\": 1.0, \"loss\": 0.0002638974110595882, \"time-step\": 3507}, {\"accuracy\": 1.0, \"loss\": 0.00026954090571962297, \"time-step\": 3508}, {\"accuracy\": 1.0, \"loss\": 0.000263759633526206, \"time-step\": 3509}, {\"accuracy\": 1.0, \"loss\": 0.0002693943097256124, \"time-step\": 3510}, {\"accuracy\": 1.0, \"loss\": 0.00026362237986177206, \"time-step\": 3511}, {\"accuracy\": 1.0, \"loss\": 0.00026926022837869823, \"time-step\": 3512}, {\"accuracy\": 1.0, \"loss\": 0.0002634814300108701, \"time-step\": 3513}, {\"accuracy\": 1.0, \"loss\": 0.0002691126137506217, \"time-step\": 3514}, {\"accuracy\": 1.0, \"loss\": 0.000263345951680094, \"time-step\": 3515}, {\"accuracy\": 1.0, \"loss\": 0.000268981559202075, \"time-step\": 3516}, {\"accuracy\": 1.0, \"loss\": 0.0002632196992635727, \"time-step\": 3517}, {\"accuracy\": 1.0, \"loss\": 0.00026885137776844203, \"time-step\": 3518}, {\"accuracy\": 1.0, \"loss\": 0.00026308876113034785, \"time-step\": 3519}, {\"accuracy\": 1.0, \"loss\": 0.00026872416492551565, \"time-step\": 3520}, {\"accuracy\": 1.0, \"loss\": 0.0002629673108458519, \"time-step\": 3521}, {\"accuracy\": 1.0, \"loss\": 0.0002685965155251324, \"time-step\": 3522}, {\"accuracy\": 1.0, \"loss\": 0.00026284041814506054, \"time-step\": 3523}, {\"accuracy\": 1.0, \"loss\": 0.00026847075787372887, \"time-step\": 3524}, {\"accuracy\": 1.0, \"loss\": 0.00026272019022144377, \"time-step\": 3525}, {\"accuracy\": 1.0, \"loss\": 0.00026833711308427155, \"time-step\": 3526}, {\"accuracy\": 1.0, \"loss\": 0.0002625838969834149, \"time-step\": 3527}, {\"accuracy\": 1.0, \"loss\": 0.00026820143102668226, \"time-step\": 3528}, {\"accuracy\": 1.0, \"loss\": 0.00026245121262036264, \"time-step\": 3529}, {\"accuracy\": 1.0, \"loss\": 0.00026807287940755486, \"time-step\": 3530}, {\"accuracy\": 1.0, \"loss\": 0.0002623303735163063, \"time-step\": 3531}, {\"accuracy\": 1.0, \"loss\": 0.0002679451135918498, \"time-step\": 3532}, {\"accuracy\": 1.0, \"loss\": 0.0002622028114274144, \"time-step\": 3533}, {\"accuracy\": 1.0, \"loss\": 0.00026781600899994373, \"time-step\": 3534}, {\"accuracy\": 1.0, \"loss\": 0.00026208176859654486, \"time-step\": 3535}, {\"accuracy\": 1.0, \"loss\": 0.0002676989242900163, \"time-step\": 3536}, {\"accuracy\": 1.0, \"loss\": 0.00026196628459729254, \"time-step\": 3537}, {\"accuracy\": 1.0, \"loss\": 0.00026757147861644626, \"time-step\": 3538}, {\"accuracy\": 1.0, \"loss\": 0.0002618396538309753, \"time-step\": 3539}, {\"accuracy\": 1.0, \"loss\": 0.00026744266506284475, \"time-step\": 3540}, {\"accuracy\": 1.0, \"loss\": 0.00026170487399213016, \"time-step\": 3541}, {\"accuracy\": 1.0, \"loss\": 0.00026730491663329303, \"time-step\": 3542}, {\"accuracy\": 1.0, \"loss\": 0.000261570792645216, \"time-step\": 3543}, {\"accuracy\": 1.0, \"loss\": 0.0002671751717571169, \"time-step\": 3544}, {\"accuracy\": 1.0, \"loss\": 0.0002614482364151627, \"time-step\": 3545}, {\"accuracy\": 1.0, \"loss\": 0.000267041934421286, \"time-step\": 3546}, {\"accuracy\": 1.0, \"loss\": 0.0002613180549815297, \"time-step\": 3547}, {\"accuracy\": 1.0, \"loss\": 0.0002669149835128337, \"time-step\": 3548}, {\"accuracy\": 1.0, \"loss\": 0.0002611931413412094, \"time-step\": 3549}, {\"accuracy\": 1.0, \"loss\": 0.000266786984866485, \"time-step\": 3550}, {\"accuracy\": 1.0, \"loss\": 0.00026106697623617947, \"time-step\": 3551}, {\"accuracy\": 1.0, \"loss\": 0.0002666628279257566, \"time-step\": 3552}, {\"accuracy\": 1.0, \"loss\": 0.00026095149223692715, \"time-step\": 3553}, {\"accuracy\": 1.0, \"loss\": 0.0002665386418811977, \"time-step\": 3554}, {\"accuracy\": 1.0, \"loss\": 0.0002608204958960414, \"time-step\": 3555}, {\"accuracy\": 1.0, \"loss\": 0.00026640098076313734, \"time-step\": 3556}, {\"accuracy\": 1.0, \"loss\": 0.0002606912748888135, \"time-step\": 3557}, {\"accuracy\": 1.0, \"loss\": 0.00026627577608451247, \"time-step\": 3558}, {\"accuracy\": 1.0, \"loss\": 0.0002605645568110049, \"time-step\": 3559}, {\"accuracy\": 1.0, \"loss\": 0.00026614233502186835, \"time-step\": 3560}, {\"accuracy\": 1.0, \"loss\": 0.0002604360633995384, \"time-step\": 3561}, {\"accuracy\": 1.0, \"loss\": 0.00026601203717291355, \"time-step\": 3562}, {\"accuracy\": 1.0, \"loss\": 0.00026030922890640795, \"time-step\": 3563}, {\"accuracy\": 1.0, \"loss\": 0.00026588959735818207, \"time-step\": 3564}, {\"accuracy\": 1.0, \"loss\": 0.00026019831420853734, \"time-step\": 3565}, {\"accuracy\": 1.0, \"loss\": 0.0002657732693478465, \"time-step\": 3566}, {\"accuracy\": 1.0, \"loss\": 0.0002600763691589236, \"time-step\": 3567}, {\"accuracy\": 1.0, \"loss\": 0.00026564017753116786, \"time-step\": 3568}, {\"accuracy\": 1.0, \"loss\": 0.00025992514565587044, \"time-step\": 3569}, {\"accuracy\": 1.0, \"loss\": 0.0002654891286510974, \"time-step\": 3570}, {\"accuracy\": 1.0, \"loss\": 0.0002598007849883288, \"time-step\": 3571}, {\"accuracy\": 1.0, \"loss\": 0.00026537501253187656, \"time-step\": 3572}, {\"accuracy\": 1.0, \"loss\": 0.0002596844278741628, \"time-step\": 3573}, {\"accuracy\": 1.0, \"loss\": 0.00026524742133915424, \"time-step\": 3574}, {\"accuracy\": 1.0, \"loss\": 0.00025956271565519273, \"time-step\": 3575}, {\"accuracy\": 1.0, \"loss\": 0.0002651242248248309, \"time-step\": 3576}, {\"accuracy\": 1.0, \"loss\": 0.0002594358229544014, \"time-step\": 3577}, {\"accuracy\": 1.0, \"loss\": 0.0002649944508448243, \"time-step\": 3578}, {\"accuracy\": 1.0, \"loss\": 0.00025931475101970136, \"time-step\": 3579}, {\"accuracy\": 1.0, \"loss\": 0.00026487038121558726, \"time-step\": 3580}, {\"accuracy\": 1.0, \"loss\": 0.00025918325991369784, \"time-step\": 3581}, {\"accuracy\": 1.0, \"loss\": 0.00026473720208741724, \"time-step\": 3582}, {\"accuracy\": 1.0, \"loss\": 0.00025907097733579576, \"time-step\": 3583}, {\"accuracy\": 1.0, \"loss\": 0.00026463408721610904, \"time-step\": 3584}, {\"accuracy\": 1.0, \"loss\": 0.0002589607611298561, \"time-step\": 3585}, {\"accuracy\": 1.0, \"loss\": 0.00026450876612216234, \"time-step\": 3586}, {\"accuracy\": 1.0, \"loss\": 0.00025883247144520283, \"time-step\": 3587}, {\"accuracy\": 1.0, \"loss\": 0.00026437468477524817, \"time-step\": 3588}, {\"accuracy\": 1.0, \"loss\": 0.0002586909104138613, \"time-step\": 3589}, {\"accuracy\": 1.0, \"loss\": 0.0002642315230332315, \"time-step\": 3590}, {\"accuracy\": 1.0, \"loss\": 0.00025857353466562927, \"time-step\": 3591}, {\"accuracy\": 1.0, \"loss\": 0.00026411586441099644, \"time-step\": 3592}, {\"accuracy\": 1.0, \"loss\": 0.0002584483299870044, \"time-step\": 3593}, {\"accuracy\": 1.0, \"loss\": 0.0002639789308886975, \"time-step\": 3594}, {\"accuracy\": 1.0, \"loss\": 0.0002583120367489755, \"time-step\": 3595}, {\"accuracy\": 1.0, \"loss\": 0.0002638447913341224, \"time-step\": 3596}, {\"accuracy\": 1.0, \"loss\": 0.0002581817389000207, \"time-step\": 3597}, {\"accuracy\": 1.0, \"loss\": 0.00026371210697107017, \"time-step\": 3598}, {\"accuracy\": 1.0, \"loss\": 0.00025805601035244763, \"time-step\": 3599}, {\"accuracy\": 1.0, \"loss\": 0.00026358800823800266, \"time-step\": 3600}, {\"accuracy\": 1.0, \"loss\": 0.0002579381107352674, \"time-step\": 3601}, {\"accuracy\": 1.0, \"loss\": 0.000263471418293193, \"time-step\": 3602}, {\"accuracy\": 1.0, \"loss\": 0.00025781523436307907, \"time-step\": 3603}, {\"accuracy\": 1.0, \"loss\": 0.00026334484573453665, \"time-step\": 3604}, {\"accuracy\": 1.0, \"loss\": 0.0002576993720140308, \"time-step\": 3605}, {\"accuracy\": 1.0, \"loss\": 0.00026322409394197166, \"time-step\": 3606}, {\"accuracy\": 1.0, \"loss\": 0.00025757221737876534, \"time-step\": 3607}, {\"accuracy\": 1.0, \"loss\": 0.00026309414533898234, \"time-step\": 3608}, {\"accuracy\": 1.0, \"loss\": 0.00025745423045009375, \"time-step\": 3609}, {\"accuracy\": 1.0, \"loss\": 0.00026296888245269656, \"time-step\": 3610}, {\"accuracy\": 1.0, \"loss\": 0.0002573273377493024, \"time-step\": 3611}, {\"accuracy\": 1.0, \"loss\": 0.00026284687919542193, \"time-step\": 3612}, {\"accuracy\": 1.0, \"loss\": 0.00025721563724800944, \"time-step\": 3613}, {\"accuracy\": 1.0, \"loss\": 0.00026274105766788125, \"time-step\": 3614}, {\"accuracy\": 1.0, \"loss\": 0.00025711386115290225, \"time-step\": 3615}, {\"accuracy\": 1.0, \"loss\": 0.0002626272034831345, \"time-step\": 3616}, {\"accuracy\": 1.0, \"loss\": 0.0002569893258623779, \"time-step\": 3617}, {\"accuracy\": 1.0, \"loss\": 0.00026250066002830863, \"time-step\": 3618}, {\"accuracy\": 1.0, \"loss\": 0.00025687640300020576, \"time-step\": 3619}, {\"accuracy\": 1.0, \"loss\": 0.0002623932668939233, \"time-step\": 3620}, {\"accuracy\": 1.0, \"loss\": 0.00025676580844447017, \"time-step\": 3621}, {\"accuracy\": 1.0, \"loss\": 0.00026226480258628726, \"time-step\": 3622}, {\"accuracy\": 1.0, \"loss\": 0.00025663047563284636, \"time-step\": 3623}, {\"accuracy\": 1.0, \"loss\": 0.00026213316596113145, \"time-step\": 3624}, {\"accuracy\": 1.0, \"loss\": 0.0002565087634138763, \"time-step\": 3625}, {\"accuracy\": 1.0, \"loss\": 0.00026200624415650964, \"time-step\": 3626}, {\"accuracy\": 1.0, \"loss\": 0.0002563792222645134, \"time-step\": 3627}, {\"accuracy\": 1.0, \"loss\": 0.0002618725411593914, \"time-step\": 3628}, {\"accuracy\": 1.0, \"loss\": 0.0002562538720667362, \"time-step\": 3629}, {\"accuracy\": 1.0, \"loss\": 0.00026175432140007615, \"time-step\": 3630}, {\"accuracy\": 1.0, \"loss\": 0.00025614709011279047, \"time-step\": 3631}, {\"accuracy\": 1.0, \"loss\": 0.00026163813890889287, \"time-step\": 3632}, {\"accuracy\": 1.0, \"loss\": 0.00025601961533538997, \"time-step\": 3633}, {\"accuracy\": 1.0, \"loss\": 0.0002615084231365472, \"time-step\": 3634}, {\"accuracy\": 1.0, \"loss\": 0.0002558955457061529, \"time-step\": 3635}, {\"accuracy\": 1.0, \"loss\": 0.0002613936667330563, \"time-step\": 3636}, {\"accuracy\": 1.0, \"loss\": 0.000255788181675598, \"time-step\": 3637}, {\"accuracy\": 1.0, \"loss\": 0.0002612764074001461, \"time-step\": 3638}, {\"accuracy\": 1.0, \"loss\": 0.0002556609979365021, \"time-step\": 3639}, {\"accuracy\": 1.0, \"loss\": 0.00026114273350685835, \"time-step\": 3640}, {\"accuracy\": 1.0, \"loss\": 0.0002555394603405148, \"time-step\": 3641}, {\"accuracy\": 1.0, \"loss\": 0.0002610227675177157, \"time-step\": 3642}, {\"accuracy\": 1.0, \"loss\": 0.00025542135699652135, \"time-step\": 3643}, {\"accuracy\": 1.0, \"loss\": 0.0002609017537906766, \"time-step\": 3644}, {\"accuracy\": 1.0, \"loss\": 0.00025530828861519694, \"time-step\": 3645}, {\"accuracy\": 1.0, \"loss\": 0.0002607899368740618, \"time-step\": 3646}, {\"accuracy\": 1.0, \"loss\": 0.0002551988000050187, \"time-step\": 3647}, {\"accuracy\": 1.0, \"loss\": 0.000260671426076442, \"time-step\": 3648}, {\"accuracy\": 1.0, \"loss\": 0.0002550776698626578, \"time-step\": 3649}, {\"accuracy\": 1.0, \"loss\": 0.0002605508780106902, \"time-step\": 3650}, {\"accuracy\": 1.0, \"loss\": 0.00025495057343505323, \"time-step\": 3651}, {\"accuracy\": 1.0, \"loss\": 0.00026041895034722984, \"time-step\": 3652}, {\"accuracy\": 1.0, \"loss\": 0.0002548297052271664, \"time-step\": 3653}, {\"accuracy\": 1.0, \"loss\": 0.0002602977037895471, \"time-step\": 3654}, {\"accuracy\": 1.0, \"loss\": 0.0002547166950535029, \"time-step\": 3655}, {\"accuracy\": 1.0, \"loss\": 0.00026017939671874046, \"time-step\": 3656}, {\"accuracy\": 1.0, \"loss\": 0.0002545907918829471, \"time-step\": 3657}, {\"accuracy\": 1.0, \"loss\": 0.00026005360996350646, \"time-step\": 3658}, {\"accuracy\": 1.0, \"loss\": 0.00025447524967603385, \"time-step\": 3659}, {\"accuracy\": 1.0, \"loss\": 0.000259940221440047, \"time-step\": 3660}, {\"accuracy\": 1.0, \"loss\": 0.00025435921270400286, \"time-step\": 3661}, {\"accuracy\": 1.0, \"loss\": 0.0002598147257231176, \"time-step\": 3662}, {\"accuracy\": 1.0, \"loss\": 0.0002542401198297739, \"time-step\": 3663}, {\"accuracy\": 1.0, \"loss\": 0.00025969260605052114, \"time-step\": 3664}, {\"accuracy\": 1.0, \"loss\": 0.00025411363458260894, \"time-step\": 3665}, {\"accuracy\": 1.0, \"loss\": 0.0002595628902781755, \"time-step\": 3666}, {\"accuracy\": 1.0, \"loss\": 0.0002539896231610328, \"time-step\": 3667}, {\"accuracy\": 1.0, \"loss\": 0.000259440770605579, \"time-step\": 3668}, {\"accuracy\": 1.0, \"loss\": 0.00025387541973032057, \"time-step\": 3669}, {\"accuracy\": 1.0, \"loss\": 0.0002593216486275196, \"time-step\": 3670}, {\"accuracy\": 1.0, \"loss\": 0.0002537560067139566, \"time-step\": 3671}, {\"accuracy\": 1.0, \"loss\": 0.0002592109376564622, \"time-step\": 3672}, {\"accuracy\": 1.0, \"loss\": 0.00025365097098983824, \"time-step\": 3673}, {\"accuracy\": 1.0, \"loss\": 0.00025909909163601696, \"time-step\": 3674}, {\"accuracy\": 1.0, \"loss\": 0.000253539124969393, \"time-step\": 3675}, {\"accuracy\": 1.0, \"loss\": 0.0002589777868706733, \"time-step\": 3676}, {\"accuracy\": 1.0, \"loss\": 0.0002534115337766707, \"time-step\": 3677}, {\"accuracy\": 1.0, \"loss\": 0.00025884827482514083, \"time-step\": 3678}, {\"accuracy\": 1.0, \"loss\": 0.00025329823256470263, \"time-step\": 3679}, {\"accuracy\": 1.0, \"loss\": 0.0002587325870990753, \"time-step\": 3680}, {\"accuracy\": 1.0, \"loss\": 0.0002531801292207092, \"time-step\": 3681}, {\"accuracy\": 1.0, \"loss\": 0.0002586174523457885, \"time-step\": 3682}, {\"accuracy\": 1.0, \"loss\": 0.00025306674069724977, \"time-step\": 3683}, {\"accuracy\": 1.0, \"loss\": 0.0002584993781056255, \"time-step\": 3684}, {\"accuracy\": 1.0, \"loss\": 0.00025294499937444925, \"time-step\": 3685}, {\"accuracy\": 1.0, \"loss\": 0.00025836695567704737, \"time-step\": 3686}, {\"accuracy\": 1.0, \"loss\": 0.00025281612761318684, \"time-step\": 3687}, {\"accuracy\": 1.0, \"loss\": 0.0002582388697192073, \"time-step\": 3688}, {\"accuracy\": 1.0, \"loss\": 0.000252697616815567, \"time-step\": 3689}, {\"accuracy\": 1.0, \"loss\": 0.00025812373496592045, \"time-step\": 3690}, {\"accuracy\": 1.0, \"loss\": 0.00025258390814997256, \"time-step\": 3691}, {\"accuracy\": 1.0, \"loss\": 0.0002580129075795412, \"time-step\": 3692}, {\"accuracy\": 1.0, \"loss\": 0.0002524801529943943, \"time-step\": 3693}, {\"accuracy\": 1.0, \"loss\": 0.0002579021384008229, \"time-step\": 3694}, {\"accuracy\": 1.0, \"loss\": 0.00025236836518161, \"time-step\": 3695}, {\"accuracy\": 1.0, \"loss\": 0.00025779131101444364, \"time-step\": 3696}, {\"accuracy\": 1.0, \"loss\": 0.00025226062280125916, \"time-step\": 3697}, {\"accuracy\": 1.0, \"loss\": 0.0002576775150373578, \"time-step\": 3698}, {\"accuracy\": 1.0, \"loss\": 0.0002521506685297936, \"time-step\": 3699}, {\"accuracy\": 1.0, \"loss\": 0.00025756689137779176, \"time-step\": 3700}, {\"accuracy\": 1.0, \"loss\": 0.00025203541736118495, \"time-step\": 3701}, {\"accuracy\": 1.0, \"loss\": 0.000257446663454175, \"time-step\": 3702}, {\"accuracy\": 1.0, \"loss\": 0.00025191446184180677, \"time-step\": 3703}, {\"accuracy\": 1.0, \"loss\": 0.0002573218662291765, \"time-step\": 3704}, {\"accuracy\": 1.0, \"loss\": 0.0002518005494493991, \"time-step\": 3705}, {\"accuracy\": 1.0, \"loss\": 0.0002572064404375851, \"time-step\": 3706}, {\"accuracy\": 1.0, \"loss\": 0.00025167682906612754, \"time-step\": 3707}, {\"accuracy\": 1.0, \"loss\": 0.00025708205066621304, \"time-step\": 3708}, {\"accuracy\": 1.0, \"loss\": 0.00025156798074021935, \"time-step\": 3709}, {\"accuracy\": 1.0, \"loss\": 0.00025697355158627033, \"time-step\": 3710}, {\"accuracy\": 1.0, \"loss\": 0.000251462624873966, \"time-step\": 3711}, {\"accuracy\": 1.0, \"loss\": 0.00025686167646199465, \"time-step\": 3712}, {\"accuracy\": 1.0, \"loss\": 0.00025134015595540404, \"time-step\": 3713}, {\"accuracy\": 1.0, \"loss\": 0.0002567337651271373, \"time-step\": 3714}, {\"accuracy\": 1.0, \"loss\": 0.0002512193168513477, \"time-step\": 3715}, {\"accuracy\": 1.0, \"loss\": 0.0002566144394222647, \"time-step\": 3716}, {\"accuracy\": 1.0, \"loss\": 0.00025109926355071366, \"time-step\": 3717}, {\"accuracy\": 1.0, \"loss\": 0.0002564944152254611, \"time-step\": 3718}, {\"accuracy\": 1.0, \"loss\": 0.0002509891055524349, \"time-step\": 3719}, {\"accuracy\": 1.0, \"loss\": 0.00025637386715970933, \"time-step\": 3720}, {\"accuracy\": 1.0, \"loss\": 0.0002508685865905136, \"time-step\": 3721}, {\"accuracy\": 1.0, \"loss\": 0.00025625209673307836, \"time-step\": 3722}, {\"accuracy\": 1.0, \"loss\": 0.00025074981385841966, \"time-step\": 3723}, {\"accuracy\": 1.0, \"loss\": 0.00025614441256038845, \"time-step\": 3724}, {\"accuracy\": 1.0, \"loss\": 0.0002506523160263896, \"time-step\": 3725}, {\"accuracy\": 1.0, \"loss\": 0.0002560355351306498, \"time-step\": 3726}, {\"accuracy\": 1.0, \"loss\": 0.00025053651188500226, \"time-step\": 3727}, {\"accuracy\": 1.0, \"loss\": 0.0002559165586717427, \"time-step\": 3728}, {\"accuracy\": 1.0, \"loss\": 0.00025041913613677025, \"time-step\": 3729}, {\"accuracy\": 1.0, \"loss\": 0.00025580491637811065, \"time-step\": 3730}, {\"accuracy\": 1.0, \"loss\": 0.00025031869881786406, \"time-step\": 3731}, {\"accuracy\": 1.0, \"loss\": 0.0002556935651227832, \"time-step\": 3732}, {\"accuracy\": 1.0, \"loss\": 0.0002502002753317356, \"time-step\": 3733}, {\"accuracy\": 1.0, \"loss\": 0.00025557534536346793, \"time-step\": 3734}, {\"accuracy\": 1.0, \"loss\": 0.0002500888076610863, \"time-step\": 3735}, {\"accuracy\": 1.0, \"loss\": 0.0002554621023591608, \"time-step\": 3736}, {\"accuracy\": 1.0, \"loss\": 0.0002499705005902797, \"time-step\": 3737}, {\"accuracy\": 1.0, \"loss\": 0.0002553391386754811, \"time-step\": 3738}, {\"accuracy\": 1.0, \"loss\": 0.0002498579560779035, \"time-step\": 3739}, {\"accuracy\": 1.0, \"loss\": 0.00025522892246954143, \"time-step\": 3740}, {\"accuracy\": 1.0, \"loss\": 0.0002497516688890755, \"time-step\": 3741}, {\"accuracy\": 1.0, \"loss\": 0.0002551160578150302, \"time-step\": 3742}, {\"accuracy\": 1.0, \"loss\": 0.00024963909527286887, \"time-step\": 3743}, {\"accuracy\": 1.0, \"loss\": 0.0002550011267885566, \"time-step\": 3744}, {\"accuracy\": 1.0, \"loss\": 0.00024952838430181146, \"time-step\": 3745}, {\"accuracy\": 1.0, \"loss\": 0.0002548853517509997, \"time-step\": 3746}, {\"accuracy\": 1.0, \"loss\": 0.00024940853472799063, \"time-step\": 3747}, {\"accuracy\": 1.0, \"loss\": 0.0002547750773373991, \"time-step\": 3748}, {\"accuracy\": 1.0, \"loss\": 0.00024930801009759307, \"time-step\": 3749}, {\"accuracy\": 1.0, \"loss\": 0.0002546676550991833, \"time-step\": 3750}, {\"accuracy\": 1.0, \"loss\": 0.0002491992199793458, \"time-step\": 3751}, {\"accuracy\": 1.0, \"loss\": 0.0002545559545978904, \"time-step\": 3752}, {\"accuracy\": 1.0, \"loss\": 0.0002490855404175818, \"time-step\": 3753}, {\"accuracy\": 1.0, \"loss\": 0.00025443057529628277, \"time-step\": 3754}, {\"accuracy\": 1.0, \"loss\": 0.0002489597536623478, \"time-step\": 3755}, {\"accuracy\": 1.0, \"loss\": 0.0002543136361055076, \"time-step\": 3756}, {\"accuracy\": 1.0, \"loss\": 0.0002488582977093756, \"time-step\": 3757}, {\"accuracy\": 1.0, \"loss\": 0.0002542102010920644, \"time-step\": 3758}, {\"accuracy\": 1.0, \"loss\": 0.000248752417974174, \"time-step\": 3759}, {\"accuracy\": 1.0, \"loss\": 0.00025409949012100697, \"time-step\": 3760}, {\"accuracy\": 1.0, \"loss\": 0.0002486398443579674, \"time-step\": 3761}, {\"accuracy\": 1.0, \"loss\": 0.0002539787965361029, \"time-step\": 3762}, {\"accuracy\": 1.0, \"loss\": 0.0002485134464222938, \"time-step\": 3763}, {\"accuracy\": 1.0, \"loss\": 0.0002538490225560963, \"time-step\": 3764}, {\"accuracy\": 1.0, \"loss\": 0.0002484006399754435, \"time-step\": 3765}, {\"accuracy\": 1.0, \"loss\": 0.0002537378459237516, \"time-step\": 3766}, {\"accuracy\": 1.0, \"loss\": 0.0002482830313965678, \"time-step\": 3767}, {\"accuracy\": 1.0, \"loss\": 0.00025361953885294497, \"time-step\": 3768}, {\"accuracy\": 1.0, \"loss\": 0.00024818230303935707, \"time-step\": 3769}, {\"accuracy\": 1.0, \"loss\": 0.00025351185468025506, \"time-step\": 3770}, {\"accuracy\": 1.0, \"loss\": 0.00024806472356431186, \"time-step\": 3771}, {\"accuracy\": 1.0, \"loss\": 0.000253399892244488, \"time-step\": 3772}, {\"accuracy\": 1.0, \"loss\": 0.00024795992067083716, \"time-step\": 3773}, {\"accuracy\": 1.0, \"loss\": 0.00025328886113129556, \"time-step\": 3774}, {\"accuracy\": 1.0, \"loss\": 0.0002478445239830762, \"time-step\": 3775}, {\"accuracy\": 1.0, \"loss\": 0.00025317168910987675, \"time-step\": 3776}, {\"accuracy\": 1.0, \"loss\": 0.0002477396046742797, \"time-step\": 3777}, {\"accuracy\": 1.0, \"loss\": 0.0002530616184230894, \"time-step\": 3778}, {\"accuracy\": 1.0, \"loss\": 0.0002476239169482142, \"time-step\": 3779}, {\"accuracy\": 1.0, \"loss\": 0.0002529461053200066, \"time-step\": 3780}, {\"accuracy\": 1.0, \"loss\": 0.0002475102082826197, \"time-step\": 3781}, {\"accuracy\": 1.0, \"loss\": 0.0002528252953197807, \"time-step\": 3782}, {\"accuracy\": 1.0, \"loss\": 0.0002473962085787207, \"time-step\": 3783}, {\"accuracy\": 1.0, \"loss\": 0.00025271630147472024, \"time-step\": 3784}, {\"accuracy\": 1.0, \"loss\": 0.0002472950145602226, \"time-step\": 3785}, {\"accuracy\": 1.0, \"loss\": 0.0002526113821659237, \"time-step\": 3786}, {\"accuracy\": 1.0, \"loss\": 0.0002471903571859002, \"time-step\": 3787}, {\"accuracy\": 1.0, \"loss\": 0.00025250809267163277, \"time-step\": 3788}, {\"accuracy\": 1.0, \"loss\": 0.00024708424462005496, \"time-step\": 3789}, {\"accuracy\": 1.0, \"loss\": 0.0002523938019294292, \"time-step\": 3790}, {\"accuracy\": 1.0, \"loss\": 0.00024697015760466456, \"time-step\": 3791}, {\"accuracy\": 1.0, \"loss\": 0.0002522768045309931, \"time-step\": 3792}, {\"accuracy\": 1.0, \"loss\": 0.00024686206597834826, \"time-step\": 3793}, {\"accuracy\": 1.0, \"loss\": 0.00025216859648935497, \"time-step\": 3794}, {\"accuracy\": 1.0, \"loss\": 0.00024675833992660046, \"time-step\": 3795}, {\"accuracy\": 1.0, \"loss\": 0.00025206588907167315, \"time-step\": 3796}, {\"accuracy\": 1.0, \"loss\": 0.00024665164528414607, \"time-step\": 3797}, {\"accuracy\": 1.0, \"loss\": 0.0002519508998375386, \"time-step\": 3798}, {\"accuracy\": 1.0, \"loss\": 0.00024653642321936786, \"time-step\": 3799}, {\"accuracy\": 1.0, \"loss\": 0.0002518387045711279, \"time-step\": 3800}, {\"accuracy\": 1.0, \"loss\": 0.00024642288917675614, \"time-step\": 3801}, {\"accuracy\": 1.0, \"loss\": 0.0002517176035325974, \"time-step\": 3802}, {\"accuracy\": 1.0, \"loss\": 0.00024631188716739416, \"time-step\": 3803}, {\"accuracy\": 1.0, \"loss\": 0.0002516026434022933, \"time-step\": 3804}, {\"accuracy\": 1.0, \"loss\": 0.0002461911062709987, \"time-step\": 3805}, {\"accuracy\": 1.0, \"loss\": 0.00025148061104118824, \"time-step\": 3806}, {\"accuracy\": 1.0, \"loss\": 0.00024608068633824587, \"time-step\": 3807}, {\"accuracy\": 1.0, \"loss\": 0.0002513781073503196, \"time-step\": 3808}, {\"accuracy\": 1.0, \"loss\": 0.0002459849347360432, \"time-step\": 3809}, {\"accuracy\": 1.0, \"loss\": 0.0002512750797905028, \"time-step\": 3810}, {\"accuracy\": 1.0, \"loss\": 0.00024587794905528426, \"time-step\": 3811}, {\"accuracy\": 1.0, \"loss\": 0.00025116378674283624, \"time-step\": 3812}, {\"accuracy\": 1.0, \"loss\": 0.0002457681985106319, \"time-step\": 3813}, {\"accuracy\": 1.0, \"loss\": 0.00025106270913966, \"time-step\": 3814}, {\"accuracy\": 1.0, \"loss\": 0.0002456672955304384, \"time-step\": 3815}, {\"accuracy\": 1.0, \"loss\": 0.0002509451296646148, \"time-step\": 3816}, {\"accuracy\": 1.0, \"loss\": 0.00024556834250688553, \"time-step\": 3817}, {\"accuracy\": 1.0, \"loss\": 0.0002508536563254893, \"time-step\": 3818}, {\"accuracy\": 1.0, \"loss\": 0.0002454695641063154, \"time-step\": 3819}, {\"accuracy\": 1.0, \"loss\": 0.0002507526660338044, \"time-step\": 3820}, {\"accuracy\": 1.0, \"loss\": 0.0002453622000757605, \"time-step\": 3821}, {\"accuracy\": 1.0, \"loss\": 0.0002506393357180059, \"time-step\": 3822}, {\"accuracy\": 1.0, \"loss\": 0.0002452565240673721, \"time-step\": 3823}, {\"accuracy\": 1.0, \"loss\": 0.0002505304291844368, \"time-step\": 3824}, {\"accuracy\": 1.0, \"loss\": 0.00024514738470315933, \"time-step\": 3825}, {\"accuracy\": 1.0, \"loss\": 0.00025041939807124436, \"time-step\": 3826}, {\"accuracy\": 1.0, \"loss\": 0.00024504022439941764, \"time-step\": 3827}, {\"accuracy\": 1.0, \"loss\": 0.0002503128198441118, \"time-step\": 3828}, {\"accuracy\": 1.0, \"loss\": 0.0002449375460855663, \"time-step\": 3829}, {\"accuracy\": 1.0, \"loss\": 0.00025020953034982085, \"time-step\": 3830}, {\"accuracy\": 1.0, \"loss\": 0.0002448391169309616, \"time-step\": 3831}, {\"accuracy\": 1.0, \"loss\": 0.0002501094131730497, \"time-step\": 3832}, {\"accuracy\": 1.0, \"loss\": 0.000244734255829826, \"time-step\": 3833}, {\"accuracy\": 1.0, \"loss\": 0.0002500006521586329, \"time-step\": 3834}, {\"accuracy\": 1.0, \"loss\": 0.00024463384761475027, \"time-step\": 3835}, {\"accuracy\": 1.0, \"loss\": 0.00024989596568048, \"time-step\": 3836}, {\"accuracy\": 1.0, \"loss\": 0.0002445275313220918, \"time-step\": 3837}, {\"accuracy\": 1.0, \"loss\": 0.0002497857203707099, \"time-step\": 3838}, {\"accuracy\": 1.0, \"loss\": 0.00024442208814434707, \"time-step\": 3839}, {\"accuracy\": 1.0, \"loss\": 0.0002496805100236088, \"time-step\": 3840}, {\"accuracy\": 1.0, \"loss\": 0.0002443112898617983, \"time-step\": 3841}, {\"accuracy\": 1.0, \"loss\": 0.00024956202832981944, \"time-step\": 3842}, {\"accuracy\": 1.0, \"loss\": 0.00024420046247541904, \"time-step\": 3843}, {\"accuracy\": 1.0, \"loss\": 0.0002494577202014625, \"time-step\": 3844}, {\"accuracy\": 1.0, \"loss\": 0.00024410041805822402, \"time-step\": 3845}, {\"accuracy\": 1.0, \"loss\": 0.00024935329565778375, \"time-step\": 3846}, {\"accuracy\": 1.0, \"loss\": 0.0002439992385916412, \"time-step\": 3847}, {\"accuracy\": 1.0, \"loss\": 0.0002492505591362715, \"time-step\": 3848}, {\"accuracy\": 1.0, \"loss\": 0.00024388861493207514, \"time-step\": 3849}, {\"accuracy\": 1.0, \"loss\": 0.00024913332890719175, \"time-step\": 3850}, {\"accuracy\": 1.0, \"loss\": 0.00024377669615205377, \"time-step\": 3851}, {\"accuracy\": 1.0, \"loss\": 0.00024902127915993333, \"time-step\": 3852}, {\"accuracy\": 1.0, \"loss\": 0.00024367644800804555, \"time-step\": 3853}, {\"accuracy\": 1.0, \"loss\": 0.0002489235484972596, \"time-step\": 3854}, {\"accuracy\": 1.0, \"loss\": 0.00024357416259590536, \"time-step\": 3855}, {\"accuracy\": 1.0, \"loss\": 0.00024881414719857275, \"time-step\": 3856}, {\"accuracy\": 1.0, \"loss\": 0.00024347174621652812, \"time-step\": 3857}, {\"accuracy\": 1.0, \"loss\": 0.00024871493224054575, \"time-step\": 3858}, {\"accuracy\": 1.0, \"loss\": 0.0002433716581435874, \"time-step\": 3859}, {\"accuracy\": 1.0, \"loss\": 0.00024860879057087004, \"time-step\": 3860}, {\"accuracy\": 1.0, \"loss\": 0.00024326657876372337, \"time-step\": 3861}, {\"accuracy\": 1.0, \"loss\": 0.0002484981087036431, \"time-step\": 3862}, {\"accuracy\": 1.0, \"loss\": 0.00024315596965607256, \"time-step\": 3863}, {\"accuracy\": 1.0, \"loss\": 0.00024838067474775016, \"time-step\": 3864}, {\"accuracy\": 1.0, \"loss\": 0.000243040980421938, \"time-step\": 3865}, {\"accuracy\": 1.0, \"loss\": 0.0002482681884430349, \"time-step\": 3866}, {\"accuracy\": 1.0, \"loss\": 0.0002429312444292009, \"time-step\": 3867}, {\"accuracy\": 1.0, \"loss\": 0.0002481650444678962, \"time-step\": 3868}, {\"accuracy\": 1.0, \"loss\": 0.00024283878155983984, \"time-step\": 3869}, {\"accuracy\": 1.0, \"loss\": 0.00024806917645037174, \"time-step\": 3870}, {\"accuracy\": 1.0, \"loss\": 0.00024274535826407373, \"time-step\": 3871}, {\"accuracy\": 1.0, \"loss\": 0.0002479690592736006, \"time-step\": 3872}, {\"accuracy\": 1.0, \"loss\": 0.00024264177773147821, \"time-step\": 3873}, {\"accuracy\": 1.0, \"loss\": 0.0002478664682712406, \"time-step\": 3874}, {\"accuracy\": 1.0, \"loss\": 0.00024254103482235223, \"time-step\": 3875}, {\"accuracy\": 1.0, \"loss\": 0.00024776021018624306, \"time-step\": 3876}, {\"accuracy\": 1.0, \"loss\": 0.00024243876396212727, \"time-step\": 3877}, {\"accuracy\": 1.0, \"loss\": 0.0002476652152836323, \"time-step\": 3878}, {\"accuracy\": 1.0, \"loss\": 0.00024234180455096066, \"time-step\": 3879}, {\"accuracy\": 1.0, \"loss\": 0.00024755674530752003, \"time-step\": 3880}, {\"accuracy\": 1.0, \"loss\": 0.00024223510990850627, \"time-step\": 3881}, {\"accuracy\": 1.0, \"loss\": 0.0002474487409926951, \"time-step\": 3882}, {\"accuracy\": 1.0, \"loss\": 0.00024213692813646048, \"time-step\": 3883}, {\"accuracy\": 1.0, \"loss\": 0.00024734935141168535, \"time-step\": 3884}, {\"accuracy\": 1.0, \"loss\": 0.0002420334203634411, \"time-step\": 3885}, {\"accuracy\": 1.0, \"loss\": 0.00024724527611397207, \"time-step\": 3886}, {\"accuracy\": 1.0, \"loss\": 0.000241932095377706, \"time-step\": 3887}, {\"accuracy\": 1.0, \"loss\": 0.0002471411426085979, \"time-step\": 3888}, {\"accuracy\": 1.0, \"loss\": 0.00024183215282391757, \"time-step\": 3889}, {\"accuracy\": 1.0, \"loss\": 0.00024703770759515464, \"time-step\": 3890}, {\"accuracy\": 1.0, \"loss\": 0.0002417277719359845, \"time-step\": 3891}, {\"accuracy\": 1.0, \"loss\": 0.00024693002342246473, \"time-step\": 3892}, {\"accuracy\": 1.0, \"loss\": 0.00024162456975318491, \"time-step\": 3893}, {\"accuracy\": 1.0, \"loss\": 0.00024682446382939816, \"time-step\": 3894}, {\"accuracy\": 1.0, \"loss\": 0.0002415186754660681, \"time-step\": 3895}, {\"accuracy\": 1.0, \"loss\": 0.00024671503342688084, \"time-step\": 3896}, {\"accuracy\": 1.0, \"loss\": 0.0002414076734567061, \"time-step\": 3897}, {\"accuracy\": 1.0, \"loss\": 0.00024660289636813104, \"time-step\": 3898}, {\"accuracy\": 1.0, \"loss\": 0.00024129854864440858, \"time-step\": 3899}, {\"accuracy\": 1.0, \"loss\": 0.0002464874996803701, \"time-step\": 3900}, {\"accuracy\": 1.0, \"loss\": 0.00024118671717587858, \"time-step\": 3901}, {\"accuracy\": 1.0, \"loss\": 0.0002463795826770365, \"time-step\": 3902}, {\"accuracy\": 1.0, \"loss\": 0.00024108649813570082, \"time-step\": 3903}, {\"accuracy\": 1.0, \"loss\": 0.00024627652601338923, \"time-step\": 3904}, {\"accuracy\": 1.0, \"loss\": 0.00024098975700326264, \"time-step\": 3905}, {\"accuracy\": 1.0, \"loss\": 0.00024618051247671247, \"time-step\": 3906}, {\"accuracy\": 1.0, \"loss\": 0.00024088745703920722, \"time-step\": 3907}, {\"accuracy\": 1.0, \"loss\": 0.0002460675605107099, \"time-step\": 3908}, {\"accuracy\": 1.0, \"loss\": 0.00024078553542494774, \"time-step\": 3909}, {\"accuracy\": 1.0, \"loss\": 0.0002459700917825103, \"time-step\": 3910}, {\"accuracy\": 1.0, \"loss\": 0.00024067626509349793, \"time-step\": 3911}, {\"accuracy\": 1.0, \"loss\": 0.0002458585659042001, \"time-step\": 3912}, {\"accuracy\": 1.0, \"loss\": 0.0002405758568784222, \"time-step\": 3913}, {\"accuracy\": 1.0, \"loss\": 0.000245755014475435, \"time-step\": 3914}, {\"accuracy\": 1.0, \"loss\": 0.00024047735496424139, \"time-step\": 3915}, {\"accuracy\": 1.0, \"loss\": 0.0002456552756484598, \"time-step\": 3916}, {\"accuracy\": 1.0, \"loss\": 0.00024037202820181847, \"time-step\": 3917}, {\"accuracy\": 1.0, \"loss\": 0.000245551869738847, \"time-step\": 3918}, {\"accuracy\": 1.0, \"loss\": 0.0002402797108516097, \"time-step\": 3919}, {\"accuracy\": 1.0, \"loss\": 0.00024545169435441494, \"time-step\": 3920}, {\"accuracy\": 1.0, \"loss\": 0.000240182809648104, \"time-step\": 3921}, {\"accuracy\": 1.0, \"loss\": 0.0002453510242048651, \"time-step\": 3922}, {\"accuracy\": 1.0, \"loss\": 0.00024006990133784711, \"time-step\": 3923}, {\"accuracy\": 1.0, \"loss\": 0.00024523274623788893, \"time-step\": 3924}, {\"accuracy\": 1.0, \"loss\": 0.00023995968513190746, \"time-step\": 3925}, {\"accuracy\": 1.0, \"loss\": 0.00024511682568117976, \"time-step\": 3926}, {\"accuracy\": 1.0, \"loss\": 0.0002398497745161876, \"time-step\": 3927}, {\"accuracy\": 1.0, \"loss\": 0.00024502447922714055, \"time-step\": 3928}, {\"accuracy\": 1.0, \"loss\": 0.00023976448574103415, \"time-step\": 3929}, {\"accuracy\": 1.0, \"loss\": 0.00024492654483765364, \"time-step\": 3930}, {\"accuracy\": 1.0, \"loss\": 0.00023966937442310154, \"time-step\": 3931}, {\"accuracy\": 1.0, \"loss\": 0.00024483707966282964, \"time-step\": 3932}, {\"accuracy\": 1.0, \"loss\": 0.00023957584926392883, \"time-step\": 3933}, {\"accuracy\": 1.0, \"loss\": 0.0002447306760586798, \"time-step\": 3934}, {\"accuracy\": 1.0, \"loss\": 0.00023946925648488104, \"time-step\": 3935}, {\"accuracy\": 1.0, \"loss\": 0.000244619237491861, \"time-step\": 3936}, {\"accuracy\": 1.0, \"loss\": 0.00023936067009344697, \"time-step\": 3937}, {\"accuracy\": 1.0, \"loss\": 0.000244513270445168, \"time-step\": 3938}, {\"accuracy\": 1.0, \"loss\": 0.00023926154244691133, \"time-step\": 3939}, {\"accuracy\": 1.0, \"loss\": 0.0002444135316181928, \"time-step\": 3940}, {\"accuracy\": 1.0, \"loss\": 0.00023915557540021837, \"time-step\": 3941}, {\"accuracy\": 1.0, \"loss\": 0.0002443041594233364, \"time-step\": 3942}, {\"accuracy\": 1.0, \"loss\": 0.0002390518639003858, \"time-step\": 3943}, {\"accuracy\": 1.0, \"loss\": 0.0002441902179270983, \"time-step\": 3944}, {\"accuracy\": 1.0, \"loss\": 0.0002389375149505213, \"time-step\": 3945}, {\"accuracy\": 1.0, \"loss\": 0.0002440792741253972, \"time-step\": 3946}, {\"accuracy\": 1.0, \"loss\": 0.00023883560788817704, \"time-step\": 3947}, {\"accuracy\": 1.0, \"loss\": 0.0002439759555272758, \"time-step\": 3948}, {\"accuracy\": 1.0, \"loss\": 0.00023873716418165714, \"time-step\": 3949}, {\"accuracy\": 1.0, \"loss\": 0.00024387870507780463, \"time-step\": 3950}, {\"accuracy\": 1.0, \"loss\": 0.00023863499518483877, \"time-step\": 3951}, {\"accuracy\": 1.0, \"loss\": 0.00024377346562687308, \"time-step\": 3952}, {\"accuracy\": 1.0, \"loss\": 0.0002385381085332483, \"time-step\": 3953}, {\"accuracy\": 1.0, \"loss\": 0.00024368046433664858, \"time-step\": 3954}, {\"accuracy\": 1.0, \"loss\": 0.00023844980751164258, \"time-step\": 3955}, {\"accuracy\": 1.0, \"loss\": 0.00024358087102882564, \"time-step\": 3956}, {\"accuracy\": 1.0, \"loss\": 0.0002383481914876029, \"time-step\": 3957}, {\"accuracy\": 1.0, \"loss\": 0.00024348316946998239, \"time-step\": 3958}, {\"accuracy\": 1.0, \"loss\": 0.00023825245443731546, \"time-step\": 3959}, {\"accuracy\": 1.0, \"loss\": 0.00024338599178008735, \"time-step\": 3960}, {\"accuracy\": 1.0, \"loss\": 0.0002381622907705605, \"time-step\": 3961}, {\"accuracy\": 1.0, \"loss\": 0.00024328978906851262, \"time-step\": 3962}, {\"accuracy\": 1.0, \"loss\": 0.00023806566605344415, \"time-step\": 3963}, {\"accuracy\": 1.0, \"loss\": 0.00024319371732417494, \"time-step\": 3964}, {\"accuracy\": 1.0, \"loss\": 0.00023796988534741104, \"time-step\": 3965}, {\"accuracy\": 1.0, \"loss\": 0.00024309192667715251, \"time-step\": 3966}, {\"accuracy\": 1.0, \"loss\": 0.00023786466044839472, \"time-step\": 3967}, {\"accuracy\": 1.0, \"loss\": 0.00024298143398482352, \"time-step\": 3968}, {\"accuracy\": 1.0, \"loss\": 0.000237760366871953, \"time-step\": 3969}, {\"accuracy\": 1.0, \"loss\": 0.00024287264386657625, \"time-step\": 3970}, {\"accuracy\": 1.0, \"loss\": 0.00023764895740896463, \"time-step\": 3971}, {\"accuracy\": 1.0, \"loss\": 0.00024276980548165739, \"time-step\": 3972}, {\"accuracy\": 1.0, \"loss\": 0.00023756042355671525, \"time-step\": 3973}, {\"accuracy\": 1.0, \"loss\": 0.00024267585831694305, \"time-step\": 3974}, {\"accuracy\": 1.0, \"loss\": 0.00023745624639559537, \"time-step\": 3975}, {\"accuracy\": 1.0, \"loss\": 0.00024257114273495972, \"time-step\": 3976}, {\"accuracy\": 1.0, \"loss\": 0.00023735983995720744, \"time-step\": 3977}, {\"accuracy\": 1.0, \"loss\": 0.00024247600231319666, \"time-step\": 3978}, {\"accuracy\": 1.0, \"loss\": 0.00023726752260699868, \"time-step\": 3979}, {\"accuracy\": 1.0, \"loss\": 0.0002423866098979488, \"time-step\": 3980}, {\"accuracy\": 1.0, \"loss\": 0.00023718127340544015, \"time-step\": 3981}, {\"accuracy\": 1.0, \"loss\": 0.00024228691472671926, \"time-step\": 3982}, {\"accuracy\": 1.0, \"loss\": 0.0002370808506384492, \"time-step\": 3983}, {\"accuracy\": 1.0, \"loss\": 0.0002421847457299009, \"time-step\": 3984}, {\"accuracy\": 1.0, \"loss\": 0.0002369829308008775, \"time-step\": 3985}, {\"accuracy\": 1.0, \"loss\": 0.00024208557442761958, \"time-step\": 3986}, {\"accuracy\": 1.0, \"loss\": 0.00023688137298449874, \"time-step\": 3987}, {\"accuracy\": 1.0, \"loss\": 0.00024198382743634284, \"time-step\": 3988}, {\"accuracy\": 1.0, \"loss\": 0.00023678151774220169, \"time-step\": 3989}, {\"accuracy\": 1.0, \"loss\": 0.0002418840304017067, \"time-step\": 3990}, {\"accuracy\": 1.0, \"loss\": 0.00023669440997764468, \"time-step\": 3991}, {\"accuracy\": 1.0, \"loss\": 0.00024178596504498273, \"time-step\": 3992}, {\"accuracy\": 1.0, \"loss\": 0.0002365845866734162, \"time-step\": 3993}, {\"accuracy\": 1.0, \"loss\": 0.0002416765782982111, \"time-step\": 3994}, {\"accuracy\": 1.0, \"loss\": 0.00023648087517358363, \"time-step\": 3995}, {\"accuracy\": 1.0, \"loss\": 0.0002415704948361963, \"time-step\": 3996}, {\"accuracy\": 1.0, \"loss\": 0.00023637792037334293, \"time-step\": 3997}, {\"accuracy\": 1.0, \"loss\": 0.00024146767100319266, \"time-step\": 3998}, {\"accuracy\": 1.0, \"loss\": 0.00023628355120308697, \"time-step\": 3999}, {\"accuracy\": 1.0, \"loss\": 0.00024137928267009556, \"time-step\": 4000}, {\"accuracy\": 1.0, \"loss\": 0.00023620031424798071, \"time-step\": 4001}, {\"accuracy\": 1.0, \"loss\": 0.0002412932662991807, \"time-step\": 4002}, {\"accuracy\": 1.0, \"loss\": 0.00023610136122442782, \"time-step\": 4003}, {\"accuracy\": 1.0, \"loss\": 0.00024118111468851566, \"time-step\": 4004}, {\"accuracy\": 1.0, \"loss\": 0.0002359984937356785, \"time-step\": 4005}, {\"accuracy\": 1.0, \"loss\": 0.00024108338402584195, \"time-step\": 4006}, {\"accuracy\": 1.0, \"loss\": 0.00023590715136379004, \"time-step\": 4007}, {\"accuracy\": 1.0, \"loss\": 0.0002410020970273763, \"time-step\": 4008}, {\"accuracy\": 1.0, \"loss\": 0.00023583359143231064, \"time-step\": 4009}, {\"accuracy\": 1.0, \"loss\": 0.00024091952946037054, \"time-step\": 4010}, {\"accuracy\": 1.0, \"loss\": 0.00023574211809318513, \"time-step\": 4011}, {\"accuracy\": 1.0, \"loss\": 0.00024082585878204554, \"time-step\": 4012}, {\"accuracy\": 1.0, \"loss\": 0.00023564604634884745, \"time-step\": 4013}, {\"accuracy\": 1.0, \"loss\": 0.00024071540974546224, \"time-step\": 4014}, {\"accuracy\": 1.0, \"loss\": 0.0002355416218051687, \"time-step\": 4015}, {\"accuracy\": 1.0, \"loss\": 0.00024061676231212914, \"time-step\": 4016}, {\"accuracy\": 1.0, \"loss\": 0.0002354451280552894, \"time-step\": 4017}, {\"accuracy\": 1.0, \"loss\": 0.00024050910724326968, \"time-step\": 4018}, {\"accuracy\": 1.0, \"loss\": 0.00023533633793704212, \"time-step\": 4019}, {\"accuracy\": 1.0, \"loss\": 0.00024039957497734576, \"time-step\": 4020}, {\"accuracy\": 1.0, \"loss\": 0.00023523304844275117, \"time-step\": 4021}, {\"accuracy\": 1.0, \"loss\": 0.00024030620988924056, \"time-step\": 4022}, {\"accuracy\": 1.0, \"loss\": 0.00023514732311014086, \"time-step\": 4023}, {\"accuracy\": 1.0, \"loss\": 0.00024021443095989525, \"time-step\": 4024}, {\"accuracy\": 1.0, \"loss\": 0.00023505455465056002, \"time-step\": 4025}, {\"accuracy\": 1.0, \"loss\": 0.00024012316134758294, \"time-step\": 4026}, {\"accuracy\": 1.0, \"loss\": 0.0002349686110392213, \"time-step\": 4027}, {\"accuracy\": 1.0, \"loss\": 0.0002400282392045483, \"time-step\": 4028}, {\"accuracy\": 1.0, \"loss\": 0.00023486615100409836, \"time-step\": 4029}, {\"accuracy\": 1.0, \"loss\": 0.0002399283112026751, \"time-step\": 4030}, {\"accuracy\": 1.0, \"loss\": 0.0002347777335671708, \"time-step\": 4031}, {\"accuracy\": 1.0, \"loss\": 0.0002398363867541775, \"time-step\": 4032}, {\"accuracy\": 1.0, \"loss\": 0.0002346839610254392, \"time-step\": 4033}, {\"accuracy\": 1.0, \"loss\": 0.00023973775387275964, \"time-step\": 4034}, {\"accuracy\": 1.0, \"loss\": 0.00023457995848730206, \"time-step\": 4035}, {\"accuracy\": 1.0, \"loss\": 0.0002396283089183271, \"time-step\": 4036}, {\"accuracy\": 1.0, \"loss\": 0.00023447802232112736, \"time-step\": 4037}, {\"accuracy\": 1.0, \"loss\": 0.00023952312767505646, \"time-step\": 4038}, {\"accuracy\": 1.0, \"loss\": 0.00023437669733539224, \"time-step\": 4039}, {\"accuracy\": 1.0, \"loss\": 0.00023941868857946247, \"time-step\": 4040}, {\"accuracy\": 1.0, \"loss\": 0.00023427524138242006, \"time-step\": 4041}, {\"accuracy\": 1.0, \"loss\": 0.00023932332987897098, \"time-step\": 4042}, {\"accuracy\": 1.0, \"loss\": 0.00023418362252414227, \"time-step\": 4043}, {\"accuracy\": 1.0, \"loss\": 0.0002392329042777419, \"time-step\": 4044}, {\"accuracy\": 1.0, \"loss\": 0.00023409152345266193, \"time-step\": 4045}, {\"accuracy\": 1.0, \"loss\": 0.00023913028417155147, \"time-step\": 4046}, {\"accuracy\": 1.0, \"loss\": 0.00023399613564834, \"time-step\": 4047}, {\"accuracy\": 1.0, \"loss\": 0.00023904374393168837, \"time-step\": 4048}, {\"accuracy\": 1.0, \"loss\": 0.00023391238937620074, \"time-step\": 4049}, {\"accuracy\": 1.0, \"loss\": 0.00023895729100331664, \"time-step\": 4050}, {\"accuracy\": 1.0, \"loss\": 0.0002338227059226483, \"time-step\": 4051}, {\"accuracy\": 1.0, \"loss\": 0.0002388568827882409, \"time-step\": 4052}, {\"accuracy\": 1.0, \"loss\": 0.00023372324358206242, \"time-step\": 4053}, {\"accuracy\": 1.0, \"loss\": 0.00023876052000559866, \"time-step\": 4054}, {\"accuracy\": 1.0, \"loss\": 0.00023363249783869833, \"time-step\": 4055}, {\"accuracy\": 1.0, \"loss\": 0.00023866555420681834, \"time-step\": 4056}, {\"accuracy\": 1.0, \"loss\": 0.00023354185395874083, \"time-step\": 4057}, {\"accuracy\": 1.0, \"loss\": 0.0002385706757195294, \"time-step\": 4058}, {\"accuracy\": 1.0, \"loss\": 0.0002334435994271189, \"time-step\": 4059}, {\"accuracy\": 1.0, \"loss\": 0.00023847359989304096, \"time-step\": 4060}, {\"accuracy\": 1.0, \"loss\": 0.00023334380239248276, \"time-step\": 4061}, {\"accuracy\": 1.0, \"loss\": 0.00023837047046981752, \"time-step\": 4062}, {\"accuracy\": 1.0, \"loss\": 0.00023325537040363997, \"time-step\": 4063}, {\"accuracy\": 1.0, \"loss\": 0.00023828449775464833, \"time-step\": 4064}, {\"accuracy\": 1.0, \"loss\": 0.00023316765145864338, \"time-step\": 4065}, {\"accuracy\": 1.0, \"loss\": 0.00023818481713533401, \"time-step\": 4066}, {\"accuracy\": 1.0, \"loss\": 0.00023306442017201334, \"time-step\": 4067}, {\"accuracy\": 1.0, \"loss\": 0.00023809484264347702, \"time-step\": 4068}, {\"accuracy\": 1.0, \"loss\": 0.0002329740091226995, \"time-step\": 4069}, {\"accuracy\": 1.0, \"loss\": 0.00023798497568350285, \"time-step\": 4070}, {\"accuracy\": 1.0, \"loss\": 0.0002328698174096644, \"time-step\": 4071}, {\"accuracy\": 1.0, \"loss\": 0.00023788654652889818, \"time-step\": 4072}, {\"accuracy\": 1.0, \"loss\": 0.0002327780530322343, \"time-step\": 4073}, {\"accuracy\": 1.0, \"loss\": 0.0002377989439992234, \"time-step\": 4074}, {\"accuracy\": 1.0, \"loss\": 0.00023269503435585648, \"time-step\": 4075}, {\"accuracy\": 1.0, \"loss\": 0.00023770314874127507, \"time-step\": 4076}, {\"accuracy\": 1.0, \"loss\": 0.00023259667796082795, \"time-step\": 4077}, {\"accuracy\": 1.0, \"loss\": 0.00023760212934575975, \"time-step\": 4078}, {\"accuracy\": 1.0, \"loss\": 0.00023249057994689792, \"time-step\": 4079}, {\"accuracy\": 1.0, \"loss\": 0.0002374999166931957, \"time-step\": 4080}, {\"accuracy\": 1.0, \"loss\": 0.0002323998196516186, \"time-step\": 4081}, {\"accuracy\": 1.0, \"loss\": 0.00023741694167256355, \"time-step\": 4082}, {\"accuracy\": 1.0, \"loss\": 0.0002323246153537184, \"time-step\": 4083}, {\"accuracy\": 1.0, \"loss\": 0.0002373309398535639, \"time-step\": 4084}, {\"accuracy\": 1.0, \"loss\": 0.00023223547032102942, \"time-step\": 4085}, {\"accuracy\": 1.0, \"loss\": 0.00023724144557490945, \"time-step\": 4086}, {\"accuracy\": 1.0, \"loss\": 0.00023214129032567143, \"time-step\": 4087}, {\"accuracy\": 1.0, \"loss\": 0.00023713811242487282, \"time-step\": 4088}, {\"accuracy\": 1.0, \"loss\": 0.00023204214812722057, \"time-step\": 4089}, {\"accuracy\": 1.0, \"loss\": 0.00023704054183326662, \"time-step\": 4090}, {\"accuracy\": 1.0, \"loss\": 0.00023195015091914684, \"time-step\": 4091}, {\"accuracy\": 1.0, \"loss\": 0.00023695021809544414, \"time-step\": 4092}, {\"accuracy\": 1.0, \"loss\": 0.00023185776080936193, \"time-step\": 4093}, {\"accuracy\": 1.0, \"loss\": 0.00023685302585363388, \"time-step\": 4094}, {\"accuracy\": 1.0, \"loss\": 0.00023175819660536945, \"time-step\": 4095}, {\"accuracy\": 1.0, \"loss\": 0.00023674452677369118, \"time-step\": 4096}, {\"accuracy\": 1.0, \"loss\": 0.00023165380116552114, \"time-step\": 4097}, {\"accuracy\": 1.0, \"loss\": 0.00023664596665184945, \"time-step\": 4098}, {\"accuracy\": 1.0, \"loss\": 0.00023156423412729055, \"time-step\": 4099}, {\"accuracy\": 1.0, \"loss\": 0.0002365531399846077, \"time-step\": 4100}, {\"accuracy\": 1.0, \"loss\": 0.00023146695457398891, \"time-step\": 4101}, {\"accuracy\": 1.0, \"loss\": 0.0002364514657529071, \"time-step\": 4102}, {\"accuracy\": 1.0, \"loss\": 0.00023137878451962024, \"time-step\": 4103}, {\"accuracy\": 1.0, \"loss\": 0.00023638064158149064, \"time-step\": 4104}, {\"accuracy\": 1.0, \"loss\": 0.00023131133639253676, \"time-step\": 4105}, {\"accuracy\": 1.0, \"loss\": 0.00023629768111277372, \"time-step\": 4106}, {\"accuracy\": 1.0, \"loss\": 0.00023121936828829348, \"time-step\": 4107}, {\"accuracy\": 1.0, \"loss\": 0.00023619714193046093, \"time-step\": 4108}, {\"accuracy\": 1.0, \"loss\": 0.00023112952476367354, \"time-step\": 4109}, {\"accuracy\": 1.0, \"loss\": 0.00023611373035237193, \"time-step\": 4110}, {\"accuracy\": 1.0, \"loss\": 0.0002310403506271541, \"time-step\": 4111}, {\"accuracy\": 1.0, \"loss\": 0.00023602013243362308, \"time-step\": 4112}, {\"accuracy\": 1.0, \"loss\": 0.00023096032964531332, \"time-step\": 4113}, {\"accuracy\": 1.0, \"loss\": 0.00023593747755512595, \"time-step\": 4114}, {\"accuracy\": 1.0, \"loss\": 0.00023086296278052032, \"time-step\": 4115}, {\"accuracy\": 1.0, \"loss\": 0.00023584057635162026, \"time-step\": 4116}, {\"accuracy\": 1.0, \"loss\": 0.0002307740069227293, \"time-step\": 4117}, {\"accuracy\": 1.0, \"loss\": 0.00023574361694045365, \"time-step\": 4118}, {\"accuracy\": 1.0, \"loss\": 0.00023068516748026013, \"time-step\": 4119}, {\"accuracy\": 1.0, \"loss\": 0.00023565805167891085, \"time-step\": 4120}, {\"accuracy\": 1.0, \"loss\": 0.0002305905509274453, \"time-step\": 4121}, {\"accuracy\": 1.0, \"loss\": 0.00023555938969366252, \"time-step\": 4122}, {\"accuracy\": 1.0, \"loss\": 0.00023049712763167918, \"time-step\": 4123}, {\"accuracy\": 1.0, \"loss\": 0.00023546110605821013, \"time-step\": 4124}, {\"accuracy\": 1.0, \"loss\": 0.00023040236555971205, \"time-step\": 4125}, {\"accuracy\": 1.0, \"loss\": 0.00023537030210718513, \"time-step\": 4126}, {\"accuracy\": 1.0, \"loss\": 0.00023030827287584543, \"time-step\": 4127}, {\"accuracy\": 1.0, \"loss\": 0.00023526781296823174, \"time-step\": 4128}, {\"accuracy\": 1.0, \"loss\": 0.0002302136563230306, \"time-step\": 4129}, {\"accuracy\": 1.0, \"loss\": 0.0002351741713937372, \"time-step\": 4130}, {\"accuracy\": 1.0, \"loss\": 0.00023012513702269644, \"time-step\": 4131}, {\"accuracy\": 1.0, \"loss\": 0.00023508662707172334, \"time-step\": 4132}, {\"accuracy\": 1.0, \"loss\": 0.00023004035756457597, \"time-step\": 4133}, {\"accuracy\": 1.0, \"loss\": 0.00023499860253650695, \"time-step\": 4134}, {\"accuracy\": 1.0, \"loss\": 0.00022995071776676923, \"time-step\": 4135}, {\"accuracy\": 1.0, \"loss\": 0.00023490327293984592, \"time-step\": 4136}, {\"accuracy\": 1.0, \"loss\": 0.0002298590843565762, \"time-step\": 4137}, {\"accuracy\": 1.0, \"loss\": 0.00023481719836127013, \"time-step\": 4138}, {\"accuracy\": 1.0, \"loss\": 0.00022977135085966438, \"time-step\": 4139}, {\"accuracy\": 1.0, \"loss\": 0.00023472073371522129, \"time-step\": 4140}, {\"accuracy\": 1.0, \"loss\": 0.00022967442055232823, \"time-step\": 4141}, {\"accuracy\": 1.0, \"loss\": 0.00023461843375116587, \"time-step\": 4142}, {\"accuracy\": 1.0, \"loss\": 0.00022958384943194687, \"time-step\": 4143}, {\"accuracy\": 1.0, \"loss\": 0.00023453039466403425, \"time-step\": 4144}, {\"accuracy\": 1.0, \"loss\": 0.00022948614787310362, \"time-step\": 4145}, {\"accuracy\": 1.0, \"loss\": 0.0002344309032196179, \"time-step\": 4146}, {\"accuracy\": 1.0, \"loss\": 0.00022940232884138823, \"time-step\": 4147}, {\"accuracy\": 1.0, \"loss\": 0.00023435478215105832, \"time-step\": 4148}, {\"accuracy\": 1.0, \"loss\": 0.000229322089580819, \"time-step\": 4149}, {\"accuracy\": 1.0, \"loss\": 0.00023426010739058256, \"time-step\": 4150}, {\"accuracy\": 1.0, \"loss\": 0.00022922936477698386, \"time-step\": 4151}, {\"accuracy\": 1.0, \"loss\": 0.00023417505144607276, \"time-step\": 4152}, {\"accuracy\": 1.0, \"loss\": 0.00022914670989848673, \"time-step\": 4153}, {\"accuracy\": 1.0, \"loss\": 0.00023408219567500055, \"time-step\": 4154}, {\"accuracy\": 1.0, \"loss\": 0.00022905827790964395, \"time-step\": 4155}, {\"accuracy\": 1.0, \"loss\": 0.00023399628116749227, \"time-step\": 4156}, {\"accuracy\": 1.0, \"loss\": 0.00022897182498127222, \"time-step\": 4157}, {\"accuracy\": 1.0, \"loss\": 0.00023390556452795863, \"time-step\": 4158}, {\"accuracy\": 1.0, \"loss\": 0.00022887806699145585, \"time-step\": 4159}, {\"accuracy\": 1.0, \"loss\": 0.00023379820049740374, \"time-step\": 4160}, {\"accuracy\": 1.0, \"loss\": 0.0002287694369442761, \"time-step\": 4161}, {\"accuracy\": 1.0, \"loss\": 0.000233701997785829, \"time-step\": 4162}, {\"accuracy\": 1.0, \"loss\": 0.00022868052474223077, \"time-step\": 4163}, {\"accuracy\": 1.0, \"loss\": 0.00023360205523204058, \"time-step\": 4164}, {\"accuracy\": 1.0, \"loss\": 0.00022858571901451796, \"time-step\": 4165}, {\"accuracy\": 1.0, \"loss\": 0.00023351740674115717, \"time-step\": 4166}, {\"accuracy\": 1.0, \"loss\": 0.00022850184177514166, \"time-step\": 4167}, {\"accuracy\": 1.0, \"loss\": 0.0002334230812266469, \"time-step\": 4168}, {\"accuracy\": 1.0, \"loss\": 0.00022842051112093031, \"time-step\": 4169}, {\"accuracy\": 1.0, \"loss\": 0.00023334483557846397, \"time-step\": 4170}, {\"accuracy\": 1.0, \"loss\": 0.00022832959075458348, \"time-step\": 4171}, {\"accuracy\": 1.0, \"loss\": 0.00023325669462792575, \"time-step\": 4172}, {\"accuracy\": 1.0, \"loss\": 0.00022824878396932036, \"time-step\": 4173}, {\"accuracy\": 1.0, \"loss\": 0.00023316493025049567, \"time-step\": 4174}, {\"accuracy\": 1.0, \"loss\": 0.00022816445562057197, \"time-step\": 4175}, {\"accuracy\": 1.0, \"loss\": 0.00023308410891331732, \"time-step\": 4176}, {\"accuracy\": 1.0, \"loss\": 0.00022807154164183885, \"time-step\": 4177}, {\"accuracy\": 1.0, \"loss\": 0.00023298208543565124, \"time-step\": 4178}, {\"accuracy\": 1.0, \"loss\": 0.0002279801556142047, \"time-step\": 4179}, {\"accuracy\": 1.0, \"loss\": 0.00023289778619073331, \"time-step\": 4180}, {\"accuracy\": 1.0, \"loss\": 0.00022789098147768527, \"time-step\": 4181}, {\"accuracy\": 1.0, \"loss\": 0.0002327937982045114, \"time-step\": 4182}, {\"accuracy\": 1.0, \"loss\": 0.00022779390565119684, \"time-step\": 4183}, {\"accuracy\": 1.0, \"loss\": 0.00023270344536285847, \"time-step\": 4184}, {\"accuracy\": 1.0, \"loss\": 0.00022770819487050176, \"time-step\": 4185}, {\"accuracy\": 1.0, \"loss\": 0.00023261195747181773, \"time-step\": 4186}, {\"accuracy\": 1.0, \"loss\": 0.0002276122395414859, \"time-step\": 4187}, {\"accuracy\": 1.0, \"loss\": 0.00023250747472047806, \"time-step\": 4188}, {\"accuracy\": 1.0, \"loss\": 0.00022751775395590812, \"time-step\": 4189}, {\"accuracy\": 1.0, \"loss\": 0.00023242914176080376, \"time-step\": 4190}, {\"accuracy\": 1.0, \"loss\": 0.00022743799490854144, \"time-step\": 4191}, {\"accuracy\": 1.0, \"loss\": 0.00023233566025737673, \"time-step\": 4192}, {\"accuracy\": 1.0, \"loss\": 0.0002273436839459464, \"time-step\": 4193}, {\"accuracy\": 1.0, \"loss\": 0.00023224641336128116, \"time-step\": 4194}, {\"accuracy\": 1.0, \"loss\": 0.00022726200404576957, \"time-step\": 4195}, {\"accuracy\": 1.0, \"loss\": 0.0002321577339898795, \"time-step\": 4196}, {\"accuracy\": 1.0, \"loss\": 0.00022717725369147956, \"time-step\": 4197}, {\"accuracy\": 1.0, \"loss\": 0.00023208372294902802, \"time-step\": 4198}, {\"accuracy\": 1.0, \"loss\": 0.00022710558550897986, \"time-step\": 4199}, {\"accuracy\": 1.0, \"loss\": 0.0002320084604434669, \"time-step\": 4200}, {\"accuracy\": 1.0, \"loss\": 0.00022703339345753193, \"time-step\": 4201}, {\"accuracy\": 1.0, \"loss\": 0.00023192026128526777, \"time-step\": 4202}, {\"accuracy\": 1.0, \"loss\": 0.00022692675702273846, \"time-step\": 4203}, {\"accuracy\": 1.0, \"loss\": 0.0002318138285772875, \"time-step\": 4204}, {\"accuracy\": 1.0, \"loss\": 0.0002268384996568784, \"time-step\": 4205}, {\"accuracy\": 1.0, \"loss\": 0.00023172961664386094, \"time-step\": 4206}, {\"accuracy\": 1.0, \"loss\": 0.00022676023945678025, \"time-step\": 4207}, {\"accuracy\": 1.0, \"loss\": 0.00023165719176176935, \"time-step\": 4208}, {\"accuracy\": 1.0, \"loss\": 0.00022668651945423335, \"time-step\": 4209}, {\"accuracy\": 1.0, \"loss\": 0.00023156752286013216, \"time-step\": 4210}, {\"accuracy\": 1.0, \"loss\": 0.00022658664966002107, \"time-step\": 4211}, {\"accuracy\": 1.0, \"loss\": 0.0002314707380719483, \"time-step\": 4212}, {\"accuracy\": 1.0, \"loss\": 0.0002265023795189336, \"time-step\": 4213}, {\"accuracy\": 1.0, \"loss\": 0.00023138063261285424, \"time-step\": 4214}, {\"accuracy\": 1.0, \"loss\": 0.0002264070644741878, \"time-step\": 4215}, {\"accuracy\": 1.0, \"loss\": 0.000231295038247481, \"time-step\": 4216}, {\"accuracy\": 1.0, \"loss\": 0.00022634339984506369, \"time-step\": 4217}, {\"accuracy\": 1.0, \"loss\": 0.00023122267157305032, \"time-step\": 4218}, {\"accuracy\": 1.0, \"loss\": 0.00022625170822720975, \"time-step\": 4219}, {\"accuracy\": 1.0, \"loss\": 0.00023113029601518065, \"time-step\": 4220}, {\"accuracy\": 1.0, \"loss\": 0.00022616132628172636, \"time-step\": 4221}, {\"accuracy\": 1.0, \"loss\": 0.0002310365962330252, \"time-step\": 4222}, {\"accuracy\": 1.0, \"loss\": 0.00022607779828831553, \"time-step\": 4223}, {\"accuracy\": 1.0, \"loss\": 0.0002309520641574636, \"time-step\": 4224}, {\"accuracy\": 1.0, \"loss\": 0.00022599312069360167, \"time-step\": 4225}, {\"accuracy\": 1.0, \"loss\": 0.00023086986038833857, \"time-step\": 4226}, {\"accuracy\": 1.0, \"loss\": 0.00022591455490328372, \"time-step\": 4227}, {\"accuracy\": 1.0, \"loss\": 0.00023078659432940185, \"time-step\": 4228}, {\"accuracy\": 1.0, \"loss\": 0.00022582669043913484, \"time-step\": 4229}, {\"accuracy\": 1.0, \"loss\": 0.00023068187874741852, \"time-step\": 4230}, {\"accuracy\": 1.0, \"loss\": 0.00022573147725779563, \"time-step\": 4231}, {\"accuracy\": 1.0, \"loss\": 0.0002306005044374615, \"time-step\": 4232}, {\"accuracy\": 1.0, \"loss\": 0.00022564468963537365, \"time-step\": 4233}, {\"accuracy\": 1.0, \"loss\": 0.00023049481387715787, \"time-step\": 4234}, {\"accuracy\": 1.0, \"loss\": 0.00022553977032657713, \"time-step\": 4235}, {\"accuracy\": 1.0, \"loss\": 0.00023039807274471968, \"time-step\": 4236}, {\"accuracy\": 1.0, \"loss\": 0.0002254494174849242, \"time-step\": 4237}, {\"accuracy\": 1.0, \"loss\": 0.00023030824377201498, \"time-step\": 4238}, {\"accuracy\": 1.0, \"loss\": 0.0002253612910863012, \"time-step\": 4239}, {\"accuracy\": 1.0, \"loss\": 0.00023022093228064477, \"time-step\": 4240}, {\"accuracy\": 1.0, \"loss\": 0.00022527917462866753, \"time-step\": 4241}, {\"accuracy\": 1.0, \"loss\": 0.0002301335334777832, \"time-step\": 4242}, {\"accuracy\": 1.0, \"loss\": 0.0002251974947284907, \"time-step\": 4243}, {\"accuracy\": 1.0, \"loss\": 0.00023005896946415305, \"time-step\": 4244}, {\"accuracy\": 1.0, \"loss\": 0.00022512614668812603, \"time-step\": 4245}, {\"accuracy\": 1.0, \"loss\": 0.00022997389896772802, \"time-step\": 4246}, {\"accuracy\": 1.0, \"loss\": 0.00022503494983538985, \"time-step\": 4247}, {\"accuracy\": 1.0, \"loss\": 0.0002298897015862167, \"time-step\": 4248}, {\"accuracy\": 1.0, \"loss\": 0.00022495610755868256, \"time-step\": 4249}, {\"accuracy\": 1.0, \"loss\": 0.00022979866480454803, \"time-step\": 4250}, {\"accuracy\": 1.0, \"loss\": 0.0002248564560431987, \"time-step\": 4251}, {\"accuracy\": 1.0, \"loss\": 0.00022969700512476265, \"time-step\": 4252}, {\"accuracy\": 1.0, \"loss\": 0.00022476620506495237, \"time-step\": 4253}, {\"accuracy\": 1.0, \"loss\": 0.00022961391368880868, \"time-step\": 4254}, {\"accuracy\": 1.0, \"loss\": 0.00022468766837846488, \"time-step\": 4255}, {\"accuracy\": 1.0, \"loss\": 0.00022953527513891459, \"time-step\": 4256}, {\"accuracy\": 1.0, \"loss\": 0.00022461605840362608, \"time-step\": 4257}, {\"accuracy\": 1.0, \"loss\": 0.00022945512318983674, \"time-step\": 4258}, {\"accuracy\": 1.0, \"loss\": 0.0002245189098175615, \"time-step\": 4259}, {\"accuracy\": 1.0, \"loss\": 0.00022935192100703716, \"time-step\": 4260}, {\"accuracy\": 1.0, \"loss\": 0.00022443100169766694, \"time-step\": 4261}, {\"accuracy\": 1.0, \"loss\": 0.00022927123063709587, \"time-step\": 4262}, {\"accuracy\": 1.0, \"loss\": 0.0002243505441583693, \"time-step\": 4263}, {\"accuracy\": 1.0, \"loss\": 0.0002291819837410003, \"time-step\": 4264}, {\"accuracy\": 1.0, \"loss\": 0.000224261122639291, \"time-step\": 4265}, {\"accuracy\": 1.0, \"loss\": 0.00022909828112460673, \"time-step\": 4266}, {\"accuracy\": 1.0, \"loss\": 0.00022419120068661869, \"time-step\": 4267}, {\"accuracy\": 1.0, \"loss\": 0.00022902248019818217, \"time-step\": 4268}, {\"accuracy\": 1.0, \"loss\": 0.00022410275414586067, \"time-step\": 4269}, {\"accuracy\": 1.0, \"loss\": 0.00022894045105203986, \"time-step\": 4270}, {\"accuracy\": 1.0, \"loss\": 0.00022402990725822747, \"time-step\": 4271}, {\"accuracy\": 1.0, \"loss\": 0.00022886144870426506, \"time-step\": 4272}, {\"accuracy\": 1.0, \"loss\": 0.00022394274128600955, \"time-step\": 4273}, {\"accuracy\": 1.0, \"loss\": 0.00022876370348967612, \"time-step\": 4274}, {\"accuracy\": 1.0, \"loss\": 0.00022385208285413682, \"time-step\": 4275}, {\"accuracy\": 1.0, \"loss\": 0.00022867886582389474, \"time-step\": 4276}, {\"accuracy\": 1.0, \"loss\": 0.00022376888955477625, \"time-step\": 4277}, {\"accuracy\": 1.0, \"loss\": 0.00022859603632241488, \"time-step\": 4278}, {\"accuracy\": 1.0, \"loss\": 0.00022369356884155422, \"time-step\": 4279}, {\"accuracy\": 1.0, \"loss\": 0.00022851783432997763, \"time-step\": 4280}, {\"accuracy\": 1.0, \"loss\": 0.00022360990988090634, \"time-step\": 4281}, {\"accuracy\": 1.0, \"loss\": 0.00022843110491521657, \"time-step\": 4282}, {\"accuracy\": 1.0, \"loss\": 0.0002235223655588925, \"time-step\": 4283}, {\"accuracy\": 1.0, \"loss\": 0.0002283359644934535, \"time-step\": 4284}, {\"accuracy\": 1.0, \"loss\": 0.0002234366547781974, \"time-step\": 4285}, {\"accuracy\": 1.0, \"loss\": 0.00022824984625913203, \"time-step\": 4286}, {\"accuracy\": 1.0, \"loss\": 0.00022334494860842824, \"time-step\": 4287}, {\"accuracy\": 1.0, \"loss\": 0.0002281585184391588, \"time-step\": 4288}, {\"accuracy\": 1.0, \"loss\": 0.00022326041653286666, \"time-step\": 4289}, {\"accuracy\": 1.0, \"loss\": 0.00022806591005064547, \"time-step\": 4290}, {\"accuracy\": 1.0, \"loss\": 0.0002231671242043376, \"time-step\": 4291}, {\"accuracy\": 1.0, \"loss\": 0.0002279787149745971, \"time-step\": 4292}, {\"accuracy\": 1.0, \"loss\": 0.00022308695770334452, \"time-step\": 4293}, {\"accuracy\": 1.0, \"loss\": 0.00022789293143432587, \"time-step\": 4294}, {\"accuracy\": 1.0, \"loss\": 0.00022300047567114234, \"time-step\": 4295}, {\"accuracy\": 1.0, \"loss\": 0.00022780607105232775, \"time-step\": 4296}, {\"accuracy\": 1.0, \"loss\": 0.00022291428467724472, \"time-step\": 4297}, {\"accuracy\": 1.0, \"loss\": 0.00022771715885028243, \"time-step\": 4298}, {\"accuracy\": 1.0, \"loss\": 0.00022283292491920292, \"time-step\": 4299}, {\"accuracy\": 1.0, \"loss\": 0.00022763780725654215, \"time-step\": 4300}, {\"accuracy\": 1.0, \"loss\": 0.00022274887305684388, \"time-step\": 4301}, {\"accuracy\": 1.0, \"loss\": 0.00022754906967747957, \"time-step\": 4302}, {\"accuracy\": 1.0, \"loss\": 0.0002226663491455838, \"time-step\": 4303}, {\"accuracy\": 1.0, \"loss\": 0.00022746443573851138, \"time-step\": 4304}, {\"accuracy\": 1.0, \"loss\": 0.0002225800126325339, \"time-step\": 4305}, {\"accuracy\": 1.0, \"loss\": 0.0002273712889291346, \"time-step\": 4306}, {\"accuracy\": 1.0, \"loss\": 0.00022248380992095917, \"time-step\": 4307}, {\"accuracy\": 1.0, \"loss\": 0.00022728268231730908, \"time-step\": 4308}, {\"accuracy\": 1.0, \"loss\": 0.0002224152412964031, \"time-step\": 4309}, {\"accuracy\": 1.0, \"loss\": 0.00022720561537425965, \"time-step\": 4310}, {\"accuracy\": 1.0, \"loss\": 0.00022232948685996234, \"time-step\": 4311}, {\"accuracy\": 1.0, \"loss\": 0.00022712285863235593, \"time-step\": 4312}, {\"accuracy\": 1.0, \"loss\": 0.00022225288557820022, \"time-step\": 4313}, {\"accuracy\": 1.0, \"loss\": 0.00022704299772158265, \"time-step\": 4314}, {\"accuracy\": 1.0, \"loss\": 0.0002221705362899229, \"time-step\": 4315}, {\"accuracy\": 1.0, \"loss\": 0.00022695958614349365, \"time-step\": 4316}, {\"accuracy\": 1.0, \"loss\": 0.0002220878523075953, \"time-step\": 4317}, {\"accuracy\": 1.0, \"loss\": 0.000226874093641527, \"time-step\": 4318}, {\"accuracy\": 1.0, \"loss\": 0.00022201103274710476, \"time-step\": 4319}, {\"accuracy\": 1.0, \"loss\": 0.00022679958783555776, \"time-step\": 4320}, {\"accuracy\": 1.0, \"loss\": 0.00022193227778188884, \"time-step\": 4321}, {\"accuracy\": 1.0, \"loss\": 0.00022670951148029417, \"time-step\": 4322}, {\"accuracy\": 1.0, \"loss\": 0.00022183640976436436, \"time-step\": 4323}, {\"accuracy\": 1.0, \"loss\": 0.0002266184164909646, \"time-step\": 4324}, {\"accuracy\": 1.0, \"loss\": 0.00022176459606271237, \"time-step\": 4325}, {\"accuracy\": 1.0, \"loss\": 0.0002265449584228918, \"time-step\": 4326}, {\"accuracy\": 1.0, \"loss\": 0.0002216757566202432, \"time-step\": 4327}, {\"accuracy\": 1.0, \"loss\": 0.0002264570794068277, \"time-step\": 4328}, {\"accuracy\": 1.0, \"loss\": 0.0002216031280113384, \"time-step\": 4329}, {\"accuracy\": 1.0, \"loss\": 0.00022638072550762445, \"time-step\": 4330}, {\"accuracy\": 1.0, \"loss\": 0.0002215245767729357, \"time-step\": 4331}, {\"accuracy\": 1.0, \"loss\": 0.00022629351587966084, \"time-step\": 4332}, {\"accuracy\": 1.0, \"loss\": 0.00022143233218230307, \"time-step\": 4333}, {\"accuracy\": 1.0, \"loss\": 0.00022620419622398913, \"time-step\": 4334}, {\"accuracy\": 1.0, \"loss\": 0.0002213570405729115, \"time-step\": 4335}, {\"accuracy\": 1.0, \"loss\": 0.00022611870372202247, \"time-step\": 4336}, {\"accuracy\": 1.0, \"loss\": 0.0002212675753980875, \"time-step\": 4337}, {\"accuracy\": 1.0, \"loss\": 0.0002260306937387213, \"time-step\": 4338}, {\"accuracy\": 1.0, \"loss\": 0.0002211810351582244, \"time-step\": 4339}, {\"accuracy\": 1.0, \"loss\": 0.0002259442553622648, \"time-step\": 4340}, {\"accuracy\": 1.0, \"loss\": 0.00022109775454737246, \"time-step\": 4341}, {\"accuracy\": 1.0, \"loss\": 0.0002258621680084616, \"time-step\": 4342}, {\"accuracy\": 1.0, \"loss\": 0.00022101688955444843, \"time-step\": 4343}, {\"accuracy\": 1.0, \"loss\": 0.00022577587515115738, \"time-step\": 4344}, {\"accuracy\": 1.0, \"loss\": 0.0002209360245615244, \"time-step\": 4345}, {\"accuracy\": 1.0, \"loss\": 0.00022569928842131048, \"time-step\": 4346}, {\"accuracy\": 1.0, \"loss\": 0.00022084904776420444, \"time-step\": 4347}, {\"accuracy\": 1.0, \"loss\": 0.00022560678189620376, \"time-step\": 4348}, {\"accuracy\": 1.0, \"loss\": 0.0002207678189733997, \"time-step\": 4349}, {\"accuracy\": 1.0, \"loss\": 0.00022552510199602693, \"time-step\": 4350}, {\"accuracy\": 1.0, \"loss\": 0.0002206944045610726, \"time-step\": 4351}, {\"accuracy\": 1.0, \"loss\": 0.0002254479768453166, \"time-step\": 4352}, {\"accuracy\": 1.0, \"loss\": 0.00022060945047996938, \"time-step\": 4353}, {\"accuracy\": 1.0, \"loss\": 0.00022535996686201543, \"time-step\": 4354}, {\"accuracy\": 1.0, \"loss\": 0.0002205234341090545, \"time-step\": 4355}, {\"accuracy\": 1.0, \"loss\": 0.00022527013788931072, \"time-step\": 4356}, {\"accuracy\": 1.0, \"loss\": 0.00022044034267310053, \"time-step\": 4357}, {\"accuracy\": 1.0, \"loss\": 0.00022519516642205417, \"time-step\": 4358}, {\"accuracy\": 1.0, \"loss\": 0.00022036812151782215, \"time-step\": 4359}, {\"accuracy\": 1.0, \"loss\": 0.00022511872521135956, \"time-step\": 4360}, {\"accuracy\": 1.0, \"loss\": 0.0002202925825258717, \"time-step\": 4361}, {\"accuracy\": 1.0, \"loss\": 0.00022504750813823193, \"time-step\": 4362}, {\"accuracy\": 1.0, \"loss\": 0.0002202300966018811, \"time-step\": 4363}, {\"accuracy\": 1.0, \"loss\": 0.00022497479221783578, \"time-step\": 4364}, {\"accuracy\": 1.0, \"loss\": 0.0002201455645263195, \"time-step\": 4365}, {\"accuracy\": 1.0, \"loss\": 0.00022488771355710924, \"time-step\": 4366}, {\"accuracy\": 1.0, \"loss\": 0.0002200689777964726, \"time-step\": 4367}, {\"accuracy\": 1.0, \"loss\": 0.00022481090854853392, \"time-step\": 4368}, {\"accuracy\": 1.0, \"loss\": 0.0002199909940827638, \"time-step\": 4369}, {\"accuracy\": 1.0, \"loss\": 0.00022473132412414998, \"time-step\": 4370}, {\"accuracy\": 1.0, \"loss\": 0.00021990299865137786, \"time-step\": 4371}, {\"accuracy\": 1.0, \"loss\": 0.00022463365166913718, \"time-step\": 4372}, {\"accuracy\": 1.0, \"loss\": 0.00021980388555675745, \"time-step\": 4373}, {\"accuracy\": 1.0, \"loss\": 0.00022453867131844163, \"time-step\": 4374}, {\"accuracy\": 1.0, \"loss\": 0.0002197294234065339, \"time-step\": 4375}, {\"accuracy\": 1.0, \"loss\": 0.0002244757633889094, \"time-step\": 4376}, {\"accuracy\": 1.0, \"loss\": 0.00021966647182125598, \"time-step\": 4377}, {\"accuracy\": 1.0, \"loss\": 0.0002243925555376336, \"time-step\": 4378}, {\"accuracy\": 1.0, \"loss\": 0.00021958054276183248, \"time-step\": 4379}, {\"accuracy\": 1.0, \"loss\": 0.00022431460092775524, \"time-step\": 4380}, {\"accuracy\": 1.0, \"loss\": 0.00021950346126686782, \"time-step\": 4381}, {\"accuracy\": 1.0, \"loss\": 0.00022423110203817487, \"time-step\": 4382}, {\"accuracy\": 1.0, \"loss\": 0.00021942669991403818, \"time-step\": 4383}, {\"accuracy\": 1.0, \"loss\": 0.00022416669526137412, \"time-step\": 4384}, {\"accuracy\": 1.0, \"loss\": 0.0002193556138081476, \"time-step\": 4385}, {\"accuracy\": 1.0, \"loss\": 0.00022406980860978365, \"time-step\": 4386}, {\"accuracy\": 1.0, \"loss\": 0.00021925616601947695, \"time-step\": 4387}, {\"accuracy\": 1.0, \"loss\": 0.00022397743305191398, \"time-step\": 4388}, {\"accuracy\": 1.0, \"loss\": 0.00021918029233347625, \"time-step\": 4389}, {\"accuracy\": 1.0, \"loss\": 0.0002239002933492884, \"time-step\": 4390}, {\"accuracy\": 1.0, \"loss\": 0.0002191001403843984, \"time-step\": 4391}, {\"accuracy\": 1.0, \"loss\": 0.00022382446331903338, \"time-step\": 4392}, {\"accuracy\": 1.0, \"loss\": 0.00021903311426285654, \"time-step\": 4393}, {\"accuracy\": 1.0, \"loss\": 0.00022375653497874737, \"time-step\": 4394}, {\"accuracy\": 1.0, \"loss\": 0.00021895616373512894, \"time-step\": 4395}, {\"accuracy\": 1.0, \"loss\": 0.00022367030032910407, \"time-step\": 4396}, {\"accuracy\": 1.0, \"loss\": 0.00021887605544179678, \"time-step\": 4397}, {\"accuracy\": 1.0, \"loss\": 0.00022359474678523839, \"time-step\": 4398}, {\"accuracy\": 1.0, \"loss\": 0.00021880687563680112, \"time-step\": 4399}, {\"accuracy\": 1.0, \"loss\": 0.00022352834639605135, \"time-step\": 4400}, {\"accuracy\": 1.0, \"loss\": 0.0002187314094044268, \"time-step\": 4401}, {\"accuracy\": 1.0, \"loss\": 0.00022343514137901366, \"time-step\": 4402}, {\"accuracy\": 1.0, \"loss\": 0.00021864105656277388, \"time-step\": 4403}, {\"accuracy\": 1.0, \"loss\": 0.0002233502164017409, \"time-step\": 4404}, {\"accuracy\": 1.0, \"loss\": 0.000218555040191859, \"time-step\": 4405}, {\"accuracy\": 1.0, \"loss\": 0.00022325865575112402, \"time-step\": 4406}, {\"accuracy\": 1.0, \"loss\": 0.000218470289837569, \"time-step\": 4407}, {\"accuracy\": 1.0, \"loss\": 0.00022318342234939337, \"time-step\": 4408}, {\"accuracy\": 1.0, \"loss\": 0.00021840952103957534, \"time-step\": 4409}, {\"accuracy\": 1.0, \"loss\": 0.00022311527573037893, \"time-step\": 4410}, {\"accuracy\": 1.0, \"loss\": 0.000218322646105662, \"time-step\": 4411}, {\"accuracy\": 1.0, \"loss\": 0.00022302217257674783, \"time-step\": 4412}, {\"accuracy\": 1.0, \"loss\": 0.0002182387834182009, \"time-step\": 4413}, {\"accuracy\": 1.0, \"loss\": 0.0002229440724477172, \"time-step\": 4414}, {\"accuracy\": 1.0, \"loss\": 0.00021817215019837022, \"time-step\": 4415}, {\"accuracy\": 1.0, \"loss\": 0.00022287736646831036, \"time-step\": 4416}, {\"accuracy\": 1.0, \"loss\": 0.0002181010931963101, \"time-step\": 4417}, {\"accuracy\": 1.0, \"loss\": 0.00022280019766185433, \"time-step\": 4418}, {\"accuracy\": 1.0, \"loss\": 0.00021802380797453225, \"time-step\": 4419}, {\"accuracy\": 1.0, \"loss\": 0.00022272062778938562, \"time-step\": 4420}, {\"accuracy\": 1.0, \"loss\": 0.00021794674103148282, \"time-step\": 4421}, {\"accuracy\": 1.0, \"loss\": 0.00022264523431658745, \"time-step\": 4422}, {\"accuracy\": 1.0, \"loss\": 0.00021786376601085067, \"time-step\": 4423}, {\"accuracy\": 1.0, \"loss\": 0.00022255192743614316, \"time-step\": 4424}, {\"accuracy\": 1.0, \"loss\": 0.0002177804271923378, \"time-step\": 4425}, {\"accuracy\": 1.0, \"loss\": 0.00022247034939937294, \"time-step\": 4426}, {\"accuracy\": 1.0, \"loss\": 0.00021770513558294624, \"time-step\": 4427}, {\"accuracy\": 1.0, \"loss\": 0.0002223986084572971, \"time-step\": 4428}, {\"accuracy\": 1.0, \"loss\": 0.0002176292473450303, \"time-step\": 4429}, {\"accuracy\": 1.0, \"loss\": 0.00022231502225622535, \"time-step\": 4430}, {\"accuracy\": 1.0, \"loss\": 0.0002175460394937545, \"time-step\": 4431}, {\"accuracy\": 1.0, \"loss\": 0.0002222211769549176, \"time-step\": 4432}, {\"accuracy\": 1.0, \"loss\": 0.00021745463891420513, \"time-step\": 4433}, {\"accuracy\": 1.0, \"loss\": 0.00022214149066712707, \"time-step\": 4434}, {\"accuracy\": 1.0, \"loss\": 0.0002173753164242953, \"time-step\": 4435}, {\"accuracy\": 1.0, \"loss\": 0.00022205775894690305, \"time-step\": 4436}, {\"accuracy\": 1.0, \"loss\": 0.00021730130538344383, \"time-step\": 4437}, {\"accuracy\": 1.0, \"loss\": 0.00022198342776391655, \"time-step\": 4438}, {\"accuracy\": 1.0, \"loss\": 0.00021723481768276542, \"time-step\": 4439}, {\"accuracy\": 1.0, \"loss\": 0.0002219124580733478, \"time-step\": 4440}, {\"accuracy\": 1.0, \"loss\": 0.00021715115872211754, \"time-step\": 4441}, {\"accuracy\": 1.0, \"loss\": 0.00022183074906934053, \"time-step\": 4442}, {\"accuracy\": 1.0, \"loss\": 0.0002170750667573884, \"time-step\": 4443}, {\"accuracy\": 1.0, \"loss\": 0.00022175221238285303, \"time-step\": 4444}, {\"accuracy\": 1.0, \"loss\": 0.00021699289209209383, \"time-step\": 4445}, {\"accuracy\": 1.0, \"loss\": 0.00022166471171658486, \"time-step\": 4446}, {\"accuracy\": 1.0, \"loss\": 0.0002169141371268779, \"time-step\": 4447}, {\"accuracy\": 1.0, \"loss\": 0.00022158847423270345, \"time-step\": 4448}, {\"accuracy\": 1.0, \"loss\": 0.00021683830709662288, \"time-step\": 4449}, {\"accuracy\": 1.0, \"loss\": 0.0002215030835941434, \"time-step\": 4450}, {\"accuracy\": 1.0, \"loss\": 0.00021674837626051158, \"time-step\": 4451}, {\"accuracy\": 1.0, \"loss\": 0.00022141104273032397, \"time-step\": 4452}, {\"accuracy\": 1.0, \"loss\": 0.0002166660997318104, \"time-step\": 4453}, {\"accuracy\": 1.0, \"loss\": 0.00022133838501758873, \"time-step\": 4454}, {\"accuracy\": 1.0, \"loss\": 0.0002165994665119797, \"time-step\": 4455}, {\"accuracy\": 1.0, \"loss\": 0.0002212690160376951, \"time-step\": 4456}, {\"accuracy\": 1.0, \"loss\": 0.0002165303740184754, \"time-step\": 4457}, {\"accuracy\": 1.0, \"loss\": 0.00022118870401754975, \"time-step\": 4458}, {\"accuracy\": 1.0, \"loss\": 0.000216450949665159, \"time-step\": 4459}, {\"accuracy\": 1.0, \"loss\": 0.00022111469297669828, \"time-step\": 4460}, {\"accuracy\": 1.0, \"loss\": 0.00021637427562382072, \"time-step\": 4461}, {\"accuracy\": 1.0, \"loss\": 0.0002210391394328326, \"time-step\": 4462}, {\"accuracy\": 1.0, \"loss\": 0.00021630268020089716, \"time-step\": 4463}, {\"accuracy\": 1.0, \"loss\": 0.0002209616213804111, \"time-step\": 4464}, {\"accuracy\": 1.0, \"loss\": 0.0002162348828278482, \"time-step\": 4465}, {\"accuracy\": 1.0, \"loss\": 0.00022089733101893216, \"time-step\": 4466}, {\"accuracy\": 1.0, \"loss\": 0.00021616036246996373, \"time-step\": 4467}, {\"accuracy\": 1.0, \"loss\": 0.0002208132646046579, \"time-step\": 4468}, {\"accuracy\": 1.0, \"loss\": 0.0002160774020012468, \"time-step\": 4469}, {\"accuracy\": 1.0, \"loss\": 0.00022072730644140393, \"time-step\": 4470}, {\"accuracy\": 1.0, \"loss\": 0.00021599570754915476, \"time-step\": 4471}, {\"accuracy\": 1.0, \"loss\": 0.00022064935183152556, \"time-step\": 4472}, {\"accuracy\": 1.0, \"loss\": 0.0002159170398954302, \"time-step\": 4473}, {\"accuracy\": 1.0, \"loss\": 0.00022056422312743962, \"time-step\": 4474}, {\"accuracy\": 1.0, \"loss\": 0.0002158329007215798, \"time-step\": 4475}, {\"accuracy\": 1.0, \"loss\": 0.00022047229867894202, \"time-step\": 4476}, {\"accuracy\": 1.0, \"loss\": 0.00021575008577201515, \"time-step\": 4477}, {\"accuracy\": 1.0, \"loss\": 0.0002203956973971799, \"time-step\": 4478}, {\"accuracy\": 1.0, \"loss\": 0.00021566108625847846, \"time-step\": 4479}, {\"accuracy\": 1.0, \"loss\": 0.00022029738465789706, \"time-step\": 4480}, {\"accuracy\": 1.0, \"loss\": 0.00021557747095357627, \"time-step\": 4481}, {\"accuracy\": 1.0, \"loss\": 0.00022022012853994966, \"time-step\": 4482}, {\"accuracy\": 1.0, \"loss\": 0.000215508189285174, \"time-step\": 4483}, {\"accuracy\": 1.0, \"loss\": 0.00022014421119820327, \"time-step\": 4484}, {\"accuracy\": 1.0, \"loss\": 0.0002154277462977916, \"time-step\": 4485}, {\"accuracy\": 1.0, \"loss\": 0.00022007458028383553, \"time-step\": 4486}, {\"accuracy\": 1.0, \"loss\": 0.00021536718122661114, \"time-step\": 4487}, {\"accuracy\": 1.0, \"loss\": 0.00022001111938152462, \"time-step\": 4488}, {\"accuracy\": 1.0, \"loss\": 0.00021529599325731397, \"time-step\": 4489}, {\"accuracy\": 1.0, \"loss\": 0.000219933339394629, \"time-step\": 4490}, {\"accuracy\": 1.0, \"loss\": 0.00021521815506275743, \"time-step\": 4491}, {\"accuracy\": 1.0, \"loss\": 0.00021985432249493897, \"time-step\": 4492}, {\"accuracy\": 1.0, \"loss\": 0.00021515099797397852, \"time-step\": 4493}, {\"accuracy\": 1.0, \"loss\": 0.00021978240692988038, \"time-step\": 4494}, {\"accuracy\": 1.0, \"loss\": 0.00021507355268113315, \"time-step\": 4495}, {\"accuracy\": 1.0, \"loss\": 0.00021970200759824365, \"time-step\": 4496}, {\"accuracy\": 1.0, \"loss\": 0.00021498918067663908, \"time-step\": 4497}, {\"accuracy\": 1.0, \"loss\": 0.00021962652681395411, \"time-step\": 4498}, {\"accuracy\": 1.0, \"loss\": 0.00021493418898899108, \"time-step\": 4499}, {\"accuracy\": 1.0, \"loss\": 0.00021956567070446908, \"time-step\": 4500}, {\"accuracy\": 1.0, \"loss\": 0.0002148615021724254, \"time-step\": 4501}, {\"accuracy\": 1.0, \"loss\": 0.00021948909852653742, \"time-step\": 4502}, {\"accuracy\": 1.0, \"loss\": 0.00021478421695064753, \"time-step\": 4503}, {\"accuracy\": 1.0, \"loss\": 0.00021941581508144736, \"time-step\": 4504}, {\"accuracy\": 1.0, \"loss\": 0.00021472184744197875, \"time-step\": 4505}, {\"accuracy\": 1.0, \"loss\": 0.00021934451069682837, \"time-step\": 4506}, {\"accuracy\": 1.0, \"loss\": 0.00021464229212142527, \"time-step\": 4507}, {\"accuracy\": 1.0, \"loss\": 0.00021926721092313528, \"time-step\": 4508}, {\"accuracy\": 1.0, \"loss\": 0.00021456620015669614, \"time-step\": 4509}, {\"accuracy\": 1.0, \"loss\": 0.00021918507991358638, \"time-step\": 4510}, {\"accuracy\": 1.0, \"loss\": 0.00021449037012644112, \"time-step\": 4511}, {\"accuracy\": 1.0, \"loss\": 0.00021910107170697302, \"time-step\": 4512}, {\"accuracy\": 1.0, \"loss\": 0.0002144081809092313, \"time-step\": 4513}, {\"accuracy\": 1.0, \"loss\": 0.00021902090520597994, \"time-step\": 4514}, {\"accuracy\": 1.0, \"loss\": 0.00021431544155348092, \"time-step\": 4515}, {\"accuracy\": 1.0, \"loss\": 0.00021891933283768594, \"time-step\": 4516}, {\"accuracy\": 1.0, \"loss\": 0.00021422830468509346, \"time-step\": 4517}, {\"accuracy\": 1.0, \"loss\": 0.00021884412853978574, \"time-step\": 4518}, {\"accuracy\": 1.0, \"loss\": 0.00021415352239273489, \"time-step\": 4519}, {\"accuracy\": 1.0, \"loss\": 0.0002187637146562338, \"time-step\": 4520}, {\"accuracy\": 1.0, \"loss\": 0.00021408905740827322, \"time-step\": 4521}, {\"accuracy\": 1.0, \"loss\": 0.00021869760530535132, \"time-step\": 4522}, {\"accuracy\": 1.0, \"loss\": 0.00021401103003881872, \"time-step\": 4523}, {\"accuracy\": 1.0, \"loss\": 0.0002186219790019095, \"time-step\": 4524}, {\"accuracy\": 1.0, \"loss\": 0.00021394764189608395, \"time-step\": 4525}, {\"accuracy\": 1.0, \"loss\": 0.00021856279636267573, \"time-step\": 4526}, {\"accuracy\": 1.0, \"loss\": 0.00021388499590102583, \"time-step\": 4527}, {\"accuracy\": 1.0, \"loss\": 0.00021849042968824506, \"time-step\": 4528}, {\"accuracy\": 1.0, \"loss\": 0.0002138125419151038, \"time-step\": 4529}, {\"accuracy\": 1.0, \"loss\": 0.0002184174518333748, \"time-step\": 4530}, {\"accuracy\": 1.0, \"loss\": 0.00021374074276536703, \"time-step\": 4531}, {\"accuracy\": 1.0, \"loss\": 0.0002183356846217066, \"time-step\": 4532}, {\"accuracy\": 1.0, \"loss\": 0.0002136582916136831, \"time-step\": 4533}, {\"accuracy\": 1.0, \"loss\": 0.0002182529424317181, \"time-step\": 4534}, {\"accuracy\": 1.0, \"loss\": 0.00021357115474529564, \"time-step\": 4535}, {\"accuracy\": 1.0, \"loss\": 0.00021816734806634486, \"time-step\": 4536}, {\"accuracy\": 1.0, \"loss\": 0.00021349076996557415, \"time-step\": 4537}, {\"accuracy\": 1.0, \"loss\": 0.00021807879966218024, \"time-step\": 4538}, {\"accuracy\": 1.0, \"loss\": 0.00021341019601095468, \"time-step\": 4539}, {\"accuracy\": 1.0, \"loss\": 0.00021800339163746685, \"time-step\": 4540}, {\"accuracy\": 1.0, \"loss\": 0.00021333077165763825, \"time-step\": 4541}, {\"accuracy\": 1.0, \"loss\": 0.00021792163897771388, \"time-step\": 4542}, {\"accuracy\": 1.0, \"loss\": 0.00021325593115761876, \"time-step\": 4543}, {\"accuracy\": 1.0, \"loss\": 0.00021785174612887204, \"time-step\": 4544}, {\"accuracy\": 1.0, \"loss\": 0.00021319017105270177, \"time-step\": 4545}, {\"accuracy\": 1.0, \"loss\": 0.00021777926303911954, \"time-step\": 4546}, {\"accuracy\": 1.0, \"loss\": 0.0002131169312633574, \"time-step\": 4547}, {\"accuracy\": 1.0, \"loss\": 0.00021770974854007363, \"time-step\": 4548}, {\"accuracy\": 1.0, \"loss\": 0.00021304610709194094, \"time-step\": 4549}, {\"accuracy\": 1.0, \"loss\": 0.00021763110999017954, \"time-step\": 4550}, {\"accuracy\": 1.0, \"loss\": 0.00021297374041751027, \"time-step\": 4551}, {\"accuracy\": 1.0, \"loss\": 0.00021756539354100823, \"time-step\": 4552}, {\"accuracy\": 1.0, \"loss\": 0.00021291356824804097, \"time-step\": 4553}, {\"accuracy\": 1.0, \"loss\": 0.00021749948791693896, \"time-step\": 4554}, {\"accuracy\": 1.0, \"loss\": 0.00021284671674948186, \"time-step\": 4555}, {\"accuracy\": 1.0, \"loss\": 0.0002174342080252245, \"time-step\": 4556}, {\"accuracy\": 1.0, \"loss\": 0.0002127791231032461, \"time-step\": 4557}, {\"accuracy\": 1.0, \"loss\": 0.00021735763584729284, \"time-step\": 4558}, {\"accuracy\": 1.0, \"loss\": 0.00021270128490868956, \"time-step\": 4559}, {\"accuracy\": 1.0, \"loss\": 0.0002172800013795495, \"time-step\": 4560}, {\"accuracy\": 1.0, \"loss\": 0.00021262920927256346, \"time-step\": 4561}, {\"accuracy\": 1.0, \"loss\": 0.0002172092063119635, \"time-step\": 4562}, {\"accuracy\": 1.0, \"loss\": 0.000212571831070818, \"time-step\": 4563}, {\"accuracy\": 1.0, \"loss\": 0.00021714408649131656, \"time-step\": 4564}, {\"accuracy\": 1.0, \"loss\": 0.00021248511620797217, \"time-step\": 4565}, {\"accuracy\": 1.0, \"loss\": 0.00021705357357859612, \"time-step\": 4566}, {\"accuracy\": 1.0, \"loss\": 0.00021240927162580192, \"time-step\": 4567}, {\"accuracy\": 1.0, \"loss\": 0.00021698047930840403, \"time-step\": 4568}, {\"accuracy\": 1.0, \"loss\": 0.00021233427105471492, \"time-step\": 4569}, {\"accuracy\": 1.0, \"loss\": 0.00021690884022973478, \"time-step\": 4570}, {\"accuracy\": 1.0, \"loss\": 0.00021226992248557508, \"time-step\": 4571}, {\"accuracy\": 1.0, \"loss\": 0.0002168371283914894, \"time-step\": 4572}, {\"accuracy\": 1.0, \"loss\": 0.0002121929283021018, \"time-step\": 4573}, {\"accuracy\": 1.0, \"loss\": 0.000216758344322443, \"time-step\": 4574}, {\"accuracy\": 1.0, \"loss\": 0.00021211666171438992, \"time-step\": 4575}, {\"accuracy\": 1.0, \"loss\": 0.00021668378030881286, \"time-step\": 4576}, {\"accuracy\": 1.0, \"loss\": 0.00021204203949309886, \"time-step\": 4577}, {\"accuracy\": 1.0, \"loss\": 0.0002166021877201274, \"time-step\": 4578}, {\"accuracy\": 1.0, \"loss\": 0.00021196833404246718, \"time-step\": 4579}, {\"accuracy\": 1.0, \"loss\": 0.00021653161093126982, \"time-step\": 4580}, {\"accuracy\": 1.0, \"loss\": 0.00021189788822084665, \"time-step\": 4581}, {\"accuracy\": 1.0, \"loss\": 0.00021646308596245944, \"time-step\": 4582}, {\"accuracy\": 1.0, \"loss\": 0.0002118306583724916, \"time-step\": 4583}, {\"accuracy\": 1.0, \"loss\": 0.00021638188627548516, \"time-step\": 4584}, {\"accuracy\": 1.0, \"loss\": 0.00021174983703531325, \"time-step\": 4585}, {\"accuracy\": 1.0, \"loss\": 0.00021630658011417836, \"time-step\": 4586}, {\"accuracy\": 1.0, \"loss\": 0.00021167278464417905, \"time-step\": 4587}, {\"accuracy\": 1.0, \"loss\": 0.00021623022621497512, \"time-step\": 4588}, {\"accuracy\": 1.0, \"loss\": 0.0002115992538165301, \"time-step\": 4589}, {\"accuracy\": 1.0, \"loss\": 0.0002161511074518785, \"time-step\": 4590}, {\"accuracy\": 1.0, \"loss\": 0.0002115208626491949, \"time-step\": 4591}, {\"accuracy\": 1.0, \"loss\": 0.00021606522204820067, \"time-step\": 4592}, {\"accuracy\": 1.0, \"loss\": 0.00021144075435586274, \"time-step\": 4593}, {\"accuracy\": 1.0, \"loss\": 0.0002159853611374274, \"time-step\": 4594}, {\"accuracy\": 1.0, \"loss\": 0.0002113626105710864, \"time-step\": 4595}, {\"accuracy\": 1.0, \"loss\": 0.00021590141113847494, \"time-step\": 4596}, {\"accuracy\": 1.0, \"loss\": 0.00021127951913513243, \"time-step\": 4597}, {\"accuracy\": 1.0, \"loss\": 0.000215827370993793, \"time-step\": 4598}, {\"accuracy\": 1.0, \"loss\": 0.00021121359895914793, \"time-step\": 4599}, {\"accuracy\": 1.0, \"loss\": 0.00021575573191512376, \"time-step\": 4600}, {\"accuracy\": 1.0, \"loss\": 0.00021113420370966196, \"time-step\": 4601}, {\"accuracy\": 1.0, \"loss\": 0.00021568144438788295, \"time-step\": 4602}, {\"accuracy\": 1.0, \"loss\": 0.0002110593777615577, \"time-step\": 4603}, {\"accuracy\": 1.0, \"loss\": 0.00021559525339398533, \"time-step\": 4604}, {\"accuracy\": 1.0, \"loss\": 0.0002109905326506123, \"time-step\": 4605}, {\"accuracy\": 1.0, \"loss\": 0.00021553867554757744, \"time-step\": 4606}, {\"accuracy\": 1.0, \"loss\": 0.0002109320048475638, \"time-step\": 4607}, {\"accuracy\": 1.0, \"loss\": 0.00021547106734942645, \"time-step\": 4608}, {\"accuracy\": 1.0, \"loss\": 0.00021086887863930315, \"time-step\": 4609}, {\"accuracy\": 1.0, \"loss\": 0.00021541227761190385, \"time-step\": 4610}, {\"accuracy\": 1.0, \"loss\": 0.00021080704755149782, \"time-step\": 4611}, {\"accuracy\": 1.0, \"loss\": 0.00021534225379582494, \"time-step\": 4612}, {\"accuracy\": 1.0, \"loss\": 0.00021073022799100727, \"time-step\": 4613}, {\"accuracy\": 1.0, \"loss\": 0.00021525601914618164, \"time-step\": 4614}, {\"accuracy\": 1.0, \"loss\": 0.00021064982865937054, \"time-step\": 4615}, {\"accuracy\": 1.0, \"loss\": 0.00021518337598536164, \"time-step\": 4616}, {\"accuracy\": 1.0, \"loss\": 0.0002105823950842023, \"time-step\": 4617}, {\"accuracy\": 1.0, \"loss\": 0.00021511776139959693, \"time-step\": 4618}, {\"accuracy\": 1.0, \"loss\": 0.00021052219381090254, \"time-step\": 4619}, {\"accuracy\": 1.0, \"loss\": 0.000215055319131352, \"time-step\": 4620}, {\"accuracy\": 1.0, \"loss\": 0.00021046145411673933, \"time-step\": 4621}, {\"accuracy\": 1.0, \"loss\": 0.00021499092690646648, \"time-step\": 4622}, {\"accuracy\": 1.0, \"loss\": 0.0002103942388202995, \"time-step\": 4623}, {\"accuracy\": 1.0, \"loss\": 0.00021491825464181602, \"time-step\": 4624}, {\"accuracy\": 1.0, \"loss\": 0.00021031501819379628, \"time-step\": 4625}, {\"accuracy\": 1.0, \"loss\": 0.00021483389718923718, \"time-step\": 4626}, {\"accuracy\": 1.0, \"loss\": 0.0002102376747643575, \"time-step\": 4627}, {\"accuracy\": 1.0, \"loss\": 0.00021476596884895116, \"time-step\": 4628}, {\"accuracy\": 1.0, \"loss\": 0.00021017443214077502, \"time-step\": 4629}, {\"accuracy\": 1.0, \"loss\": 0.00021469895727932453, \"time-step\": 4630}, {\"accuracy\": 1.0, \"loss\": 0.00021010365162510425, \"time-step\": 4631}, {\"accuracy\": 1.0, \"loss\": 0.00021461374126374722, \"time-step\": 4632}, {\"accuracy\": 1.0, \"loss\": 0.0002100182609865442, \"time-step\": 4633}, {\"accuracy\": 1.0, \"loss\": 0.00021453941008076072, \"time-step\": 4634}, {\"accuracy\": 1.0, \"loss\": 0.00020994548685848713, \"time-step\": 4635}, {\"accuracy\": 1.0, \"loss\": 0.0002144592726835981, \"time-step\": 4636}, {\"accuracy\": 1.0, \"loss\": 0.00020987214520573616, \"time-step\": 4637}, {\"accuracy\": 1.0, \"loss\": 0.0002143898600479588, \"time-step\": 4638}, {\"accuracy\": 1.0, \"loss\": 0.00020979871624149382, \"time-step\": 4639}, {\"accuracy\": 1.0, \"loss\": 0.0002143073797924444, \"time-step\": 4640}, {\"accuracy\": 1.0, \"loss\": 0.00020972358470316976, \"time-step\": 4641}, {\"accuracy\": 1.0, \"loss\": 0.00021423131693154573, \"time-step\": 4642}, {\"accuracy\": 1.0, \"loss\": 0.00020965622388757765, \"time-step\": 4643}, {\"accuracy\": 1.0, \"loss\": 0.00021416670642793179, \"time-step\": 4644}, {\"accuracy\": 1.0, \"loss\": 0.0002095842210110277, \"time-step\": 4645}, {\"accuracy\": 1.0, \"loss\": 0.00021409301552921534, \"time-step\": 4646}, {\"accuracy\": 1.0, \"loss\": 0.00020951188344042748, \"time-step\": 4647}, {\"accuracy\": 1.0, \"loss\": 0.00021402191487140954, \"time-step\": 4648}, {\"accuracy\": 1.0, \"loss\": 0.0002094434021273628, \"time-step\": 4649}, {\"accuracy\": 1.0, \"loss\": 0.00021394075884018093, \"time-step\": 4650}, {\"accuracy\": 1.0, \"loss\": 0.000209362362511456, \"time-step\": 4651}, {\"accuracy\": 1.0, \"loss\": 0.00021386842126958072, \"time-step\": 4652}, {\"accuracy\": 1.0, \"loss\": 0.00020929586025886238, \"time-step\": 4653}, {\"accuracy\": 1.0, \"loss\": 0.00021378947712946683, \"time-step\": 4654}, {\"accuracy\": 1.0, \"loss\": 0.00020921735267620534, \"time-step\": 4655}, {\"accuracy\": 1.0, \"loss\": 0.00021371315233409405, \"time-step\": 4656}, {\"accuracy\": 1.0, \"loss\": 0.0002091403293889016, \"time-step\": 4657}, {\"accuracy\": 1.0, \"loss\": 0.00021363030828069896, \"time-step\": 4658}, {\"accuracy\": 1.0, \"loss\": 0.0002090572234010324, \"time-step\": 4659}, {\"accuracy\": 1.0, \"loss\": 0.0002135610266122967, \"time-step\": 4660}, {\"accuracy\": 1.0, \"loss\": 0.00020899702212773263, \"time-step\": 4661}, {\"accuracy\": 1.0, \"loss\": 0.00021349472808651626, \"time-step\": 4662}, {\"accuracy\": 1.0, \"loss\": 0.00020892715838272125, \"time-step\": 4663}, {\"accuracy\": 1.0, \"loss\": 0.00021341057436075062, \"time-step\": 4664}, {\"accuracy\": 1.0, \"loss\": 0.00020884624973405153, \"time-step\": 4665}, {\"accuracy\": 1.0, \"loss\": 0.00021333585027605295, \"time-step\": 4666}, {\"accuracy\": 1.0, \"loss\": 0.00020877808856312186, \"time-step\": 4667}, {\"accuracy\": 1.0, \"loss\": 0.00021326924616005272, \"time-step\": 4668}, {\"accuracy\": 1.0, \"loss\": 0.0002087179309455678, \"time-step\": 4669}, {\"accuracy\": 1.0, \"loss\": 0.00021321035455912352, \"time-step\": 4670}, {\"accuracy\": 1.0, \"loss\": 0.00020865985425189137, \"time-step\": 4671}, {\"accuracy\": 1.0, \"loss\": 0.00021315240883268416, \"time-step\": 4672}, {\"accuracy\": 1.0, \"loss\": 0.0002085958985844627, \"time-step\": 4673}, {\"accuracy\": 1.0, \"loss\": 0.0002130775828845799, \"time-step\": 4674}, {\"accuracy\": 1.0, \"loss\": 0.000208512952667661, \"time-step\": 4675}, {\"accuracy\": 1.0, \"loss\": 0.00021299908985383809, \"time-step\": 4676}, {\"accuracy\": 1.0, \"loss\": 0.00020844509708695114, \"time-step\": 4677}, {\"accuracy\": 1.0, \"loss\": 0.00021292673773132265, \"time-step\": 4678}, {\"accuracy\": 1.0, \"loss\": 0.0002083752042381093, \"time-step\": 4679}, {\"accuracy\": 1.0, \"loss\": 0.00021285054390318692, \"time-step\": 4680}, {\"accuracy\": 1.0, \"loss\": 0.00020829756977036595, \"time-step\": 4681}, {\"accuracy\": 1.0, \"loss\": 0.00021278669009916484, \"time-step\": 4682}, {\"accuracy\": 1.0, \"loss\": 0.00020824154489673674, \"time-step\": 4683}, {\"accuracy\": 1.0, \"loss\": 0.0002127209008904174, \"time-step\": 4684}, {\"accuracy\": 1.0, \"loss\": 0.00020817843324039131, \"time-step\": 4685}, {\"accuracy\": 1.0, \"loss\": 0.00021264981478452682, \"time-step\": 4686}, {\"accuracy\": 1.0, \"loss\": 0.00020809996931347996, \"time-step\": 4687}, {\"accuracy\": 1.0, \"loss\": 0.00021257933985907584, \"time-step\": 4688}, {\"accuracy\": 1.0, \"loss\": 0.0002080406848108396, \"time-step\": 4689}, {\"accuracy\": 1.0, \"loss\": 0.00021251157158985734, \"time-step\": 4690}, {\"accuracy\": 1.0, \"loss\": 0.00020796139142476022, \"time-step\": 4691}, {\"accuracy\": 1.0, \"loss\": 0.00021243098308332264, \"time-step\": 4692}, {\"accuracy\": 1.0, \"loss\": 0.00020789512200281024, \"time-step\": 4693}, {\"accuracy\": 1.0, \"loss\": 0.00021236707107163966, \"time-step\": 4694}, {\"accuracy\": 1.0, \"loss\": 0.0002078239049296826, \"time-step\": 4695}, {\"accuracy\": 1.0, \"loss\": 0.00021229416597634554, \"time-step\": 4696}, {\"accuracy\": 1.0, \"loss\": 0.00020775562734343112, \"time-step\": 4697}, {\"accuracy\": 1.0, \"loss\": 0.0002122141158906743, \"time-step\": 4698}, {\"accuracy\": 1.0, \"loss\": 0.00020767160458490252, \"time-step\": 4699}, {\"accuracy\": 1.0, \"loss\": 0.00021213314903434366, \"time-step\": 4700}, {\"accuracy\": 1.0, \"loss\": 0.0002076045493595302, \"time-step\": 4701}, {\"accuracy\": 1.0, \"loss\": 0.0002120682765962556, \"time-step\": 4702}, {\"accuracy\": 1.0, \"loss\": 0.00020753787248395383, \"time-step\": 4703}, {\"accuracy\": 1.0, \"loss\": 0.00021200221090111881, \"time-step\": 4704}, {\"accuracy\": 1.0, \"loss\": 0.0002074766089208424, \"time-step\": 4705}, {\"accuracy\": 1.0, \"loss\": 0.00021193549036979675, \"time-step\": 4706}, {\"accuracy\": 1.0, \"loss\": 0.0002074080111924559, \"time-step\": 4707}, {\"accuracy\": 1.0, \"loss\": 0.00021186652884352952, \"time-step\": 4708}, {\"accuracy\": 1.0, \"loss\": 0.00020733564451802522, \"time-step\": 4709}, {\"accuracy\": 1.0, \"loss\": 0.0002117877156706527, \"time-step\": 4710}, {\"accuracy\": 1.0, \"loss\": 0.00020725993090309203, \"time-step\": 4711}, {\"accuracy\": 1.0, \"loss\": 0.0002117132826242596, \"time-step\": 4712}, {\"accuracy\": 1.0, \"loss\": 0.00020718616724479944, \"time-step\": 4713}, {\"accuracy\": 1.0, \"loss\": 0.00021164103236515075, \"time-step\": 4714}, {\"accuracy\": 1.0, \"loss\": 0.00020711406250484288, \"time-step\": 4715}, {\"accuracy\": 1.0, \"loss\": 0.00021155791182536632, \"time-step\": 4716}, {\"accuracy\": 1.0, \"loss\": 0.0002070379996439442, \"time-step\": 4717}, {\"accuracy\": 1.0, \"loss\": 0.0002114968083333224, \"time-step\": 4718}, {\"accuracy\": 1.0, \"loss\": 0.0002069868496619165, \"time-step\": 4719}, {\"accuracy\": 1.0, \"loss\": 0.0002114436647389084, \"time-step\": 4720}, {\"accuracy\": 1.0, \"loss\": 0.00020693648548331112, \"time-step\": 4721}, {\"accuracy\": 1.0, \"loss\": 0.0002113822556566447, \"time-step\": 4722}, {\"accuracy\": 1.0, \"loss\": 0.0002068647590931505, \"time-step\": 4723}, {\"accuracy\": 1.0, \"loss\": 0.00021131466201040894, \"time-step\": 4724}, {\"accuracy\": 1.0, \"loss\": 0.00020680578018072993, \"time-step\": 4725}, {\"accuracy\": 1.0, \"loss\": 0.00021125416969880462, \"time-step\": 4726}, {\"accuracy\": 1.0, \"loss\": 0.00020674952247645706, \"time-step\": 4727}, {\"accuracy\": 1.0, \"loss\": 0.00021119251323398203, \"time-step\": 4728}, {\"accuracy\": 1.0, \"loss\": 0.00020667615171987563, \"time-step\": 4729}, {\"accuracy\": 1.0, \"loss\": 0.0002111167850671336, \"time-step\": 4730}, {\"accuracy\": 1.0, \"loss\": 0.00020661266171373427, \"time-step\": 4731}, {\"accuracy\": 1.0, \"loss\": 0.00021106003259774297, \"time-step\": 4732}, {\"accuracy\": 1.0, \"loss\": 0.00020655021944548935, \"time-step\": 4733}, {\"accuracy\": 1.0, \"loss\": 0.00021098768047522753, \"time-step\": 4734}, {\"accuracy\": 1.0, \"loss\": 0.00020647665951400995, \"time-step\": 4735}, {\"accuracy\": 1.0, \"loss\": 0.00021091444068588316, \"time-step\": 4736}, {\"accuracy\": 1.0, \"loss\": 0.00020640432194340974, \"time-step\": 4737}, {\"accuracy\": 1.0, \"loss\": 0.00021083322644699365, \"time-step\": 4738}, {\"accuracy\": 1.0, \"loss\": 0.00020632569794543087, \"time-step\": 4739}, {\"accuracy\": 1.0, \"loss\": 0.0002107501932187006, \"time-step\": 4740}, {\"accuracy\": 1.0, \"loss\": 0.00020624919852707535, \"time-step\": 4741}, {\"accuracy\": 1.0, \"loss\": 0.00021069419744890183, \"time-step\": 4742}, {\"accuracy\": 1.0, \"loss\": 0.00020619849965441972, \"time-step\": 4743}, {\"accuracy\": 1.0, \"loss\": 0.00021062638552393764, \"time-step\": 4744}, {\"accuracy\": 1.0, \"loss\": 0.00020612901425920427, \"time-step\": 4745}, {\"accuracy\": 1.0, \"loss\": 0.00021056042169220746, \"time-step\": 4746}, {\"accuracy\": 1.0, \"loss\": 0.00020606617908924818, \"time-step\": 4747}, {\"accuracy\": 1.0, \"loss\": 0.00021049358474556357, \"time-step\": 4748}, {\"accuracy\": 1.0, \"loss\": 0.0002059934486169368, \"time-step\": 4749}, {\"accuracy\": 1.0, \"loss\": 0.00021041864238213748, \"time-step\": 4750}, {\"accuracy\": 1.0, \"loss\": 0.0002059344988083467, \"time-step\": 4751}, {\"accuracy\": 1.0, \"loss\": 0.00021035895042587072, \"time-step\": 4752}, {\"accuracy\": 1.0, \"loss\": 0.0002058679237961769, \"time-step\": 4753}, {\"accuracy\": 1.0, \"loss\": 0.00021029621711932123, \"time-step\": 4754}, {\"accuracy\": 1.0, \"loss\": 0.00020580623822752386, \"time-step\": 4755}, {\"accuracy\": 1.0, \"loss\": 0.00021022521832492203, \"time-step\": 4756}, {\"accuracy\": 1.0, \"loss\": 0.00020573398796841502, \"time-step\": 4757}, {\"accuracy\": 1.0, \"loss\": 0.0002101448772009462, \"time-step\": 4758}, {\"accuracy\": 1.0, \"loss\": 0.00020565373415593058, \"time-step\": 4759}, {\"accuracy\": 1.0, \"loss\": 0.00021006903261877596, \"time-step\": 4760}, {\"accuracy\": 1.0, \"loss\": 0.00020558538381010294, \"time-step\": 4761}, {\"accuracy\": 1.0, \"loss\": 0.0002100081037497148, \"time-step\": 4762}, {\"accuracy\": 1.0, \"loss\": 0.00020552214118652046, \"time-step\": 4763}, {\"accuracy\": 1.0, \"loss\": 0.00020993457292206585, \"time-step\": 4764}, {\"accuracy\": 1.0, \"loss\": 0.00020545438746921718, \"time-step\": 4765}, {\"accuracy\": 1.0, \"loss\": 0.00020987310563214123, \"time-step\": 4766}, {\"accuracy\": 1.0, \"loss\": 0.00020539431716315448, \"time-step\": 4767}, {\"accuracy\": 1.0, \"loss\": 0.00020980094268452376, \"time-step\": 4768}, {\"accuracy\": 1.0, \"loss\": 0.00020531524205580354, \"time-step\": 4769}, {\"accuracy\": 1.0, \"loss\": 0.00020972598576918244, \"time-step\": 4770}, {\"accuracy\": 1.0, \"loss\": 0.0002052422205451876, \"time-step\": 4771}, {\"accuracy\": 1.0, \"loss\": 0.000209641526453197, \"time-step\": 4772}, {\"accuracy\": 1.0, \"loss\": 0.00020515406504273415, \"time-step\": 4773}, {\"accuracy\": 1.0, \"loss\": 0.00020955181389581412, \"time-step\": 4774}, {\"accuracy\": 1.0, \"loss\": 0.00020506748114712536, \"time-step\": 4775}, {\"accuracy\": 1.0, \"loss\": 0.00020947231678292155, \"time-step\": 4776}, {\"accuracy\": 1.0, \"loss\": 0.00020501145627349615, \"time-step\": 4777}, {\"accuracy\": 1.0, \"loss\": 0.0002094182709697634, \"time-step\": 4778}, {\"accuracy\": 1.0, \"loss\": 0.0002049492613878101, \"time-step\": 4779}, {\"accuracy\": 1.0, \"loss\": 0.00020935734210070223, \"time-step\": 4780}, {\"accuracy\": 1.0, \"loss\": 0.0002048946189461276, \"time-step\": 4781}, {\"accuracy\": 1.0, \"loss\": 0.00020929377933498472, \"time-step\": 4782}, {\"accuracy\": 1.0, \"loss\": 0.00020481999672483653, \"time-step\": 4783}, {\"accuracy\": 1.0, \"loss\": 0.00020921557734254748, \"time-step\": 4784}, {\"accuracy\": 1.0, \"loss\": 0.00020475183555390686, \"time-step\": 4785}, {\"accuracy\": 1.0, \"loss\": 0.00020915261120535433, \"time-step\": 4786}, {\"accuracy\": 1.0, \"loss\": 0.0002046872687060386, \"time-step\": 4787}, {\"accuracy\": 1.0, \"loss\": 0.0002090793859679252, \"time-step\": 4788}, {\"accuracy\": 1.0, \"loss\": 0.00020461203530430794, \"time-step\": 4789}, {\"accuracy\": 1.0, \"loss\": 0.00020900872186757624, \"time-step\": 4790}, {\"accuracy\": 1.0, \"loss\": 0.0002045604051090777, \"time-step\": 4791}, {\"accuracy\": 1.0, \"loss\": 0.000208963145269081, \"time-step\": 4792}, {\"accuracy\": 1.0, \"loss\": 0.00020451076852623373, \"time-step\": 4793}, {\"accuracy\": 1.0, \"loss\": 0.00020889677398372442, \"time-step\": 4794}, {\"accuracy\": 1.0, \"loss\": 0.000204435084015131, \"time-step\": 4795}, {\"accuracy\": 1.0, \"loss\": 0.0002088268520310521, \"time-step\": 4796}, {\"accuracy\": 1.0, \"loss\": 0.00020437169587239623, \"time-step\": 4797}, {\"accuracy\": 1.0, \"loss\": 0.0002087613829644397, \"time-step\": 4798}, {\"accuracy\": 1.0, \"loss\": 0.00020430685253813863, \"time-step\": 4799}, {\"accuracy\": 1.0, \"loss\": 0.00020870266598649323, \"time-step\": 4800}, {\"accuracy\": 1.0, \"loss\": 0.0002042503620032221, \"time-step\": 4801}, {\"accuracy\": 1.0, \"loss\": 0.00020863510144408792, \"time-step\": 4802}, {\"accuracy\": 1.0, \"loss\": 0.00020418384519871324, \"time-step\": 4803}, {\"accuracy\": 1.0, \"loss\": 0.00020856838091276586, \"time-step\": 4804}, {\"accuracy\": 1.0, \"loss\": 0.000204120617127046, \"time-step\": 4805}, {\"accuracy\": 1.0, \"loss\": 0.0002085058222291991, \"time-step\": 4806}, {\"accuracy\": 1.0, \"loss\": 0.00020405343093443662, \"time-step\": 4807}, {\"accuracy\": 1.0, \"loss\": 0.00020843828679062426, \"time-step\": 4808}, {\"accuracy\": 1.0, \"loss\": 0.00020398774358909577, \"time-step\": 4809}, {\"accuracy\": 1.0, \"loss\": 0.0002083709987346083, \"time-step\": 4810}, {\"accuracy\": 1.0, \"loss\": 0.00020393176237121224, \"time-step\": 4811}, {\"accuracy\": 1.0, \"loss\": 0.00020831530855502933, \"time-step\": 4812}, {\"accuracy\": 1.0, \"loss\": 0.0002038723905570805, \"time-step\": 4813}, {\"accuracy\": 1.0, \"loss\": 0.00020824522653128952, \"time-step\": 4814}, {\"accuracy\": 1.0, \"loss\": 0.00020379398483783007, \"time-step\": 4815}, {\"accuracy\": 1.0, \"loss\": 0.0002081748389173299, \"time-step\": 4816}, {\"accuracy\": 1.0, \"loss\": 0.00020372800645418465, \"time-step\": 4817}, {\"accuracy\": 1.0, \"loss\": 0.00020809273701161146, \"time-step\": 4818}, {\"accuracy\": 1.0, \"loss\": 0.00020364880037959665, \"time-step\": 4819}, {\"accuracy\": 1.0, \"loss\": 0.00020802808285225183, \"time-step\": 4820}, {\"accuracy\": 1.0, \"loss\": 0.00020358590700197965, \"time-step\": 4821}, {\"accuracy\": 1.0, \"loss\": 0.00020794745068997145, \"time-step\": 4822}, {\"accuracy\": 1.0, \"loss\": 0.00020350991690065712, \"time-step\": 4823}, {\"accuracy\": 1.0, \"loss\": 0.00020787969697266817, \"time-step\": 4824}, {\"accuracy\": 1.0, \"loss\": 0.0002034373173955828, \"time-step\": 4825}, {\"accuracy\": 1.0, \"loss\": 0.0002078029210679233, \"time-step\": 4826}, {\"accuracy\": 1.0, \"loss\": 0.00020337790192570537, \"time-step\": 4827}, {\"accuracy\": 1.0, \"loss\": 0.00020774868607986718, \"time-step\": 4828}, {\"accuracy\": 1.0, \"loss\": 0.00020331227278802544, \"time-step\": 4829}, {\"accuracy\": 1.0, \"loss\": 0.0002076750824926421, \"time-step\": 4830}, {\"accuracy\": 1.0, \"loss\": 0.0002032490010606125, \"time-step\": 4831}, {\"accuracy\": 1.0, \"loss\": 0.0002076148521155119, \"time-step\": 4832}, {\"accuracy\": 1.0, \"loss\": 0.00020318407041486353, \"time-step\": 4833}, {\"accuracy\": 1.0, \"loss\": 0.00020754501747433096, \"time-step\": 4834}, {\"accuracy\": 1.0, \"loss\": 0.00020312544074840844, \"time-step\": 4835}, {\"accuracy\": 1.0, \"loss\": 0.0002074899966828525, \"time-step\": 4836}, {\"accuracy\": 1.0, \"loss\": 0.00020306625810917467, \"time-step\": 4837}, {\"accuracy\": 1.0, \"loss\": 0.00020742419292218983, \"time-step\": 4838}, {\"accuracy\": 1.0, \"loss\": 0.0002029945608228445, \"time-step\": 4839}, {\"accuracy\": 1.0, \"loss\": 0.00020735239377245307, \"time-step\": 4840}, {\"accuracy\": 1.0, \"loss\": 0.0002029325842158869, \"time-step\": 4841}, {\"accuracy\": 1.0, \"loss\": 0.0002072929055429995, \"time-step\": 4842}, {\"accuracy\": 1.0, \"loss\": 0.0002028684102697298, \"time-step\": 4843}, {\"accuracy\": 1.0, \"loss\": 0.00020721845794469118, \"time-step\": 4844}, {\"accuracy\": 1.0, \"loss\": 0.000202797309611924, \"time-step\": 4845}, {\"accuracy\": 1.0, \"loss\": 0.00020714204583782703, \"time-step\": 4846}, {\"accuracy\": 1.0, \"loss\": 0.00020271683752071112, \"time-step\": 4847}, {\"accuracy\": 1.0, \"loss\": 0.0002070645714411512, \"time-step\": 4848}, {\"accuracy\": 1.0, \"loss\": 0.00020265113562345505, \"time-step\": 4849}, {\"accuracy\": 1.0, \"loss\": 0.00020700610184576362, \"time-step\": 4850}, {\"accuracy\": 1.0, \"loss\": 0.00020259367011021823, \"time-step\": 4851}, {\"accuracy\": 1.0, \"loss\": 0.00020694352861028165, \"time-step\": 4852}, {\"accuracy\": 1.0, \"loss\": 0.00020253124239388853, \"time-step\": 4853}, {\"accuracy\": 1.0, \"loss\": 0.00020688203221652657, \"time-step\": 4854}, {\"accuracy\": 1.0, \"loss\": 0.00020247482461854815, \"time-step\": 4855}, {\"accuracy\": 1.0, \"loss\": 0.0002068215108010918, \"time-step\": 4856}, {\"accuracy\": 1.0, \"loss\": 0.0002024119603447616, \"time-step\": 4857}, {\"accuracy\": 1.0, \"loss\": 0.00020676157146226615, \"time-step\": 4858}, {\"accuracy\": 1.0, \"loss\": 0.00020235966076143086, \"time-step\": 4859}, {\"accuracy\": 1.0, \"loss\": 0.0002067135355900973, \"time-step\": 4860}, {\"accuracy\": 1.0, \"loss\": 0.0002023083798121661, \"time-step\": 4861}, {\"accuracy\": 1.0, \"loss\": 0.0002066465822281316, \"time-step\": 4862}, {\"accuracy\": 1.0, \"loss\": 0.00020223909814376384, \"time-step\": 4863}, {\"accuracy\": 1.0, \"loss\": 0.0002065801527351141, \"time-step\": 4864}, {\"accuracy\": 1.0, \"loss\": 0.00020217036944814026, \"time-step\": 4865}, {\"accuracy\": 1.0, \"loss\": 0.00020650355145335197, \"time-step\": 4866}, {\"accuracy\": 1.0, \"loss\": 0.00020209746435284615, \"time-step\": 4867}, {\"accuracy\": 1.0, \"loss\": 0.00020643773314077407, \"time-step\": 4868}, {\"accuracy\": 1.0, \"loss\": 0.000202038194402121, \"time-step\": 4869}, {\"accuracy\": 1.0, \"loss\": 0.0002063727006316185, \"time-step\": 4870}, {\"accuracy\": 1.0, \"loss\": 0.0002019745734287426, \"time-step\": 4871}, {\"accuracy\": 1.0, \"loss\": 0.00020631056395359337, \"time-step\": 4872}, {\"accuracy\": 1.0, \"loss\": 0.00020191716612316668, \"time-step\": 4873}, {\"accuracy\": 1.0, \"loss\": 0.0002062549174297601, \"time-step\": 4874}, {\"accuracy\": 1.0, \"loss\": 0.00020185578614473343, \"time-step\": 4875}, {\"accuracy\": 1.0, \"loss\": 0.00020618480630218983, \"time-step\": 4876}, {\"accuracy\": 1.0, \"loss\": 0.00020179233979433775, \"time-step\": 4877}, {\"accuracy\": 1.0, \"loss\": 0.00020611970103345811, \"time-step\": 4878}, {\"accuracy\": 1.0, \"loss\": 0.0002017224906012416, \"time-step\": 4879}, {\"accuracy\": 1.0, \"loss\": 0.00020605360623449087, \"time-step\": 4880}, {\"accuracy\": 1.0, \"loss\": 0.00020165684691164643, \"time-step\": 4881}, {\"accuracy\": 1.0, \"loss\": 0.0002059817488770932, \"time-step\": 4882}, {\"accuracy\": 1.0, \"loss\": 0.000201589020434767, \"time-step\": 4883}, {\"accuracy\": 1.0, \"loss\": 0.00020591396605595946, \"time-step\": 4884}, {\"accuracy\": 1.0, \"loss\": 0.00020152254728600383, \"time-step\": 4885}, {\"accuracy\": 1.0, \"loss\": 0.00020584164303727448, \"time-step\": 4886}, {\"accuracy\": 1.0, \"loss\": 0.0002014518395299092, \"time-step\": 4887}, {\"accuracy\": 1.0, \"loss\": 0.00020577599934767932, \"time-step\": 4888}, {\"accuracy\": 1.0, \"loss\": 0.00020139524713158607, \"time-step\": 4889}, {\"accuracy\": 1.0, \"loss\": 0.000205724747502245, \"time-step\": 4890}, {\"accuracy\": 1.0, \"loss\": 0.00020132957433816046, \"time-step\": 4891}, {\"accuracy\": 1.0, \"loss\": 0.00020564802980516106, \"time-step\": 4892}, {\"accuracy\": 1.0, \"loss\": 0.00020127379684709013, \"time-step\": 4893}, {\"accuracy\": 1.0, \"loss\": 0.0002055967488558963, \"time-step\": 4894}, {\"accuracy\": 1.0, \"loss\": 0.0002012206387007609, \"time-step\": 4895}, {\"accuracy\": 1.0, \"loss\": 0.000205532880499959, \"time-step\": 4896}, {\"accuracy\": 1.0, \"loss\": 0.00020114803919568658, \"time-step\": 4897}, {\"accuracy\": 1.0, \"loss\": 0.00020545990264508873, \"time-step\": 4898}, {\"accuracy\": 1.0, \"loss\": 0.00020108529133722186, \"time-step\": 4899}, {\"accuracy\": 1.0, \"loss\": 0.0002053991483990103, \"time-step\": 4900}, {\"accuracy\": 1.0, \"loss\": 0.00020102075359318405, \"time-step\": 4901}, {\"accuracy\": 1.0, \"loss\": 0.000205331074539572, \"time-step\": 4902}, {\"accuracy\": 1.0, \"loss\": 0.00020095884974580258, \"time-step\": 4903}, {\"accuracy\": 1.0, \"loss\": 0.00020527155720628798, \"time-step\": 4904}, {\"accuracy\": 1.0, \"loss\": 0.00020089675672352314, \"time-step\": 4905}, {\"accuracy\": 1.0, \"loss\": 0.00020520352700259537, \"time-step\": 4906}, {\"accuracy\": 1.0, \"loss\": 0.00020082258561160415, \"time-step\": 4907}, {\"accuracy\": 1.0, \"loss\": 0.00020512963237706572, \"time-step\": 4908}, {\"accuracy\": 1.0, \"loss\": 0.00020076461078133434, \"time-step\": 4909}, {\"accuracy\": 1.0, \"loss\": 0.00020506492001004517, \"time-step\": 4910}, {\"accuracy\": 1.0, \"loss\": 0.00020069124002475291, \"time-step\": 4911}, {\"accuracy\": 1.0, \"loss\": 0.00020499817037489265, \"time-step\": 4912}, {\"accuracy\": 1.0, \"loss\": 0.00020063323609065264, \"time-step\": 4913}, {\"accuracy\": 1.0, \"loss\": 0.00020493801275733858, \"time-step\": 4914}, {\"accuracy\": 1.0, \"loss\": 0.00020056594803463668, \"time-step\": 4915}, {\"accuracy\": 1.0, \"loss\": 0.0002048630703939125, \"time-step\": 4916}, {\"accuracy\": 1.0, \"loss\": 0.0002005006099352613, \"time-step\": 4917}, {\"accuracy\": 1.0, \"loss\": 0.00020479892555158585, \"time-step\": 4918}, {\"accuracy\": 1.0, \"loss\": 0.00020044272241648287, \"time-step\": 4919}, {\"accuracy\": 1.0, \"loss\": 0.00020475167548283935, \"time-step\": 4920}, {\"accuracy\": 1.0, \"loss\": 0.0002003897971007973, \"time-step\": 4921}, {\"accuracy\": 1.0, \"loss\": 0.0002046735171461478, \"time-step\": 4922}, {\"accuracy\": 1.0, \"loss\": 0.00020030318410135806, \"time-step\": 4923}, {\"accuracy\": 1.0, \"loss\": 0.0002045992878265679, \"time-step\": 4924}, {\"accuracy\": 1.0, \"loss\": 0.00020024392870254815, \"time-step\": 4925}, {\"accuracy\": 1.0, \"loss\": 0.00020452766329981387, \"time-step\": 4926}, {\"accuracy\": 1.0, \"loss\": 0.0002001697284867987, \"time-step\": 4927}, {\"accuracy\": 1.0, \"loss\": 0.00020445924019441009, \"time-step\": 4928}, {\"accuracy\": 1.0, \"loss\": 0.00020009827858302742, \"time-step\": 4929}, {\"accuracy\": 1.0, \"loss\": 0.00020438911451492459, \"time-step\": 4930}, {\"accuracy\": 1.0, \"loss\": 0.00020003097597509623, \"time-step\": 4931}, {\"accuracy\": 1.0, \"loss\": 0.00020431926532182842, \"time-step\": 4932}, {\"accuracy\": 1.0, \"loss\": 0.00019997544586658478, \"time-step\": 4933}, {\"accuracy\": 1.0, \"loss\": 0.0002042688720393926, \"time-step\": 4934}, {\"accuracy\": 1.0, \"loss\": 0.00019991930457763374, \"time-step\": 4935}, {\"accuracy\": 1.0, \"loss\": 0.0002042060368694365, \"time-step\": 4936}, {\"accuracy\": 1.0, \"loss\": 0.00019985913240816444, \"time-step\": 4937}, {\"accuracy\": 1.0, \"loss\": 0.0002041403204202652, \"time-step\": 4938}, {\"accuracy\": 1.0, \"loss\": 0.00019979223725385964, \"time-step\": 4939}, {\"accuracy\": 1.0, \"loss\": 0.00020408010459505022, \"time-step\": 4940}, {\"accuracy\": 1.0, \"loss\": 0.00019974591850768775, \"time-step\": 4941}, {\"accuracy\": 1.0, \"loss\": 0.00020402282825671136, \"time-step\": 4942}, {\"accuracy\": 1.0, \"loss\": 0.00019967090338468552, \"time-step\": 4943}, {\"accuracy\": 1.0, \"loss\": 0.00020395105821080506, \"time-step\": 4944}, {\"accuracy\": 1.0, \"loss\": 0.00019961064390372485, \"time-step\": 4945}, {\"accuracy\": 1.0, \"loss\": 0.00020388723351061344, \"time-step\": 4946}, {\"accuracy\": 1.0, \"loss\": 0.00019954402523580939, \"time-step\": 4947}, {\"accuracy\": 1.0, \"loss\": 0.00020383004448376596, \"time-step\": 4948}, {\"accuracy\": 1.0, \"loss\": 0.00019949188572354615, \"time-step\": 4949}, {\"accuracy\": 1.0, \"loss\": 0.0002037655794993043, \"time-step\": 4950}, {\"accuracy\": 1.0, \"loss\": 0.00019942874496337026, \"time-step\": 4951}, {\"accuracy\": 1.0, \"loss\": 0.00020370283164083958, \"time-step\": 4952}, {\"accuracy\": 1.0, \"loss\": 0.00019936435273848474, \"time-step\": 4953}, {\"accuracy\": 1.0, \"loss\": 0.00020363516523502767, \"time-step\": 4954}, {\"accuracy\": 1.0, \"loss\": 0.00019929869449697435, \"time-step\": 4955}, {\"accuracy\": 1.0, \"loss\": 0.00020356856111902744, \"time-step\": 4956}, {\"accuracy\": 1.0, \"loss\": 0.00019923120271414518, \"time-step\": 4957}, {\"accuracy\": 1.0, \"loss\": 0.00020349910482764244, \"time-step\": 4958}, {\"accuracy\": 1.0, \"loss\": 0.00019916474411729723, \"time-step\": 4959}, {\"accuracy\": 1.0, \"loss\": 0.0002034374774666503, \"time-step\": 4960}, {\"accuracy\": 1.0, \"loss\": 0.00019911088747903705, \"time-step\": 4961}, {\"accuracy\": 1.0, \"loss\": 0.00020337743626441807, \"time-step\": 4962}, {\"accuracy\": 1.0, \"loss\": 0.00019904621876776218, \"time-step\": 4963}, {\"accuracy\": 1.0, \"loss\": 0.00020330987172201276, \"time-step\": 4964}, {\"accuracy\": 1.0, \"loss\": 0.00019897622405551374, \"time-step\": 4965}, {\"accuracy\": 1.0, \"loss\": 0.00020323543867561966, \"time-step\": 4966}, {\"accuracy\": 1.0, \"loss\": 0.00019891143892891705, \"time-step\": 4967}, {\"accuracy\": 1.0, \"loss\": 0.00020317832240834832, \"time-step\": 4968}, {\"accuracy\": 1.0, \"loss\": 0.00019885635992977768, \"time-step\": 4969}, {\"accuracy\": 1.0, \"loss\": 0.0002031214244198054, \"time-step\": 4970}, {\"accuracy\": 1.0, \"loss\": 0.0001988105068448931, \"time-step\": 4971}, {\"accuracy\": 1.0, \"loss\": 0.00020307728846091777, \"time-step\": 4972}, {\"accuracy\": 1.0, \"loss\": 0.00019875721773132682, \"time-step\": 4973}, {\"accuracy\": 1.0, \"loss\": 0.00020302376651670784, \"time-step\": 4974}, {\"accuracy\": 1.0, \"loss\": 0.00019870177493430674, \"time-step\": 4975}, {\"accuracy\": 1.0, \"loss\": 0.00020295086142141372, \"time-step\": 4976}, {\"accuracy\": 1.0, \"loss\": 0.0001986191637115553, \"time-step\": 4977}, {\"accuracy\": 1.0, \"loss\": 0.00020287233928684145, \"time-step\": 4978}, {\"accuracy\": 1.0, \"loss\": 0.00019856548169627786, \"time-step\": 4979}, {\"accuracy\": 1.0, \"loss\": 0.00020282494369894266, \"time-step\": 4980}, {\"accuracy\": 1.0, \"loss\": 0.00019850618264172226, \"time-step\": 4981}, {\"accuracy\": 1.0, \"loss\": 0.0002027607406489551, \"time-step\": 4982}, {\"accuracy\": 1.0, \"loss\": 0.00019844605412799865, \"time-step\": 4983}, {\"accuracy\": 1.0, \"loss\": 0.0002026992297032848, \"time-step\": 4984}, {\"accuracy\": 1.0, \"loss\": 0.000198386114789173, \"time-step\": 4985}, {\"accuracy\": 1.0, \"loss\": 0.0002026355650741607, \"time-step\": 4986}, {\"accuracy\": 1.0, \"loss\": 0.0001983241963898763, \"time-step\": 4987}, {\"accuracy\": 1.0, \"loss\": 0.00020257578580640256, \"time-step\": 4988}, {\"accuracy\": 1.0, \"loss\": 0.00019826836069114506, \"time-step\": 4989}, {\"accuracy\": 1.0, \"loss\": 0.0002025162393692881, \"time-step\": 4990}, {\"accuracy\": 1.0, \"loss\": 0.0001982076355488971, \"time-step\": 4991}, {\"accuracy\": 1.0, \"loss\": 0.00020245237101335078, \"time-step\": 4992}, {\"accuracy\": 1.0, \"loss\": 0.00019813657854683697, \"time-step\": 4993}, {\"accuracy\": 1.0, \"loss\": 0.00020237555145286024, \"time-step\": 4994}, {\"accuracy\": 1.0, \"loss\": 0.00019806833006441593, \"time-step\": 4995}, {\"accuracy\": 1.0, \"loss\": 0.00020231620874255896, \"time-step\": 4996}, {\"accuracy\": 1.0, \"loss\": 0.00019801102462224662, \"time-step\": 4997}, {\"accuracy\": 1.0, \"loss\": 0.00020224715990480036, \"time-step\": 4998}, {\"accuracy\": 1.0, \"loss\": 0.00019794142281170934, \"time-step\": 4999}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.LayerChart(...)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alt.data_transformers.disable_max_rows()\n",
    "\n",
    "loss = history.history['loss']\n",
    "accuracy = history.history['acc']\n",
    "\n",
    "df = pd.DataFrame({\"accuracy\":accuracy, \"loss\":loss, \"time-step\": np.arange(0, len(accuracy))})\n",
    "\n",
    "base = alt.Chart(df).mark_line(color=\"blue\").encode(x=\"time-step\", y=\"accuracy\")\n",
    "loss = alt.Chart(df).mark_line(color=\"red\").encode(x=\"time-step\", y=\"loss\")\n",
    "(base  + loss).properties(title='Chart 2').resolve_scale(y='independent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that accuracy goes to 100% in less than 1,000 epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading data from Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word embeddings with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM architecture in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Applicatiion: text prediction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limitations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- 3 gradients problem papers.\n",
    "- RNN deep learning chapters\n",
    " \n",
    "- Botvinick, M., & Plaut, D. C. (2004). Doing without schema hierarchies: A recurrent connectionist approach to normal and impaired routine sequential action. Psychological Review, 111(2), 395.\n",
    "- Barak, O. (2017). Recurrent neural networks as versatile tools of neuroscience research. Current Opinion in Neurobiology, 46, 1–6. https://doi.org/10.1016/j.conb.2017.06.003\n",
    "- Chen, G. (2016). A gentle tutorial of recurrent neural network with error backpropagation. arXiv preprint arXiv:1610.02583.\n",
    "- Elman, J. L. (1990). Finding Structure in Time. Cognitive Science, 14(2), 179–211. https://doi.org/10.1207/s15516709cog1402_1\n",
    "- François, C. (2017). 6. Deep Learning for text and sequences. Deep learning with Python. Manning.\n",
    "- Christiansen, M. H., & Chater, N. (1999). Toward a connectionist model of recursion in human linguistic performance. Cognitive Science, 23(2), 157–205.\n",
    "- Hebb, D. O. (1949). The organization of behavior: A neuropsychological theory. Psychology Press.\n",
    "- Hochreiter, S., & Schmidhuber, J. (1997). Long short-term memory. Neural Computation, 9(8), 1735–1780.\n",
    "- Güçlü, U., & van Gerven, M. A. (2017). Modeling the dynamics of human brain activity with recurrent neural networks. Frontiers in Computational Neuroscience, 11, 7.\n",
    "- Graves, A. (2012). Supervised sequence labelling. In Supervised sequence labelling with recurrent neural networks (pp. 5-13). Springer, Berlin, Heidelberg.\n",
    "- Jarne, C., & Laje, R. (2019). A detailed study of recurrent neural networks used to model tasks in the cerebral cortex. ArXiv Preprint ArXiv:1906.01094.\n",
    "- John, M. F. (1992). The story gestalt: A model of knowledge-intensive processes in text comprehension. Cognitive Science, 16(2), 271–306.\n",
    "- Munakata, Y., McClelland, J. L., Johnson, M. H., & Siegler, R. S. (1997). Rethinking infant knowledge: Toward an adaptive process account of successes and failures in object permanence tasks. Psychological Review, 104(4), 686.\n",
    "- Muñoz-Organero, M., Powell, L., Heller, B., Harpin, V., & Parker, J. (2019). Using Recurrent Neural Networks to Compare Movement Patterns in ADHD and Normally Developing Children Based on Acceleration Signals from the Wrist and Ankle. Sensors (Basel, Switzerland), 19(13). https://doi.org/10.3390/s19132935\n",
    "- K. J. Lang, A. H. Waibel, and G. E. Hinton. A Time-delay Neural Network Architecture for Isolated Word Recognition. Neural Networks, 3(1):23-43, 1990\n",
    "- Plaut, D. C., McClelland, J. L., Seidenberg, M. S., & Patterson, K. (1996). Understanding normal and impaired word reading: Computational principles in quasi-regular domains. Psychological Review, 103(1), 56.\n",
    "- Raj, B. (2020, Spring). Neural Networks: Hopfield Nets and Auto Associators [Lecture]. http://deeplearning.cs.cmu.edu/document/slides/lec17.hopfield.pdf\n",
    "- Zhang, A., Lipton, Z. C., Li, M., & Smola, A. J. (2020). 8. Recurrent Neural Networks. In Dive into Deep Learning. https://d2l.ai/chapter_convolutional-neural-networks/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful on-line resources"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
