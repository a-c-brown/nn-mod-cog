{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Understand the principles behind the creation of the recurrent neural network\n",
    "2. MATH\n",
    "3. CODE/MODELS\n",
    "4. MORE STUFF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Historical and theoretical background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The poet Derlmore Schartz once wrote: **\"...time is the fire in which we burn\"**. We can't scape time. Time is embedded in every human thought and action. Yet, so far, we have been oblivious to the role of time in neural network modeling. Indeed, in all models we have examined so far we have implicitly assumed that **data is \"perceived\" all at once**, although there are countless examples where time is a critical consideration: movement, speech production, planning, decision-making, etc. We also have implicitly assumed that **past-states have no influence in future-states**. This is, the input pattern at time-step $t-1$ has no influence in the output of time-step $t-0$, or $t+1$, or any subsequent outcome for that matter. In probabilistic jargon, this equals to assume that each sample is drawn independently from each other. We know in many scenarios this is simply not true: when giving a talk, my next utterance will depend upon my past utterances; when running, my last stride will condition my next stride, and so on. You can imagine endless examples.\n",
    "\n",
    "Multilayer perceptrons and Convolutional Networks, in principle, can be used to approach problems where time and sequences are a consideration. Nevertheless, such architectures where not designed with time in mind, and better architectures have been envisioned. In particular, **Recurrent Neural Networks (RNNs)** are the modern standard to deal with **time-dependent** and/or **sequence-dependent** problems. This type of networks are \"recurrent\" in the sense that they can **revisit or reuse past states as inputs to predict the next or future states**. To put it plainly, they have **memory**. Indeed, memory is what allow us to incorporate our past thoughts and behaviors into our future thoughts and behaviors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hopfield Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of earliest examples of networks incorporating \"recurrences\" was the so-called **Hopfield Network**, introduced in 1982 by [John Hopfield](https://en.wikipedia.org/wiki/John_Hopfield), at the time, a physicist at Caltech. Hopfield networks were important as they helped to reignite the interest in neural networks in the early '80s (along with backpropagation). Hopfield wanted to address the fundamental question of **emergence** in cognitive systems: Can relatively stable cognitive phenomena, like memories, emerge from the collective action of large numbers of simple neurons? After all, such behavior was observed in other physicial systems like vortex patterns in fluid flow. Brains seemed like another promising candidate.\n",
    "\n",
    "Hopfield networks are known as a type of **energy-based** (instead of error-based) network because their properties derive from a global energy-function. In resemblence to the McCulloch-Pitts neuron, Hopfield neurons are binary threshold units but with recurrent instead of feed-forward connections, where each unit is **bi-directionally connected** to each other, as shown in **Figure X**. This means that each unit *receives* inputs and *sends* inputs to every other connected unit. A consequence of this architecture is that **weights values are symmetric**, such that weights *coming into* a unit are the same as the ones *coming out* of a unit. The value of each unit is determined by a linear function wrapped into a threshold function $T$, as $y_i = T(\\sum w_{ji}y_j + b_i)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> Figure X </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/rec-net/hopfield-net.svg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic idea of Hopfield networks is that each configuration of binary-values $C$ in the network is associated with a **global energy value $-E$**. Here is a simplified picture of the training process: imagine you have a network with five neurons with a configuration of $C_1=(0, 1, 0, 1, 0)$. Now, imagine $C_1$ yields a global energy-value $E_1= 2$ (following the energy function formula). Your goal is to *minimize* $E$ by changing one element of the network $c_i$ at a time. By using the weight updating rule $\\Delta w$, you can subsequently get a new configuration like $C_2=(1, 1, 0, 1, 0)$, as new weights will cause a change in the activation values $(0,1)$. If $C_2$ yields a *lower value of $E$*, let's say, $1.5$, you are moving in the right direction. If you keep iterating with new configurations the network will eventually \"settle\" into a **global energy minimun** (conditioned to the initial state of the network).\n",
    "\n",
    "A fascinanting aspect of Hopfield networks, besides the introduction of recurrence, is that is closely based in neuroscience research about learning and memory, particularly Hebbian learning (XXXX). In fact, Hopfield proposed this model as a way to capture **memory formation and retrieval**. Basically, the idea is that the energy-minima of the network could represent the **formation of a memory**, which further give rise to a property known as **content-addressable memory (CAM)**. Here is the idea with a computer analogy: when you access information storaged in the random access memory of your computer (RAM), you give the \"address\" where the \"memory\" is located to retrieve it. CAM works the other way around: you give information about the **content** you are searching for, and the computer should retrieve the \"memory\". This is great because this works even when you have **partial or corrupted** information about the content, which is a much more **realistic depiction of how human memory works**. It is similar to doing a google search. Just think in how many times you have searched for lyrics with partial information, like \"song with the beeeee bop ba bodda bope!\".\n",
    "\n",
    "Is important to highlight that the sequential adjustment of Hopfield networks is **not driven by error correction**. Actually, there isn't a \"target\" as in supervised-based neural networks. Hopfield networks are systems that \"evolve\" until they find an stable low-energy state. If you \"perturb\" such a system, the system will \"re-evolve\" towards its previous stable-state, similar to how those inflatable \"Bop Bags\" toys get back to their initial position no matter how hard you punch them. It is almost like the system \"remembers\" its previous stable-state (in't?). This ability to \"return\" to a previous stable-state after perturbation is why they serve as models of memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elman Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although Hopfield networks where innovative and fascinating models, the first successful example of a recurrent network trained with backpropagation was introduced by [Jeffrey Elman](https://en.wikipedia.org/wiki/Jeffrey_Elman), the so-called **Elman Network**. Elman was a cognitive scientist at UC San Diego at the time, part of the group of researchers that published the famous PDP book.\n",
    "\n",
    "In 1990, Elman published \"Finding Structure in Time\", an highly influential work for both cognitive science and machine learning (particularly natural language processing). Elman was concerned with the problem of representing \"time\" or \"sequences\" in neural networks. In his view, you could take either a \"explicit\" approach or an \"implicit\" approach. The **explicit** approach represents time **spacially**. Consider a vector $x = [x_1,x_2 \\cdots, x_n]$, where element $x_1$ represents the first value of a sequence, $x_2$ the second element, and $x_n$ the last element. Hence, the spacial location in $\\bf{x}$ is indicating the temporal location of a value. You can think about elements of $\\bf{x}$ as sequences of words or actions, one after the other. Elman saw several drawbacks to this approach. First, although $\\bf{x}$ is a sequence, the network still needs to represent the entire sequence all at once as an input. Second, imposes a rigid limit on the durations of pattern, in other words, the newtwork needs fixed number of elements for every input vector $bf{x}$. This is a problem for most domains where sequences have variable duation (e.g., language, movement). Finally, it can't easily distinguish **relative** temporal position from **absolute** temporal position. Consider the sequence $s = [1, 1]$ and a vector input lenght of four bits. Such a sequence can be presented in at least three variations:\n",
    "\n",
    "$$\n",
    "x_1 = [0, 1, 1, 0]\\\\\n",
    "x_2 = [0, 0, 1, 1]\\\\\n",
    "x_3 = [1, 1, 0, 0]\n",
    "$$\n",
    "\n",
    "Here, $\\bf{x_1}$, $\\bf{x_2}$, and $\\bf{x_3}$ are instances of $\\bf{s}$ but spacially displaced in the input vector. Geometrically, those three vectors are very different from each other (you can compute similarity measures to put a number on that), although represent the same instance. Even though you can train a neural net to learn those three patterns are associated with the same target, their inherent disimilarity will hinder the network ability to generalize the learned association. It is like training network to learn that blue, yellow, and red are associated with the same target \"1\": the network will learn the association, but it won't make sense outside the training set.\n",
    "\n",
    "The **explicit** approach represents time by **its effect in intermediate computations**. To do this, you have to add a unit that can storage past computations to incorporate those in future computations. In short, **memory**. There are many ways to do this. Elman based his approach in the work of Michael Jordan ([this one](https://people.eecs.berkeley.edu/~jordan/), not the 2nd-best basketball player in history) on serial processing (1986). Jordan's network implement recurrent connections from the network output $\\hat{y}$ to its hidden units $h$, via a \"memory unit\" $\\mu$ (which Elman also called \"context unit\") as depicted in **Figure X**. In short, the the \"memory\" unit keeps a running average of **all past outputs**: this is how the entire past history is implicitly accounted for on each new computation. There is no learning in the memory unit, which means the weights are fixed to $1$.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> Figure X: Jordan Network </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/rec-net/jordan-net.svg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: Jordan's network diagrams exemplifies the two ways in which recurrent nets are usually represented: the **compact format** depicts the network structure as circuits, whereas the **unfolded representation** incorporates the notion of time-steps calculations. The unfolded representation also illustrate how a recurrent network can be seen in a pure feed-forward fashion.\n",
    "\n",
    "Elman's innovation was twofold: **recurrent conections between hidden units and memory** (contex) units, and **trainable parameters from the memory units to the hidden units**. Memory units now have to \"remember\" the past state of hidden units, which means that instead of keeping a running average, \"copy\" or \"clone\" the vaue at the previous time-step $t-1$. Memory untis also have to learn useful representations (weights) for encoding temporal properties of the sequential input.  **Figure X** summarizes Elman's network in compact and unfolded fashion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> Figure X: Elman Network </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/rec-net/elman-net.svg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: there is something weird with Elman's architecture. What it is the point of \"cloning\" $h$ into $c$ at each time-step? You could bypass $c$ altogether by sending the value of $h_t$ straight into $h_{t+1}$, wich yield matehmatically identical results. The most likely explanation for this was that Elman started point was Jordan's network, which had a separated memory unit. Regardless, keep in mind we don't need $c$ units to design a mathematically identical network. \n",
    "\n",
    "Elman performed multiple experiments with this architecture demostrating that it was capable to solve multiple problems with a sequential structure: a temporal version of the XOR problem; the structure (i.e., vowels and consonants sequential order) in sequences of letters; discovering the notion of \"word\"; and even complex lexical classes like word order in short sentences. Let's briefly explore the temporal XOR solution as an exemplar. **Table 1** shows the XOR problem:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table 1**: Truth Table For XOR Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| $x_1$ | $x_2$ | $y$ |\n",
    "|---|---|--------|\n",
    "| 0 | 0 | 0      |\n",
    "| 0 | 1 | 1      |\n",
    "| 1 | 0 | 1      |\n",
    "| 1 | 1 | 0      |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a way to transform the XOR problem into a sequence. Consider the following vector: \n",
    "$$\n",
    "s= [1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,...]\n",
    "$$\n",
    "\n",
    "In $\\bf{s}$, the first and second elements, $s_1$ and $s_2$, represent $x_1$ and $x_2$ inputs of **Table 1**, whereas the third element, $s_3$, represents the corresponding output $y$. This pattern repeats until the end of the sequence $s$ as shown in **Figure X**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> Figure X: Temporal XOR </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/rec-net/temporal-xor.svg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elman trained his network with a 3,000 elements sequence for 600 iterations over the entire dataset, on the task of predicting the next item $s_{t+1}$ of the sequence $s$. He showed that **error pattern** followed a predictable trend: the mean squared error was **lower every 3 elements**, and higher in between, meaning that the network learned to predict the third element in the sequence, as shown in **Figure X** (the numbers are made up, but the pattern is the same found by Elman)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import altair as alt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-80171bf7675443898c40c9808275e503\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  (function(spec, embedOpt){\n",
       "    const outputDiv = document.getElementById(\"altair-viz-80171bf7675443898c40c9808275e503\");\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.0.2?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function loadScript(lib) {\n",
       "      return new Promise(function(resolve, reject) {\n",
       "        var s = document.createElement('script');\n",
       "        s.src = paths[lib];\n",
       "        s.async = true;\n",
       "        s.onload = () => resolve(paths[lib]);\n",
       "        s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "        document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "      });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else if (typeof vegaEmbed === \"function\") {\n",
       "      displayChart(vegaEmbed);\n",
       "    } else {\n",
       "      loadScript(\"vega\")\n",
       "        .then(() => loadScript(\"vega-lite\"))\n",
       "        .then(() => loadScript(\"vega-embed\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-2fb8460af59365532c40b2d7e4fe648a\"}, \"mark\": \"line\", \"encoding\": {\"x\": {\"type\": \"quantitative\", \"field\": \"cycle\"}, \"y\": {\"type\": \"quantitative\", \"field\": \"MSE\"}}, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.0.2.json\", \"datasets\": {\"data-2fb8460af59365532c40b2d7e4fe648a\": [{\"MSE\": 0.35, \"cycle\": 1}, {\"MSE\": 0.15, \"cycle\": 2}, {\"MSE\": 0.3, \"cycle\": 3}, {\"MSE\": 0.27, \"cycle\": 4}, {\"MSE\": 0.14, \"cycle\": 5}, {\"MSE\": 0.4, \"cycle\": 6}, {\"MSE\": 0.35, \"cycle\": 7}, {\"MSE\": 0.12, \"cycle\": 8}, {\"MSE\": 0.36, \"cycle\": 9}, {\"MSE\": 0.31, \"cycle\": 10}, {\"MSE\": 0.15, \"cycle\": 11}, {\"MSE\": 0.32, \"cycle\": 12}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.Chart(...)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = pd.DataFrame({\"MSE\": [0.35, 0.15, 0.30, 0.27, 0.14, 0.40, 0.35, 0.12, 0.36, 0.31, 0.15, 0.32],\n",
    "                  \"cycle\": np.arange(1, 13)})\n",
    "alt.Chart(s).mark_line().encode(x=\"cycle\", y=\"MSE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In subsequent experiments, Elman showed that the **internal (hidden) representations** learned by the network grouped into meaningful categories, this is, **semantically similar words group together** when analyzed with [hierarchical clustering](https://en.wikipedia.org/wiki/Hierarchical_clustering). This was remarkable as demostrated the utility of neural networks models as model of cognition in sequence-based problems. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interlude: vanishing and exploding gradients in RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turns out, training recurrent neural networks is hard. Considerably harder than multilayer-perceptrons. When faced with the task of training **very deep networks**, like RNNs, the gradients of such networks have the impolite tendency of either (1) **blow up**, or (2) **vanishing**.   \n",
    "\n",
    "**When gradients begin small**, as you move backwards through the network computing the gradients, they will get smaller and smaller as we get closer to the input layer. Consequently, when we do the weight update based on such gradients, the weights closer to the output layer will obtain bigger updates than weights closer to the input layer. This means that the weights closer to the input layer, will hardly change at all, whereas the weights closer to the ouput layer will change a lot. This is a serious problem when earlier layers actually matter for prediction: they will keep propagating more or less the same signal forward, no learning will happen, which may significantly hinder the network performance.  \n",
    "\n",
    "**When gradients begin too large**, as you move backwards through the network computing the gradients, they will get bigger and bigger as we get closer to the input layer. Learning can go wrong really fast. Remember that the signal propagated by each layer is the outcome of taking the dot product between a weight matrix and the output of the previous layer. If the weights get really large, the layers will forward-propagate larger and larger signals on each iteration, and the predicted output values will spiral-up out of control. The error will be so large that the network will be unable to learn at all.   \n",
    "\n",
    "Here is an image for vanishing gradients: imagine you have 20 parrots (yikes!). Every morning, you feed them. They line up nicely in a queue waiting for food. You love your parrots, and your anxious, so you begin serving food  bit too generously. As you move further in the queue you realize you won't have enough food for everyone at the current rate, so you start to serve less and less food as you move along the line. By the time you reach your last parrot (your favorite, \"Coco\"), you barely have any food left, and poor Coco will stays hungry for the rest of the morning. You have been a victim of the \"vanishing food problem\". If you keep commiting this error, the parrots at the beginning of the queue will go obese, whereas the ones at the end will go malnourished. Neural networks face similar problems: layers (parrots) need gradients (food) as nurishment. Too much and they blow up, too little and they vanish. \n",
    "\n",
    "Now, there are very precise mathematical reasons (unrelated to generosity and anxiety) why the vanishing and exploding gradient problems happen in recurrent networks (and very deep networks more generally). The proofs get complicated quickly. More than what I want to cover here. I'll give a short numerical example for intuition. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Long Short-Term Memory Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematical formalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limitations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
