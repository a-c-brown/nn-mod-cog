{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory and history of the perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frank Rosenblatt proposed the 'perceptron' algorithm as a way to formalize several ideas about knowledge representation and cognition in the mid 20th century (Rossenblatt, 1958). He wanted to approach to fundamental questions: \n",
    "\n",
    "* In what form is information stored or remembered? \n",
    "* How does the information contained in storage, or in memory, influence recognition and behavior?\n",
    "\n",
    "According to Rossenblat, there were two distinct approaches to answer such questions: the **code theorist** and the **empiricist/connectionist theorist**. Code theorist proposed that sensory information is in the form of *coded representations* or images, with some sort of one to one mapping between the sensory stimulus and the stored pattern. On the other hand, empiricist/connectionist theorist, inspired by how biological systems actually work, proposed that the central nervous system acts as an intricate *switching network*, where information takes the form of new connections or pathways between centers of activity. In other words, accoding to empiricist/connectionist theorist, there is no simple one-to-one mapping from stimulus into memory, i.e., *the information or knowledge is contained in the connections or associations between neurons*, rather than as static blueprints to be compared with incoming sensory stimulus. \n",
    "\n",
    "Rosenblatt, inspired by the work of empiricist/connectionist theorist like Hebb, Hayek, and others, wanted to express these ideas more formally and rigorously in order to be tested computationally. Some key ideas guiding Rosenblatt reasoning where: \n",
    "\n",
    "* The physical connections in the brain involved in learning and recognition vary from organism to organism\n",
    "* Plasticity is a key feature of the nervous system, such that changes in response to experience\n",
    "* Rewards and punishments can increase or decrease the probability of certain connections reacting to similar stimulus\n",
    "\n",
    "Therefore, information is understood as a dynamic ever-changing pattern of connectivity among neurons in the brain, which are subject to the influence of experience and the environment of the organism. As such, no two brains are the same, and the way in which your brain and may brain storage information about the same fact, may be completely different. \n",
    "\n",
    "The perceptron (or the ‘photoperceptron’ in the case of vision) is a formal mathematical manifestation of these ideas. In essence, a **single-layer perceptron** is a system that: \n",
    "\n",
    "* Takes and input through the sensory sensory system (e.g., the retina)\n",
    "* Combines the values of the inputs with the weights of the connections between the neurons\n",
    "* Produce an output in an all-or-nothing fashion belonging to a class of objects (mathematically, as a step-function). \n",
    "\n",
    "Next, we formally define a perceptron and implement the algorithm in code. \n",
    "\n",
    "Ref: \n",
    "- Rosenblatt, F. (1958). The perceptron: a probabilistic model for information storage and organization in the brain. Psychological review, 65(6), 386.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formal definition of the perceptron algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mathematically, the perceptron is a simple **linear classifier** defined by two formulas\n",
    "* a **decision or threshold function**\n",
    "* a **training or learning rule**\n",
    "\n",
    "![alt text](https://github.com/pabloinsente/comp_models_cog_beh/blob/master/figures/images/perceptron_flow.png \"Single-layer perceptron\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **decision or threshold function** for the perceptron is defined as:  \n",
    "\n",
    "\n",
    "$$  f(z) =\n",
    "\\begin{cases}\n",
    " 1,  & \\text{if $\\boldsymbol w \\cdot \\boldsymbol x + b$ > 0} \\\\\n",
    "-1, & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "where:  \n",
    "- ${z}$ is a vector of reald-valued features  \n",
    "- $w$ is a vector of real-valued weights  \n",
    "- $w \\cdot x$ is a inner product   \n",
    "- $b$ is the bias term  \n",
    "\n",
    "Note about the bias term: the we add a bias term to allow the model to adjust the intercept of the line on the plane. Otherwise, the line would be forced to go through the origin in the cartesian plane (in the 2-Dimensional case). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute the **net-input** for the decision function, we need to compute the inner product (or dot product) of the feature vector and the weigths vector. The inner product is defined as: \n",
    "\n",
    "\\begin{equation*}\n",
    "\\sum_{k=1}^n w_k x_k = w_0 x_0 + w_1 x_1 + ... + w_n x_n\n",
    "\\end{equation*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The perceptron learning rule is simple: compute the mistmatch between the **predicted value** for a given exemplar and the **actual value**, and then use such delta to update the value the weights vector. This is defined as:\n",
    "\n",
    "\\begin{equation*}\n",
    "w_k = w_k + \\Delta w_k\n",
    "\\end{equation*}\n",
    "\n",
    "The $\\Delta w_k$ is computed as:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\Delta w_k = \\eta(y^i - \\hat{y}^i)x^i_k\n",
    "\\end{equation*}\n",
    "\n",
    "where:\n",
    "- $\\eta$ is the learning rate (0 -1 value)\n",
    "- $y^i$ is the actual value (\"true class\")\n",
    "- $\\hat{y}^i$ is the predicted value (\"predicted class\")\n",
    "- $x^i_k$ is the feature vector for case $k$\n",
    "\n",
    "Note about the learning rate $\\eta$: the learning rate has the role of facilitating the training process by weighting the delta used to update the weights. This basically means that instead of completely replacing the previous weight with the sum of the weight + delta, we incorporate a **proportion** of the error into the updating process. This makes the learning process more stable and smooth. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron algorithm implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will implement the perceptron algorithm from scrath with Python and Numpy (a Python package for scientific computing). The goal is to understand the perceptron step-by-step execution rather than achieving an elegant implementation. I'll break down each step into functions to ensemble everything at the end. \n",
    "\n",
    "The scikit-learn implementation can be found [here](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Perceptron.html). In general, we don't want to re-implemented algorithms that has been thouroughly tested by others, to avoid duplication of work an potential bugs in our code. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate random weights vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def random_weights(X, random_state: int):\n",
    "    '''create vector of random weights'''\n",
    "    rand = np.random.RandomState(random_state)\n",
    "    w = rand.normal(loc=0.0, scale=0.01, size=1 + X.shape[1])\n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predictions from the percetron are obtained by a linear combination of features and weights. It is common practice to begin with a vector of small random weights that would be updated later by the perceptron learning rule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute net input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def net_input(X, w):\n",
    "    '''Compute net input as dot product'''\n",
    "    return np.dot(X, w[1:]) + w[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we pass the featue matrix and the previously generated vector of random weights to compute the inner product. Remember that we need to add an extra weight for the bias term at the begining of the vector (`w[0`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, w):\n",
    "    '''Return class label after unit step'''\n",
    "    return np.where(net_input(X, w) >= 0.0, 1, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method implements the **threshold function** which takes the net-value of the inner product and outputs a 1 if the predicted value is >= 0, and -1 otherwise. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop - Learning rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(X, y, eta=0.01, n_iter=50):\n",
    "    '''loop over exemplars and update weights'''\n",
    "    errors = []\n",
    "    w = random_weights(X, random_state=1)\n",
    "    for exemplar in range(n_iter):\n",
    "        error = 0\n",
    "        for xi, target in zip(X, y):\n",
    "            delta = eta * (target - predict(xi, w))\n",
    "            w[1:] += delta * xi\n",
    "            w[0] += delta\n",
    "            error += int(delta != 0.0)\n",
    "        errors.append(error)\n",
    "    return w, errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine the fit method that implements the learning rule: \n",
    "* Create a vector of random weights by using the `random_weights` function with dimensionality equal to the number of columns in the feature matrix.\n",
    "* Loop over each row of the feature matrix with `for exemplar in range(n_iter)`\n",
    "* Compute the inner product between the feature vector for row $i$ and the weight vector by using the `predict(xi, w)` function\n",
    "* Compute the difference between the predicted value and the target value times the learning rate with `delta = eta * (target - predict(xi, w))`\n",
    "* Update the weights by `w[1:] += delta * xi` and `w[0] += delta`\n",
    "* We also save the errors for further plotting `errors.append(error)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will test the implementation by creating a minimal example with only 4 cases (rows) and 3 features (columns). The association between features and targets is extremely simple and obvious: cases with high values predict '1' (positive class) and cases with low values predict '-1' (negative class). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vector of weights: [-0.38375655  0.01388244  0.01471828  0.06927031]\n",
      "errors at each time step: [3, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "predicted value for each case: [ 1 -1  1 -1]\n"
     ]
    }
   ],
   "source": [
    "# create matrix of features\n",
    "X = np.array([[11, 21, 33],\n",
    "              [1, 2, 3],\n",
    "              [12, 24, 37],\n",
    "              [1, 2, 3]])\n",
    "\n",
    "# create targets\n",
    "y = np.array([1, -1, 1, -1])\n",
    "\n",
    "# fit and predict values\n",
    "w, errors = fit(X, y)\n",
    "y_pred = predict(X, w)\n",
    "print(f'vector of weights: {w}')\n",
    "print(f'errors at each time step: {errors}')\n",
    "print(f'predicted value for each case: {y_pred}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By printing the prediction error ar each time step we can see that the perceptron predicts each case correctly after 12 training cicles. Let's plot the errors to complement the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "def plot_errors(errors):\n",
    "    step = np.arange(0, len(errors))\n",
    "    fig = plt.figure()\n",
    "    ax = plt.axes()\n",
    "    ax.plot(step, errors)\n",
    "    return plt.show()\n",
    "\n",
    "plot_errors(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
