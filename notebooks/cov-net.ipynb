{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Understand the principles behind the creation of the convolutional network\n",
    "2. Gain a intuitive understanding of the convolution (feature map) and pooling (subsampling) operations\n",
    "3. Develop a basic code implementation of the LeNet-5 and AlexNet netoworks in Python\n",
    "4. Identify what kind of problems can be approached with the convolutional network architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Historical and theoretical background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hubel and Wiesel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rosenblatt's photo-perceptron (1958) was the first neural network model attempting to emulate human visual and perceptual capacities. Unfortunetaly, little was known at the time about the mammalian visual cortex that could inform Rosenblatt's work. Consequently, the photo-perceptron architecture was inspired by a very coarse idea of how the information flows from the eyeballs to be processed by the brain. This changed fast in the years following the introduction of the perceptron. \n",
    "\n",
    "In 1962, [David H. Hubel](https://en.wikipedia.org/wiki/David_H._Hubel) and [Torsten Wiesel](https://en.wikipedia.org/wiki/Torsten_Wiesel) published one the major breaktroughts in the neurophysiology of the visual cortex: **the existence of orientation selectivity and columnar organization**. This is what they did: they placed tiny microelectrode in a single neuron in the primary visual cortex (V1) of an anesthetized cat, and proyected light and dark dots into the cat's eye. It did not work at all, they could not get a response from the neuron. But, they had a lucky accident. Since they were using a slide projector to show the dots, the *margin of the slide* with the dot also was projected into the cat's ayes and bam! the neuron fired. From there, they experimented with light and dark bars in different orientations, which led them to propose the existence of **three types of cells in the visual cortex**:\n",
    "\n",
    "1. **simple cells**, that fire at higher (or lower) rate depending on the bar orientation. Sometimes called \"line detectors\".\n",
    "2. **complex cells** that fire in response to a wider variety of orientations, yet, they still show a preference (higher firing rate) to certain orientations. Sometimes are called \"motion detectors\". Importantly, these cells receive input from several *simple cells*. \n",
    "3. **hypercomplex cell**, characterized by reacting to \"stopped\" oriented edges. Again, these cells receive input from several *complex cells*.\n",
    "\n",
    "As you may have noticed, there three types of cells are **hierarchically organized**. Keeps this in mind as it'll become important later. Altogether, these discoveries were the basis of the work that granted them the Nobel Prize in Physiology in 1981. Below is short video from their experiments. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "[![](http://img.youtube.com/vi/jw6nBWo21Zk/0.jpg)](http://www.youtube.com/watch?v=jw6nBWo21Zk \"Hubel & Wiesel's demonstration of simple, complex and hypercomplex cells in the cat's visual cortex\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fukushima's Neocognitron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The work of Hubel and Wiesel served as the basis for the precursor of modern convolutional neural netwroks: **Fukushima's Neocognitron** (1980). [Kunihiko Fukushima](https://en.wikipedia.org/wiki/Kunihiko_Fukushima), a japanese computer scientist, developed the the Neocognitron idea while working at the [NHK Science & Technology Research Laboratories](https://en.wikipedia.org/wiki/NHK_Science_%26_Technology_Research_Laboratories). He did this by implementing the simple-cells and complex-cells discovered by Hubel and Wiesel in a multilayer neural network architecture. **Figure X** shows a simplified diagram of the Neocognitron with 3 layers (4 if you count the inputs). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> Figure X: Simplified Neocognitrone </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/cov-net/neocognitron.svg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The general idea behind the Neocognitron is the following: the **input layer $L_0$ works as the retina**, reading the raw input pattern. Then, each cell in a $S_1$ patch \"reads\" a sub-section of the input image based on a \"preference\" for certain type of pattern. Any given layer $L_n$ will have several of this $S_j$ patches as a collection of **feature \"filters\"**. Some may detect a diagonal line, while other a small triangle, or something else. Each $S_j$ patch connects to a $C_k$ cell, and such a cell fires if gets any positive input from its corresponding patch. This process is also known as **\"pooling\"**. This cycle of \"feature\" detection and \"pooling\" is repeated as many times as intermediate layers in the network. The last layer correspond to the output, where some neuron will fire depending of the input pattern. Mathematically, \"feature detection\" is accomplished by multiplying the input by a fix matrix of weights, whereas \"pooling\" corresponding to taking an average of the S-cells. \n",
    "\n",
    "You may have noticed that the behavior of the S-cells and C-cells replicate (to some extent) what Hubel and Wiesel found in their experiments. The great thing about this architecture is that is **robust to shifts in the input image**: you can move the image around and the combination of \"feature detection\" and \"pooling\" will detect the precense of each part of the image regardless of its position. **Figure X** exemplifies this characteristic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> Figure X <\\center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/cov-net/neocognitron-cells.svg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The neocognitron is also **robust to deformation**: it will detect the object even if it's enlarged, reduced in size, or blurred, by virtue of the same mechanism that allows robustness to positional shifting. It is also important to notice that the pooling operation will \"blur\" the input image, and the fact that C-cells take the average of its corresponding S-cells makes the pooling more robust to random noise added to the image. Below you can find a short video explaining the basics of the Neocognitron as well."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "[![](http://img.youtube.com/vi/Qil4kmvm2Sw/0.jpg)](http://www.youtube.com/watch?v=Qil4kmvm2Sw \"Neocognitron Movie - Part #1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are familiar with convolutional neural networks, you may be wondering what is the difference between the Neocognitron and later models like Yann LeCun's LeNet (1989), since they look remarkably similar. They main (but not only) difference is the training algorithm: **the Neocognitron does not use backpropagation**. At the time, backpropagation was not widely known as a training method for multilayer neural networks reason why Fukushima never use it, and trained his model by using an unsupervised learning approach. Regardless, the Neocognitron lay the groundwork of modern neural network models of vision and computer vision more generally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LeCun's LeNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The architecture that today is known as the convolutional neural network was introduced by [Yann LeCun](http://yann.lecun.com/) in 1989. Although Yann LeCun was trained as an Electrical Engineer, he got interested in the idea of inteligent mahines from early on in his undergradute education by reading a book about the [Piaget vs Chomsky debate on language acquisition](https://www.sciencedirect.com/science/article/abs/pii/0010027794900345). In that book, several researchers argued in favor or against each authors view. Among those contributors was [Seymour Papert](https://en.wikipedia.org/wiki/Seymour_Papert) who mentioned Rosenblatt's perceptron in his article, which inspired LeCun to learn about neural networks for the first time. Ironically, this was the same Seymour Papert that published the book (along with Marvin Minsky) that broguht the demise on the interest on neural networks in the late '60s. I don't belive in karma, but this certainly looks like it. \n",
    "\n",
    "Eventually, LeCun became postdoc at the University of Toronto with Geoffrey Hinton and started to prototype the first convolutional network. By the late '80s LeCun was working at [Bell Labs](https://en.wikipedia.org/wiki/Bell_Labs) in New Jersey, place where he and his colleagues developed at published the **first convolutional neural network trained with backpropagation**, the \"LeNet\", that could effectively recognize handwritten zip codes from US post office. This early convolutional network went through several rounds of modifications and improvements (LeNet-1, LeNet-2, etc.), until by 1998 the [LeNet-5](http://yann.lecun.com/exdb/lenet/) reached test error rates of 0.95% in the [MNIST dataset of handwritten digits](http://yann.lecun.com/exdb/mnist/). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The convolution opertion: feature detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll begin by schematically describing the LeNet-5 model and leave the mathematics for the next section. This conceptual explanation should be enough to have a higher-level understanding of the model but not necesarilly to implement a convolutional network. For this understanding the mathematics is fundamental.\n",
    "\n",
    "The general architecture of the LeNet-5 is shown in **Figure X**. The input layer $L-0$ acts like the retina receiving images of characters that are centered and size-normalized (otherwise, some images may not fit the in the input layer). The next layer $L-1$ is composed of several **feature maps**, which have the same role that the Neocognitron simple-cells: to extract simple features as oriented edges, corners, end-points, etc. In practice, a feature map is a squared matrix of **identical weights**.  Weights *within* a feature map pattern need to be identical so they can detect *the same* local feature in the input image. Weights *between* feature maps patterns are different so they can detect *different* local features. Each unit in a feature map has a **receptive field**. This is, a small $n \\times n$ sub-area of the input image that can be \"perceived\" by a unit in the feature map at any given time.  \n",
    "\n",
    "Feature maps and receptive fields sounds complicated. Here is a methaphor thay may be helpful: imagine that you have 6 flashlights with a *square* beam of light. Each flashlight has the special quality of revealing certain \"features\" of images drawn with invisible ink, like corners or oriented edges. Also imagine that you have a set of images that were drawn witn invisible ink. Now, you need your special flashlights to reveal the hidden character in the image. What you need to do is to carefully iluminate each section of the invisible image, from *right to left and top to bottom*, with each of your 6 flashlights. Once you finish the process, you should be able to put together all the little \"features\" revealed by each flashlight to compose the full image shape. Here, the square beam of light sweeping through each pixel represents the aforementioned *receptive field*, and each flashlight represents a *feature map*. \n",
    "\n",
    "**Figure X** shows a represention of the feature detection process (assuming that each time a pixel in the input image matchs a pixel in the feature detector we add a value of 1, athough in practice it can be any real-valued scalar). In this example we use a **stride** of 1, meaning that we shift the receptive field by 1 pixel (to the right or down) for each cell in the feature map."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> Figure X: Feature detection (convolution) <\\center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/cov-net/convolution.svg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process of sliding over the image with feature maps (matrix of weights) equals to a mathematical operation called **convolution**, hence the name **convolutional network**. The full convolution operation involves repeating the process in **Figure X** for each feature map. If you are wondering how do you come up with appropiated features detectors, the answer is that you don't need to: *feature maps are learned in the training process*. More on the mathematics of this later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The pooling operation: subsampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the convolution operation is done, what we have learned is whether a feature is present in the image or not. Now, knowing that a collection of features is present in an imagine won't tell us, by itself, which image they correspond to. What we need to know is their **approximate position relative to each other**. For instance, if we know that we have a \"curvy horizontal line\" at the center-bottom, a \"curvy vertical line\" at the middle-right, a \"stright vertical line\" at upper-left, and a \"stright horizontal line\" at the center-top, we should be able to tell we have a \"5\". This is even more important considering that real-life images like handwritten numbers, have considerable *variability* in their shape. No two individuals write numbers in the exact same manner. Hence, we want our network to be as *insensitive as possible* to the exact position of a feature, and as *sensitive as possible* to its relative position. One way to accomplish this is by **reducing the spacial resolution of the image**. This is what **sub-sampling** or **pooling** does. \n",
    "\n",
    "The are many ways to sub-sample an image. In the LeNet-5, this operation performs a **local averaging** of a section of the feature map, effectively *reducing the resolution* of the feature map as a whole, and the sensitivity of the network to shifts and distorsions in the input image. A colloquial example is what happens when you \"pixelate\" an image, like in **Figure X**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> Figure X: sub-sampling effect <\\center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/cov-net/pixelated.svg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A sub-sampling layer will have as many \"pixelated\" feature maps as \"normal\" feature maps in the convolutional layer. The **mechanics of sub-sampling** are as follow: again, we have $n \\times n$ receptive field that \"perceives\" a section of the \"normal\" feature map and connect to a unit in the \"pixelated\" feature map. This time, there is no overlap between each \"stride\" of the receptive field: each unit is connected to a *non-overlapping section* of the original feature map. You can think on this as taking \"strides\" of a size equal to $n$, e.g., for a $3 \\times 3$ feature map, we take a stride of $3$. Then, we take weighted average of each pixel in the receptive field, and pass the resulting sum through a sigmoid function (or any other non-linear function). The *weights* in the weigthed average are also parameters that the network learns over training. **Figure X** shows this process for a *single* sub-sampled feature map."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> Figure X: Sub-sampling (pooling)<\\center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/cov-net/pooling.svg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of sub-sampling is another grid of numbers (note that the numbers in **Figure X** are made up). We went from a $12 \\times 12$ input image, to a $3 \\times 3$ feature map after convolution and pooling. Keep in mind that I intentionally reduced LeNet-5 original dimnensions to simplify the examples. Since in our original example we had 6 features map, we need to repeat the process in **Figure X** 6 times, one of each feature map.\n",
    "\n",
    "The next sub-sampling hidden layer $S_2$ increases the number of feature maps compared to $S_1$. If you were to add more sets of $S_n$ and $C_n$ hidden layers, you will repeat this altenating pattern again: *as the spatial resolution is reduced (by pooling), the number of feature maps in the next layer is increased*. The idea here is to **compensate the reduction in spatial resolution by increasing the richness of the learned representations** (i.e., more feature maps).\n",
    "\n",
    "Once we are done with the sequence of convolution and pooling, the network implements a traditional fully-connected layer as in the multi-layer perceptron. The first fully-connected $F_1$ layer has the role of **\"flattening\"** the $C_2$ pooling layer. Remember that fully-connected layers take a one-dimensional input vector, and the dimensions of the LeNet-5 $C_2$ layer are  $5 \\times 5 \\times 16$, this is, sixteen 5 by 5 feature maps. The dimensionality of the first  fully-connected layer is simply $5 \\times 5 \\times 16 = 120$. The next one hidden layer $F_2$ \"compress\" the output even further into a $1 \\times 84$ vector. Finally we have the **output-layer** the implements a **softmax function** with 10 neurons to perform the classification of numbers (0-9)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> Figure X: LeNet-5 Architecture <\\center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/cov-net/LeNet.svg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AlexNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LeNet-5 perfomance in the MNIST dataset was impressive but not out of the ordinary. Others methods like the Suport Vector Machines could reach [similar or better perfomance at the time](http://yann.lecun.com/exdb/mnist/). Training neural networks was still costly and complicatd compared to others machine learning techniques, so the interest in neural nets faded in the late '90s again. However, serveral research group continued to work in neural networks. The next big breaktrough in computer vision came in 2012  when Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton introduced the [\"AlexNet\"](https://en.wikipedia.org/wiki/AlexNet), a convolutional neural network that won the [\"ImageNet Large Scale Visual Recognition Challenge\"](https://en.wikipedia.org/wiki/ImageNet#ImageNet_Challenge) for a wide margin, surprising the entire computer vision community.\n",
    "\n",
    "The main innovation introduced by AlexNet compared to the LeNet-5 was **its sheer size**. AlexNet main elements are the same: a sequence of convolutional and pooling layers with a couple of dense layers at the end. The LeNet-5 has **two sets** of convolutional and pooling layers, two fully-connected layers, and softmax classifier at the end. AlexNet has **five sets** of convolutional and pooling layers, three fully-connected layers, and the softmax classifier layer, as shown in **Figure X**. The training time and dataset was larger as well. All of this was possible because of the availability of raw computational processing (particularly GPUs) and larger datasets.\n",
    "\n",
    "I won't go into describing the AlexNet. Their main elements are essentially the same as in LeNet-5 with a few changes, like [max-pooling](https://computersciencewiki.org/index.php/Max-pooling_/_Pooling) (i.e., taking the larger value in the receptive field instead of the average) instead of average-pooling. If you followed the description of LeNet-5 you should be able to understand AlexNet architecture without much diffculty. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# TODO: \n",
    "- AlexNet graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network models of vision and computer vision drifting apart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you ask to a random researcher in computer vision about the correspondene betwen the human visual/perceptual system and convolutional nets, the most likely answer would be something like: \"*Well, CNNs are roughly inspired in the brain but aren't actual models of the brain. I care about solving the problem artificial vision by any means necessary, regardless of the biological correspondance to human vision, more or less in the same manner we solved flying withouth having to imitate birds flapping*\". Or some version of that. This talks to how **computer vision has become an independent area of research with its own goals**. Most researchers are fully aware that many of the design properties of modern convolutional nets are not biologically realistic. Beyond the parallels with human vision, strictly speaking, LeNet-5 and AlexNet are designed to maximize object-recognition perfomance, not biological-realism. And that's is perfectly fine. For instance, the LeNet 5 paper (1998) was published in the context of the debate between traditional pattern recognition with handcrafted features vs the automated learning-based approach of neural nets. Nothing was said about human perception. However, from our perspective, the issue of **whether convolutional nets are useful model of human perception and vision** is critical. This is a open debate. Many researchers do beleive that convolutionls nets are useful models for human vision and perception, and there is a long list of [scientific articles trying to show this point](https://www.mitpressjournals.org/doi/abs/10.1162/jocn_a_01544). I won't review those arguments now. My point is to highlight the fact that what I'll describe next are just \"coarse\" models attempting to reproduce human abilities in a narrow setting, not a full-blown model of human vision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematical formalization"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code implementation"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limitations"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lindsay, G. (2020). Convolutional Neural Networks as a Model of the Visual System: Past, Present, and Future. Journal of Cognitive Neuroscience, 1–15."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
