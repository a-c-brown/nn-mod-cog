{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning objectives"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Historical and theoretical background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hubel and Wiesel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rosenblatt's photo-perceptron (1958) was the first neural network model attempting to emulate human visual and perceptual capacities. Unfortunetaly, little was known at the time about the mammalian visual cortex that could inform Rosenblatt's work. Consequently, the photo-perceptron architecture was inspired by a very coarse idea of how the information flows from the eyeballs to be processed by the brain. This changed fast in the years following the introduction of the perceptron. \n",
    "\n",
    "In 1962, [David H. Hubel](https://en.wikipedia.org/wiki/David_H._Hubel) and [Torsten Wiesel](https://en.wikipedia.org/wiki/Torsten_Wiesel) published one the major breaktroughts in the neurophysiology of the visual cortex: **the existence of orientation selectivity and columnar organization**. This is what they did: they placed tiny microelectrode in a single neuron in the primary visual cortex (V1) of an anesthetized cat, and proyected light and dark dots into the cat's eye. It did not work at all, they could not get a response from the neuron. But, they had a lucky accident. Since they were using a slide projector to show the dots, the *margin of the slide* with the dot also was projected into the cat's ayes and bam! the neuron fired. From there, they experimented with light and dark bars in different orientations, which led them to propose the existence of **three types of cells in the visual cortex**:\n",
    "\n",
    "1. **simple cells**, that fire at higher (or lower) rate depending on the bar orientation. Sometimes called \"line detectors\".\n",
    "2. **complex cells** that fire in response to a wider variety of orientations, yet, they still show a preference (higher firing rate) to certain orientations. Sometimes are called \"motion detectors\". Importantly, these cells receive input from several *simple cells*. \n",
    "3. **hypercomplex cell**, characterized by reacting to \"stopped\" oriented edges. Again, these cells receive input from several *complex cells*.\n",
    "\n",
    "As you may have noticed, there three types of cells are **hierarchically organized**. Keeps this in mind as it'll become important later. Altogether, these discoveries were the basis of the work that granted them the Nobel Prize in Physiology in 1981. Below is short video from their experiments. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "[![](http://img.youtube.com/vi/jw6nBWo21Zk/0.jpg)](http://www.youtube.com/watch?v=jw6nBWo21Zk \"Hubel & Wiesel's demonstration of simple, complex and hypercomplex cells in the cat's visual cortex\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fukushima's Neocognitron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The work of Hubel and Wiesel served as the basis for the precursor of modern convolutional neural netwroks: **Fukushima's Neocognitron** (1980). [Kunihiko Fukushima](https://en.wikipedia.org/wiki/Kunihiko_Fukushima), a japanese computer scientist, developed the the Neocognitron idea while working at the [NHK Science & Technology Research Laboratories](https://en.wikipedia.org/wiki/NHK_Science_%26_Technology_Research_Laboratories). He did this by implementing the simple-cells and complex-cells discovered by Hubel and Wiesel in a multilayer neural network architecture. **Figure X** shows a simplified diagram of the Neocognitron with 3 layers (4 if you count the inputs). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> Figure X: Simplified Neocognitrone </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/cov-net/neocognitron.svg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The general idea behind the Neocognitron is the following: the **input layer $L_0$ works as the retina**, reading the raw input pattern. Then, each cell in a $S_1$ patch \"reads\" a sub-section of the input image based on a \"preference\" for certain type of pattern. Any given layer $L_n$ will have several of this $S_j$ patches as a collection of **feature \"filters\"**. Some may detect a diagonal line, while other a small triangle, or something else. Each $S_j$ patch connects to a $C_k$ cell, and such a cell fires if gets any positive input from its corresponding patch. This process is also known as **\"pooling\"**. This cycle of \"feature\" detection and \"pooling\" is repeated as many times as intermediate layers in the network. The last layer correspond to the output, where some neuron will fire depending of the input pattern. Mathematically, \"feature detection\" is accomplished by multiplying the input by a fix matrix of weights, whereas \"pooling\" corresponding to taking an average of the S-cells. \n",
    "\n",
    "You may have noticed that the behavior of the S-cells and C-cells replicate (to some extent) what Hubel and Wiesel found in their experiments. The great thing about this architecture is that is **robust to shifts in the input image**: you can move the image around and the combination of \"feature detection\" and \"pooling\" will detect the precense of each part of the image regardless of its position. **Figure X** exemplifies this characteristic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> Figure X <\\center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/cov-net/neocognitron-cells.svg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The neocognitron is also **robust to deformation**: it will detect the object even if it's enlarged, reduced in size, or blurred, by virtue of the same mechanism that allows robustness to positional shifting. It is also important to notice that the pooling operation will \"blur\" the input image, and the fact that C-cells take the average of its corresponding S-cells makes the pooling more robust to random noise added to the image. Below you can find a short video explaining the basics of the Neocognitron as well."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "[![](http://img.youtube.com/vi/Qil4kmvm2Sw/0.jpg)](http://www.youtube.com/watch?v=Qil4kmvm2Sw \"Neocognitron Movie - Part #1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are familiar with convolutional neural networks, you may be wondering what is the difference between the Neocognitron and later models like Yann LeCun's LeNet (1989), since they look remarkably similar. They main (but not only) difference is the training algorithm: **the Neocognitron does not use backpropagation**. At the time, backpropagation was not widely known as a training method for multilayer neural networks reason why Fukushima never use it, and trained his model by using an unsupervised learning approach. Regardless, the Neocognitron lay the groundwork of modern neural network models of vision and computer vision more generally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LeCun's LeNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The architecture that today is known as the convolutional neural network was introduced by [Yann LeCun](http://yann.lecun.com/) in 1989. Although Yann LeCun was trained as an Electrical Engineer, he got interested in the idea of inteligent mahines from early on in his undergradute education by reading a book about the [Piaget vs Chomsky debate on language acquisition](https://www.sciencedirect.com/science/article/abs/pii/0010027794900345). In that book, several researchers argued in favor or against each authors view. Among those contributors was [Seymour Papert](https://en.wikipedia.org/wiki/Seymour_Papert) who mentioned Rosenblatt's perceptron in his article, which inspired LeCun to learn about neural networks for the first time. Ironically, this was the same Seymour Papert that published the book (along with Marvin Minsky) that broguht the demise on the interest on neural networks in the late '60s. I don't belive in karma, but this certainly looks like it. \n",
    "\n",
    "Eventually, LeCun became postdoc at the University of Toronto with Geoffrey Hinton and started to prototype the first convolutional network. By the late '80s LeCun was working at [Bell Labs](https://en.wikipedia.org/wiki/Bell_Labs) in New Jersey, place where he and his colleagues developed at published the **first convolutional neural network trained with backpropagation**, the \"LeNet\", that could effectively recognize handwritten zip codes from US post office. This early convolutional network went through several rounds of modifications and improvements (LeNet-1, LeNet-2, etc.), until by 1998 the [LeNet-5](http://yann.lecun.com/exdb/lenet/) reached test error rates of 0.95% in the [MNIST dataset of handwritten digits](http://yann.lecun.com/exdb/mnist/). \n",
    "\n",
    "The general architecture of the LeNet-5 is shown in **Figure X**. The input layer $L-0$ acts like the retina receiving images of characters that are centered and size-normalized (otherwise, some images may not fit the in the input layer). The next layer $L-1$ is composed of several **feature maps**, which have the same role that the Neocognitron simple-cells: to extract simple low-level features as oriented edges, corners, end-points, etc. In practice, a feature map is a squared matrix of **identical weights**.  Weights *within* a feature map need to be identical so they can detect *the same* local feature in the input image. Weights *between* feature maps are different so they can detect *different* local features. Each unit in a feature map has a **receptive field**. This is, a small $n \\times n$ sub-area of the input image that can be \"perceived\" by a unit in the feature map at any given time.  \n",
    "\n",
    "Feature maps and receptive fields sounds complicated. Here is a **methaphor** thay may be helpful: imagine that you have 6 flashlights with a *square* beam of light. Each flashlight has the special quality of revealing certain \"features\" of images drawn with invisible ink, like corners or oriented edges. Also imagine that you have a set of images that were drawn witn invisible ink. Now, you need your special flashlights to reveal the hidden character in the image. What you need to do is to carefully iluminate each section of the invisible image, from *right to left and top to bottom*, with each of your 6 flashlights. Once you finish the process, you should be able to put together all the little \"features\" revealed by each flashlight to compose the full image shape. Here, the square beam of light sweeping through each pixel represents the aforementioned *receptive field*, and each flashlight represents a *feature map*. **Figure X** shows a represention of this process. The process of sweeping through over the image with feature maps (matrix of weights) equals to a mathematical operation called **convolution**, hence the name **convolutional network**. \n",
    "\n",
    "TODO: \n",
    "- Explain pooling\n",
    "- Explain feature flattening\n",
    "- Explain output"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# TODO: \n",
    "- LeNet graph\n",
    "- Feature map and pooling zoomed image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AlexNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LeNet-5 perfomance in the MNIST dataset was impressive but not out of the ordinary. Others methods like the Suport Vector Machines could reach [similar or better perfomance at the time](http://yann.lecun.com/exdb/mnist/). Training neural networks was still costly and complicatd compared to others machine learning techniques, so the interst in neural nets faded in the late '90s again. However, serveral research group continued to work in neural networks. The next big breaktrough in computer vision came in 2012  when Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton introduced the [\"AlexNet\"](https://en.wikipedia.org/wiki/AlexNet), a convolutional neural network that won the [\"ImageNet Large Scale Visual Recognition Challenge\"](https://en.wikipedia.org/wiki/ImageNet#ImageNet_Challenge) for a wide margin, surprising the entire computer vision community.\n",
    "\n",
    "The main innovation introduced by AlexNet compared to the LeNet-5 was **its sheer size**. AlexNet main elements are the same: a sequence of convolutional and pooling layers with a couple of dense layers at the end. The LeNet-5 has **two sets** of convolutional and pooling layers, two fully-connected layers, and softmax classifier at the end. AlexNet has **five sets** of convolutional and pooling layers, three fully-connected layers, and the softmax classifier layer. The training time and dataset was larger as well. All of this was possible because of the availability of raw computational processing (particularly GPUs) and larger datasets. \n",
    "\n",
    "In the next section I'll describe and implement both the LeNet and AlexNet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network models of vision and computer vision drifting apart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you ask to a random researcher in computer vision about the correspondene betwen the human visiual and perceptual system and convolutional nets, the most likely answer something like: \"*Well, CNNs are roughly inspired in the brain but aren't actual models of the brain. I care about solving the problem artificial vision by any means necessary, regardless of the biological correspondance to human vision*\". Or some version of that. This talks to how **computer vision has become an independent area of research with its own goals**. Most researchers are fully aware that many of the design properties of modern convolutional nets are not biologically realistic. And that's is perfectly fine. For instance, the LeNet 5 paper (1998) was published in the context of the debate between traditional pattern recognition with handcrafted features vs the automated learning-based approach of neural nets. Nothing was said about human perception. However, from our perspective, the issue of **whether convolutional nets are useful model of human perception and vision** is critical. This is a open debate. Many researchers do beleive that convolutionls nets are useful models for human vision and perception, and there is a long list of [scientific articles trying to show this point](https://www.mitpressjournals.org/doi/abs/10.1162/jocn_a_01544). I won't review those arguments now. My point is to highlight the fact that what I'll describe next are just \"coarse\" models attempting to reproduce human abilities in a narrow setting, not a full-blown model of human vision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematical formalization"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code implementation"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limitations"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lindsay, G. (2020). Convolutional Neural Networks as a Model of the Visual System: Past, Present, and Future. Journal of Cognitive Neuroscience, 1–15."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
