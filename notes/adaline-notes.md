- Adaline works by minimizing mean squared error. Gradient descent
- delta rule learning - only work with linear units
- perceptron learns only when it makes mistakes. Adaline learns all the time
- Used extensively in telecommunications, filters, and antennas
- With the perceptron, was no measure of optimatility of the weights
- Introduce idea of error surface, minimization, local minima, local maxima
- How to search for weights. Introduce brute force idea.
- Gradient: the direction of steepest descent
- Briefly explain the derivative of a function

ADD adaline scheme
